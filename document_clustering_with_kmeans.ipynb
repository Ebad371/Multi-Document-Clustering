{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Goddard_project#1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d3c25a21d21b42adb251c3499d2445e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cfe6e8ccad2a460c8e46bd90bfedef28",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fa15e15fb5174723ab5616fd589925db",
              "IPY_MODEL_e963cc87aacc4e958697780504b741b9"
            ]
          }
        },
        "cfe6e8ccad2a460c8e46bd90bfedef28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa15e15fb5174723ab5616fd589925db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ac2f2d7a5949452aac3a4d2a9f34e04a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 305584576,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 305584576,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b3bed97c2fca419ab002a30d5ad5d14c"
          }
        },
        "e963cc87aacc4e958697780504b741b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7251f864e70242bda6d241e20d1bd07e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 306M/306M [00:21&lt;00:00, 14.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a978baf429845739af411ce7111f160"
          }
        },
        "ac2f2d7a5949452aac3a4d2a9f34e04a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b3bed97c2fca419ab002a30d5ad5d14c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7251f864e70242bda6d241e20d1bd07e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a978baf429845739af411ce7111f160": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxoy885uTknc"
      },
      "source": [
        "IMPORTING NECESSARY LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VgdbdDt66LI",
        "outputId": "51f19df3-674e-47c5-d577-514f904c1a4e"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH9AEpXvep1i",
        "outputId": "32a97166-e4ab-4d47-bfab-bcb98acaefe8"
      },
      "source": [
        "#We will use arxiv to extract abstracts from pdfs\n",
        "\n",
        "\n",
        "!pip install arxiv\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting arxiv\n",
            "  Downloading https://files.pythonhosted.org/packages/63/cb/01bb646298dd4646277d0b366b6f50001cb2971b0fc2d3879cc5fce01097/arxiv-1.2.0-py3-none-any.whl\n",
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.7MB/s \n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=3fb520a665138fa3936c5ab8c014668486eb4039073f3df4867f7be6cd6e1c76\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-1.2.0 feedparser-6.0.2 sgmllib3k-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WITT3M8Nep4v"
      },
      "source": [
        "import arxiv\n",
        "import os\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXKe5aX866fF",
        "outputId": "8e9f8e11-656a-45a9-b741-abea76d66b1f"
      },
      "source": [
        "!pip install tika"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tika\n",
            "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika) (56.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (3.0.4)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-cp37-none-any.whl size=32885 sha256=62fb3c88880cb6ecf055ae18cf25a98b6383faa8e95ed30509b8fba063e0f536\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6t4czxwATpJ"
      },
      "source": [
        "from tika import parser\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzkRVXDchNw4"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey_ihXKg6lae"
      },
      "source": [
        "def Reverse(lst): \n",
        "    return [ele for ele in reversed(lst)] \n",
        "      \n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6xOcydLT3QC"
      },
      "source": [
        "CLEANING AND CALCULATING THE EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ_WGG01HbPS"
      },
      "source": [
        "#we will give path of the drive where there are pdfs\n",
        "\n",
        "path = '/content/drive/MyDrive/100_2/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwb_dS25HbPc",
        "outputId": "4984537d-bd38-4fdf-a5d0-7f205c5b500e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT6iPuvmHbPe"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "onlyfiles2 = [f for f in listdir(path) if isfile(join(path, f))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH4mrGH5HbPe",
        "outputId": "28c41d6d-9717-4688-c90e-c59048b1b0e1"
      },
      "source": [
        "onlyfiles2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['summarization (42).pdf',\n",
              " 'summarization (43).pdf',\n",
              " 'summarization (44).pdf',\n",
              " 'summarization (45).pdf',\n",
              " 'summarization (46).pdf',\n",
              " 'summarization (47).pdf',\n",
              " 'summarization (48).pdf',\n",
              " 'summarization (49).pdf',\n",
              " 'summarization (50).pdf',\n",
              " 'sentiment_analysis (35).pdf',\n",
              " 'sentiment_analysis (36).pdf',\n",
              " 'sentiment_analysis (37).pdf',\n",
              " 'sentiment_analysis (38).pdf',\n",
              " 'sentiment_analysis (39).pdf',\n",
              " 'sentiment_analysis (40).pdf',\n",
              " 'sentiment_analysis (41).pdf',\n",
              " 'sentiment_analysis (42).pdf',\n",
              " 'sentiment_analysis (43).pdf',\n",
              " 'sentiment_analysis (44).pdf',\n",
              " 'document_classification (6).pdf',\n",
              " 'document_classification (7).pdf',\n",
              " 'document_classification (8).pdf',\n",
              " 'document_classification (9).pdf',\n",
              " 'document_classification (10).pdf',\n",
              " 'document_classification (30).pdf',\n",
              " 'document_classification (31).pdf',\n",
              " 'document_classification (32).pdf',\n",
              " 'document_classification (33).pdf',\n",
              " 'document_classification (34).pdf',\n",
              " 'document_classification (35).pdf',\n",
              " 'document_classification (36).pdf',\n",
              " 'document_classification (1).pdf',\n",
              " 'document_classification (2).pdf',\n",
              " 'document_classification (3).pdf',\n",
              " 'document_classification (4).pdf',\n",
              " 'document_classification (5).pdf',\n",
              " 'document_classification (37).pdf',\n",
              " 'document_classification (38).pdf',\n",
              " 'document_classification (39).pdf',\n",
              " 'topic modeling (10).pdf',\n",
              " 'topic modeling (11).pdf',\n",
              " 'topic modeling (12).pdf',\n",
              " 'topic modeling (13).pdf',\n",
              " 'topic modeling (14).pdf',\n",
              " 'topic modeling (15).pdf',\n",
              " 'topic modeling (16).pdf',\n",
              " 'topic modeling (17).pdf',\n",
              " 'topic modeling (18).pdf',\n",
              " 'topic modeling (19).pdf',\n",
              " 'summarization (51).pdf',\n",
              " 'text_and_speech_processing (6).pdf',\n",
              " 'text_and_speech_processing (5).pdf',\n",
              " 'text_and_speech_processing (4).pdf',\n",
              " 'text_and_speech_processing (3).pdf',\n",
              " 'text_and_speech_processing (2).pdf',\n",
              " 'text_and_speech_processing (1).pdf',\n",
              " 'text_and_speech_processing (10).pdf',\n",
              " 'text_and_speech_processing (9).pdf',\n",
              " 'text_and_speech_processing (8).pdf',\n",
              " 'text_and_speech_processing (7).pdf',\n",
              " 'relation_extraction (3).pdf',\n",
              " 'relation_extraction (4).pdf',\n",
              " 'relation_extraction (2).pdf',\n",
              " 'relation_extraction (1).pdf',\n",
              " 'relation_extraction (10).pdf',\n",
              " 'relation_extraction (9).pdf',\n",
              " 'relation_extraction (8).pdf',\n",
              " 'relation_extraction (7).pdf',\n",
              " 'relation_extraction (6).pdf',\n",
              " 'relation_extraction (5).pdf',\n",
              " 'semantic_analysis (9).pdf',\n",
              " 'semantic_analysis (8).pdf',\n",
              " 'machine_translation (4).pdf',\n",
              " 'machine_translation (3).pdf',\n",
              " 'machine_translation (2).pdf',\n",
              " 'machine_translation (1).pdf',\n",
              " 'machine_translation (10).pdf',\n",
              " 'machine_translation (9).pdf',\n",
              " 'machine_translation (8).pdf',\n",
              " 'machine_translation (7).pdf',\n",
              " 'machine_translation (6).pdf',\n",
              " 'machine_translation (5).pdf']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo5kQlMSHbPg",
        "outputId": "cfb8be5b-d106-4116-fc92-dc8a80d83cfc"
      },
      "source": [
        "#this is the code to extract abstracts/ titles from a pdf, We will simply first separate every line of the pdf and then find the Abstract and the paper name (title). Also we can find further stuff\n",
        "\n",
        "myabstract=[]\n",
        "topic_names=[]\n",
        "file_names=[]\n",
        "for j in onlyfiles2:\n",
        "  file_names.append(j)\n",
        "  parsed_pdf = parser.from_file(os.path.join(path, j))\n",
        "  data = parsed_pdf['content']\n",
        "  \n",
        "  # Printing of content  \n",
        "  with open('text.txt','w+') as pr:\n",
        "    for key in parsed_pdf['content']:\n",
        "      \n",
        "      pr.write(key)\n",
        "  strt=\"\"\n",
        "  with open('text.txt','r+') as f:\n",
        "    strt+=f.read()\n",
        "  contents = strt.split('\\n')\n",
        "  indexes=[]\n",
        "  try:\n",
        "    for i in range(len(contents)):\n",
        "      if contents[i].find('Abstract') >= 0 or contents[i].find('ABSTRACT') >= 0:\n",
        "        indexes.append(i)\n",
        "    #indexes.append(contents.index('Abstract'))\n",
        "    # for i in range(len(contents)):\n",
        "    #   if contents[i].find('INTRODUCTION') >= 0 or contents[i].find('Introduction') >= 0:\n",
        "    #     indexes.append(i)\n",
        "    \n",
        "    mydata = contents[indexes[0]:indexes[0]+300]\n",
        "    mydata = '\\n'.join(mydata)\n",
        "    myabstract.append(mydata)\n",
        "    \n",
        "\n",
        "    for i in contents:\n",
        "      if len(i)>1:\n",
        "        topic_names.append(i)\n",
        "        file_names.append(j)\n",
        "        break\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  \n",
        "dict={}\n",
        "#dict['file'] = file_list\n",
        "dict['topic'] = topic_names\n",
        "dict['abstract'] = myabstract"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-08 14:27:29,379 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to /tmp/tika-server.jar.\n",
            "2021-04-08 14:27:30,379 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "2021-04-08 14:27:30,796 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyjGBzjVHbPh"
      },
      "source": [
        "#from the above code we get a dictionary and then convert it into a dataframe\n",
        "\n",
        "data2 = pd.DataFrame(dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ppbu8itZsyW"
      },
      "source": [
        "#Use this line of code separately. (If we are using any third party data) \n",
        "\n",
        "data2 = pd.read_csv('test_set_3.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "V9Zhmt3XI-0H",
        "outputId": "fe5202b9-a20c-466f-d1f3-47bbc05b971e"
      },
      "source": [
        "data2.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>(Probably) Concave Graph Matching</td>\n",
              "      <td>In this paper we address the graph matching pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3DAware Scene Manipulation via Inverse Graphics</td>\n",
              "      <td>We aim to obtain an interpretable, expressive,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A Block Coordinate Ascent Algorithm for MeanVa...</td>\n",
              "      <td>Risk management in dynamic decision problems i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>A Deep Bayesian Policy Reuse Approach Against ...</td>\n",
              "      <td>In multiagent domains, coping with non-station...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>A flexible model for training action localizat...</td>\n",
              "      <td>Spatio-temporal action detection in videos is ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                         abstract  \n",
              "0           0  ...  In this paper we address the graph matching pr...\n",
              "1           1  ...  We aim to obtain an interpretable, expressive,...\n",
              "2           2  ...  Risk management in dynamic decision problems i...\n",
              "3           3  ...  In multiagent domains, coping with non-station...\n",
              "4           4  ...  Spatio-temporal action detection in videos is ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA6XKnpf66kL"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemma = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr6a0-nI66nf"
      },
      "source": [
        "# This code cleans the abstracts with necessary libraries\n",
        "\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"lxml\").text\n",
        "    #text = re.sub(r'http\\S+','',text)\n",
        "    text = re.sub('[^a-zA-Z]',' ',text)\n",
        "    #text = re.sub(r'\\s{2}','',text)\n",
        "    text = str(text).lower()\n",
        "    if '.' in str(text):\n",
        "      text = str(text).replace('.',' ')\n",
        "    if ',' in str(text):\n",
        "      text = str(text).replace(',',' ')\n",
        "    text = re.sub(r'abst\\S+','',text)\n",
        "    text = word_tokenize(text)\n",
        "    text = [item for item in text if item not in stop_words]\n",
        "    text = [i for i in text if len(i) > 2]\n",
        "    # text = [stemmer.stem(i) for i in text]\n",
        "    text = [lemma.lemmatize(word=w,pos='v') for w in text]\n",
        "    text = ' '.join(text)\n",
        "\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1W2yH1vHbPi"
      },
      "source": [
        "data2['cleaned_text'] = data2.iloc[:,2].apply(clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjvFNHc-I-IF",
        "outputId": "93aa7c71-508a-450c-bc95-a0cecbbda792"
      },
      "source": [
        "data2['cleaned_text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      paper address graph match problem follow recen...\n",
              "1      aim obtain interpretable expressive disentangl...\n",
              "2      risk management dynamic decision problems prim...\n",
              "3      multiagent domains cop non stationary agents c...\n",
              "4      spatio temporal action detection videos typica...\n",
              "                             ...                        \n",
              "287    introduce new efficient principled backpropaga...\n",
              "288    infer intent observe behavior study extensivel...\n",
              "289    give rigorous analysis statistical behavior gr...\n",
              "290    humans make repeat choices among options imper...\n",
              "291    object orient representations reinforcement le...\n",
              "Name: cleaned_text, Length: 292, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHvB-XYSbj1S",
        "outputId": "6a3d9fbc-a07d-45f5-b112-1299eb2fa6dc"
      },
      "source": [
        "len(data2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "292"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UEd5bupHbPj"
      },
      "source": [
        "data_list2 = []\n",
        "for i in data2['cleaned_text']:\n",
        "  data_list2.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhVFYAKVV9Kh"
      },
      "source": [
        "topic_list2 = []\n",
        "for i in data2['title']:\n",
        "  topic_list2.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVrJaftzHbPj"
      },
      "source": [
        "#finding abstracts which are of very small length and then removing them from the data\n",
        "\n",
        "count=0\n",
        "my_list=[]\n",
        "for i in data_list2:\n",
        "  if len(i) < 40:\n",
        "    my_list.append(count)\n",
        "    count+=1\n",
        "  else:\n",
        "    count+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hNwnh_2WRQC",
        "outputId": "14f03448-dbec-4bcb-b941-d5fa7be61b0f"
      },
      "source": [
        "my_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVoULPsIHbPk"
      },
      "source": [
        "data_list2 = [i for j, i in enumerate(data_list2) if j not in my_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWJlZBaYWMe4"
      },
      "source": [
        "topic_list2 = [i for j, i in enumerate(topic_list2) if j not in my_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDOcwTHUBW2M",
        "outputId": "f6b5dbc0-a8d2-4624-f6ce-c4a53a4f1c11"
      },
      "source": [
        "data_list2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['paper address graph match problem follow recent work cite zaslavskiy path vestner analyze generalize idea concave relaxations introduce concepts emph conditionally concave emph probably conditionally concave energies polytopes show encapsulate many instance graph match problem include match euclidean graph graph surface prove local minima probably conditionally concave energies general match polytopes doubly stochastic high probability extreme point match polytope permutations',\n",
              " 'aim obtain interpretable expressive disentangle scene representation contain comprehensive structural textural information object previous scene representations learn neural network often uninterpretable limit single object lack knowledge work propose scene render network sdn address issue integrate disentangle representations semantics geometry appearance deep generative model scene encoder perform inverse graphics translate scene structure object wise representation decoder two components differentiable shape renderer neural texture generator disentanglement semantics geometry appearance support aware scene manipulation rotate move object freely keep consistent shape texture change object appearance without affect shape experiment demonstrate edit scheme base sdn superior counterpart',\n",
              " 'risk management dynamic decision problems primary concern many field include financial investment autonomous drive healthcare mean variance function one widely use objective function risk management due simplicity interpretability exist algorithms mean variance optimization base multi time scale stochastic approximation whose learn rate schedule often hard tune asymptotic convergence proof paper develop model free policy search framework mean variance optimization finite sample error bind analysis local optima start point reformulation original mean variance function fenchel dual propose stochastic block coordinate ascent policy search algorithm asymptotic convergence guarantee last iteration solution convergence rate randomly pick solution provide applicability demonstrate several benchmark domains',\n",
              " 'multiagent domains cop non stationary agents change behaviors time time challenge problem agent usually require able quickly detect agent policy online interaction adapt policy accordingly paper study efficient policy detect reuse techniques play non stationary agents markov game propose new deep bpr algorithm extend recent bpr algorithm neural network value function approximator detect policy accurately propose textit rectify belief model take advantage textit opponent model infer agent policy reward signal behaviors instead directly store individual policies bpr introduce textit distil policy network serve policy library bpr use policy distillation achieve efficient online policy learn reuse deep bpr inherit advantage bpr empirically show better performance term detection accuracy cumulative reward speed convergence compare exist algorithms complex markov game raw visual input',\n",
              " 'spatio temporal action detection videos typically address fully supervise setup manual annotation train videos require every frame since annotation extremely tedious prohibit scalability clear need minimize amount manual supervision work propose unify framework handle combine vary type less demand weak supervision model base discriminative cluster integrate different type supervision constraints optimization investigate applications model train setups alternative supervisory signal range video level class label temporal point sparse action bound box full per frame annotation action bound box experiment challenge ucf daly datasets demonstrate competitive performance method fraction supervision use previous methods flexibility model enable joint learn data different level annotation experimental result demonstrate significant gain add fully supervise examples otherwise weakly label videos',\n",
              " 'introduce game theoretic approach study recommendation systems strategic content providers systems fair stable show traditional approach fail satisfy requirements propose shapley mediator show shapley mediator satisfy fairness stability requirements run linear time economically efficient mechanism satisfy properties',\n",
              " 'propose novel adaptive test goodness fit computational cost linear number sample learn test feature best indicate differences observe sample reference model minimize false negative rate feature construct via stein method mean necessary compute normalise constant model analyse asymptotic bahadur efficiency new test prove mean shift alternative test always greater relative efficiency previous linear time kernel test regardless choice parameters test experiment performance method exceed earlier linear time test match exceed power quadratic time kernel test high dimension model structure may exploit goodness fit test perform far better quadratic time two sample test base maximum mean discrepancy sample draw model',\n",
              " 'give sample probability distribution anomaly detection problem determine give point lie low density region paper concern calibrate anomaly detection practically relevant extension additionally wish produce confidence score point anomalous build classification framework anomaly detection show minimisation suitably modify proper loss produce density estimate anomalous instance show incorporate quantile control relate objective generalise version pinball loss finally show efficiently optimise objective kernelised scorer leverage recent result point process literature result objective capture close relative one class svm special case',\n",
              " 'recent work suggest enhance bloom filter use pre filter base apply machine learn determine function model data set bloom filter mean represent model learn bloom filter follow outcomes clarify guarantee associate structure show estimate size learn function must obtain order obtain improve performance provide simple method sandwich optimize learn bloom filter propose design analysis approach learn bloomier filter base model approach',\n",
              " 'mainstream caption model often follow sequential structure generate cap tions lead issue introduction irrelevant semantics lack diversity generate caption inadequate generalization performance paper present alternative paradigm image caption factorize caption procedure two stag extract explicit semantic representation give image construct caption base recursive compositional procedure bottom manner compare conventional ones paradigm better preserve semantic content explicit factorization semantics syntax use compositional generation procedure caption construction follow recursive structure naturally fit properties human language moreover propose compositional procedure require less data train generalize better yield diverse caption',\n",
              " 'study classic mean median cluster fundamental problems unsupervised learn set data partition across multiple sit allow discard small portion data label outliers propose simple approach base construct small summary original dataset propose method time communication efficient good approximation guarantee identify global outliers effectively best knowledge first practical algorithm theoretical guarantee distribute cluster outliers experiment real synthetic data demonstrate clear superiority algorithm baseline algorithms almost metrics',\n",
              " 'sequential prediction problems imitation learn future observations depend previous predictions action violate common assumptions make statistical learn lead poor performance theory often practice recent approach provide stronger guarantee set remain somewhat unsatisfactory train either non stationary stochastic policies require large number iterations paper propose new iterative algorithm train stationary deterministic policy see regret algorithm online learn set show regret algorithm combine additional reduction assumptions must find policy good performance distribution observations induce sequential settings demonstrate new approach outperform previous approach two challenge imitation learn problems benchmark sequence label problem',\n",
              " 'relational reason central component generally intelligent behavior prove difficult neural network learn paper describe use relation network rns simple plug play module solve problems fundamentally hinge relational reason test augment network three task visual question answer use challenge dataset call clevr achieve state art super human performance text base question answer use babi suite task complex reason dynamic physical systems use curated dataset call sort clevr show powerful convolutional network general capacity solve relational question gain capacity augment rns work show deep learn architecture equip module implicitly discover learn reason entities relations',\n",
              " 'bandit learn characterize tension long term exploration short term exploitation however recently note settings choices learn algorithm correspond important decisions individual people criminal recidivism prediction lend sequential drug trials exploration correspond explicitly sacrifice well one individual potential future benefit others settings one might like run greedy algorithm always make optimal decision individuals hand result catastrophic failure learn paper consider linear contextual bandit problem revisit performance greedy algorithm',\n",
              " 'probabilistic generative model provide powerful framework represent data avoid expense manual annotation typically need discriminative approach model selection generative set challenge however particularly likelihoods easily accessible address issue introduce statistical test relative similarity use determine two model generate sample significantly closer real world reference dataset interest use test statistic difference maximum mean discrepancies mmds reference dataset model dataset derive powerful low variance test base joint asymptotic distribution mmds reference model pair experiment deep generative model include variational auto encoder generative moment match network test provide meaningful rank model performance function parameter train settings',\n",
              " 'present novel unify deep learn framework capable learn domain invariant representation data across multiple domains realize adversarial train additional ability exploit domain specific information propose network able perform continuous cross domain image translation manipulation produce desirable output image accordingly addition result feature representation exhibit superior performance unsupervised domain adaptation also verify effectiveness propose model learn disentangle feature describe cross domain data',\n",
              " 'learn capture long range relations fundamental image video recognition exist cnn model generally rely increase depth model relations highly inefficient work propose double attention block novel component aggregate propagate informative global feature entire spatio temporal space input image videos enable subsequent convolution layer access feature entire space efficiently component design double attention mechanism two step first step gather feature entire space compact set second order attention pool second step adaptively select distribute feature location via another attention propose double attention block easy adopt plug exist deep neural network conveniently conduct extensive ablation study experiment image video recognition task evaluate performance image recognition task resnet equip double attention block outperform much larger resnet architecture imagenet dataset less number parameters less flop action recognition task propose model achieve state art result kinetics ucf datasets significantly higher efficiency recent work',\n",
              " 'present first accelerate randomize algorithm solve linear systems euclidean space one essential problem type matrix inversion problem particular algorithm specialize invert positive definite matrices way iterate approximate solutions generate algorithm positive definite matrices open way many applications field optimization machine learn application general theory develop first accelerate deterministic stochastic quasi newton update update lead provably aggressive approximations inverse hessian lead speed up classical non accelerate rule numerical experiment experiment empirical risk minimization show rule accelerate train machine learn model',\n",
              " 'stochastic gradient descent popular large scale optimization slow convergence asymptotically due inherent variance remedy problem introduce explicit variance reduction method stochastic gradient descent call stochastic variance reduce gradient svrg smooth strongly convex function prove method enjoy fast convergence rate stochastic dual coordinate ascent sdca stochastic average gradient sag however analysis significantly simpler intuitive moreover unlike sdca sag method require storage gradients thus easily applicable complex problems structure prediction problems neural network learn',\n",
              " 'active learn task use label data select additional point label goal fit accurate model fix budget label point binary classification active learn know produce faster rat passive learn broad range settings however regression restrictive structure tailor methods previously need obtain theoretically superior performance paper propose intuitive tree base active learn algorithm non parametric regression provable improvement random sample implement mondrian tree algorithm tune parameter free consistent minimax optimal lipschitz function',\n",
              " 'focus machine learn branch beyond train classifiers single task investigate previously acquire knowledge source domain leverage facilitate learn relate target domain know inductive transfer learn three active line research independently explore transfer learn use neural network weight transfer model train source domain use initialization point network train target domain deep metric learn source domain use construct embed capture class structure source target domains shoot learn focus generalize well target domain base limit number label examples compare state art methods three paradigms also explore hybrid adapt embed methods use limit target domain data fine tune embeddings construct source domain data conduct systematic comparison methods variety domains vary number label instance available target domain well number target domain class reach three principal conclusions deep embeddings far superior compare weight transfer start point inter domain transfer model use hybrid methods robustly outperform every shoot learn every deep metric learn method previously propose mean error reduction state art among loss function discover embeddings histogram loss ustinova lempitsky robust hope result motivate unification research weight transfer deep metric learn shoot learn',\n",
              " 'machine learn model vulnerable adversarial examples small change image cause computer vision model make mistake identify school bus ostrich however still open question whether humans prone similar mistake address question leverage recent techniques transfer adversarial examples computer vision model know parameters architecture model unknown parameters architecture match initial process human visual system find adversarial examples strongly transfer across computer vision model influence classifications make time limit human observers',\n",
              " 'great progress make recently automatic image manipulation limit object centric image like face structure scene datasets work take step towards general scene level image edit develop automatic interaction free object removal model model learn find remove object general scene image use image level label unpaired data generative adversarial network gin framework achieve two key contributions two stage editor architecture consist mask generator image painter operate remove object novel gin base prior mask generator allow flexibly incorporate knowledge object shape experimentally show two datasets method effectively remove wide variety object use weak supervision',\n",
              " 'despite achieve impressive performance state art classifiers remain highly vulnerable small imperceptible adversarial perturbations vulnerability prove empirically intricate address paper study phenomenon adversarial perturbations assumption data generate smooth generative model derive fundamental upper bound robustness perturbations classification function prove existence adversarial perturbations transfer well across different classifiers small risk analysis robustness also provide insights onto key properties generative model smoothness dimensionality latent space conclude numerical experimental result show bound provide informative baselines maximal achievable robustness several datasets',\n",
              " 'algorithmically construct multi output gaussian process priors satisfy linear differential equations approach attempt parametrize solutions equations use bner base successful push forward gaussian process along paramerization desire prior consider several examples physics geomathmatics control among full inhomogeneous system maxwell equations bring together stochastic learn computeralgebra novel way combine noisy observations precise algebraic computations',\n",
              " 'learn decision tree data difficult optimization problem widespread algorithm practice date base greedy growth tree structure recursively split nod possibly prune back final tree parameters decision function internal node approximately estimate minimize impurity measure give algorithm give input tree structure parameter value nod produce new tree smaller structure new parameter value provably lower leave unchanged misclassification error apply axis align oblique tree experiment show consistently outperform various algorithms highly scalable large datasets tree algorithm handle sparsity penalty learn sparse oblique tree structure subset original tree nonzero parameters combine best axis align oblique tree flexibility model correlate data low generalization error fast inference interpretable nod involve feature decision',\n",
              " 'study generalization classic isotonic regression problem allow separable nonconvex objective function focus case estimators use robust regression simple dynamic program approach allow solve problem within accuracy global minimum time linear dimension combine techniques convex case branch bind ideas form new algorithm problem naturally exploit shape objective function algorithm achieve best bound general nonconvex convex case linear log perform much faster practice straightforward dynamic program approach especially desire accuracy increase',\n",
              " 'ideas enjoy large impact deep learn convolution problem involve pixels spatial representations common intuition hold convolutional neural network may appropriate paper show strike counterexample intuition via seemingly trivial coordinate transform problem simply require learn map coordinate cartesian space coordinate one hot pixel space although convolutional network would seem appropriate task show fail spectacularly demonstrate carefully analyze failure first toy problem point simple fix become obvious call solution coordconv work give convolution access input coordinate use extra coordinate channel without sacrifice computational parametric efficiency ordinary convolution coordconv allow network learn either complete translation invariance vary degrees translation dependence require end task coordconv solve coordinate transform problem perfect generalization time faster time fewer parameters convolution stark contrast raise question extent inability convolution persist insidiously inside task subtly hamper performance within complete answer question require investigation show preliminary evidence swap convolution coordconv improve model diverse set task use coordconv gin produce less mode collapse transform high level spatial latents pixels become easier learn faster cnn detection model train mnist detection show better iou use coordconv reinforcement learn domain agents play atari game benefit significantly use coordconv layer',\n",
              " 'policy gradient methods widely use control reinforcement learn particularly continuous action set host theoretically sound algorithms propose policy set due existence policy gradient theorem provide simplify form gradient policy learn however behaviour policy necessarily attempt learn follow optimal policy give task existence theorem elusive work solve open problem provide first policy policy gradient theorem key derivation use emphatic weight develop new actor critic algorithm call actor critic emphatic weight ace approximate simplify gradients provide theorem demonstrate simple counterexample previous policy policy gradient methods particularly offpac dpg converge wrong solution whereas ace find optimal solution',\n",
              " 'paper try organize machine teach coherent set ideas idea present vary along dimension collection dimension form problem space machine teach exist teach problems characterize space hope organization allow gain deeper understand individual teach problems discover connections among identify gap field',\n",
              " 'goal orient dialog give attention due numerous applications artificial intelligence goal orient dialogue task occur questioner ask action orient question answerer respond intent let questioner know correct action take ask adequate question deep learn reinforcement learn recently apply however approach struggle find competent recurrent neural questioner owe complexity learn series sentence motivate theory mind propose answerer questioner mind aqm novel information theoretic algorithm goal orient dialog aqm questioner ask infer base approximate probabilistic model answerer questioner figure answerer intention via select plausible question explicitly calculate information gain candidate intentions possible answer question test framework two goal orient visual dialog task mnist count dialog guesswhat experiment aqm outperform comparative algorithms large margin',\n",
              " 'generative adversarial network gin powerful subclass generative model despite rich research activity lead numerous interest gin algorithms still hard assess algorithm perform better others conduct neutral multi faceted large scale empirical study state art model evaluation measure find model reach similar score enough hyperparameter optimization random restart suggest improvements arise higher computational budget tune fundamental algorithmic change overcome limitations current metrics also propose several data set precision recall compute experimental result suggest future gin research base systematic objective evaluation procedures finally find evidence test algorithms consistently outperform non saturate gin introduce cite goodfellow generative',\n",
              " 'residual network resnet standard deep neural net architecture state art performance across numerous applications main premise resnets allow train layer focus fit residual previous layer output target output thus expect train network worse obtain remove residual layer train shallower network instead however due non convexity optimization problem clear resnets indeed achieve behavior rather get stick arbitrarily poor local minimum paper rigorously prove arbitrarily deep nonlinear residual units indeed exhibit behavior sense optimization landscape contain local minima value obtain linear predictor namely layer network notably show minimal assumptions precise network architecture data distribution loss function use also provide quantitative analysis approximate stationary point problem finally show certain tweak architecture train network standard stochastic gradient descent achieve objective value close better linear predictor',\n",
              " 'convolutional long short term memory lstm network widely use action gesture recognition different attention mechanisms also embed lstm convolutional lstm convlstm network base previous gesture recognition architectures combine three dimensional convolution neural network dcnn convlstm paper explore effect attention mechanism convlstm several variants convlstm evaluate remove convolutional structure three gate convlstm apply attention mechanism input convlstm reconstruct input output gate respectively modify channel wise attention mechanism evaluation result demonstrate spatial convolutions three gate scarcely contribute spatiotemporal feature fusion attention mechanisms embed input output gate improve feature fusion word convlstm mainly contribute temporal fusion along recurrent step learn long term spatiotemporal feature take input spatial spatiotemporal feature basis new variant lstm derive convolutional structure embed input state transition lstm code lstm variants publicly available',\n",
              " 'dominant sequence transduction model base complex recurrent convolutional neural network encoder decoder configuration best perform model also connect encoder decoder attention mechanism propose new simple network architecture transformer base solely attention mechanisms dispense recurrence convolutions entirely experiment two machine translation task show model superior quality parallelizable require significantly less time train model achieve bleu wmt english german translation task improve exist best result include ensembles bleu wmt english french translation task model establish new single model state art bleu score train days eight gpus small fraction train cost best model literature show transformer generalize well task apply successfully english constituency parse large limit train data',\n",
              " 'perform efficient inference learn direct probabilistic model presence continuous latent variables intractable posterior distributions large datasets introduce stochastic variational inference learn algorithm scale large datasets mild differentiability condition even work intractable case contributions two fold first show reparameterization variational lower bind yield lower bind estimator straightforwardly optimize use standard stochastic gradient methods second show datasets continuous latent variables per datapoint posterior inference make especially efficient fit approximate inference model also call recognition model intractable posterior use propose lower bind estimator theoretical advantage reflect experimental result',\n",
              " 'consider problem generate automatic code give sample input output pair train neural network map current state output program next statement neural network optimize multiple task concurrently next operation set high level command operands next statement variables drop memory use method able create program twice long exist state art solutions improve success rate comparable lengths cut run time two order magnitude code include implementation various literature baselines publicly available https github com amitz pccoder',\n",
              " 'train deep neural network complicate fact distribution layer input change train parameters previous layer change slow train require lower learn rat careful parameter initialization make notoriously hard train model saturate nonlinearities refer phenomenon internal covariate shift address problem normalize layer input method draw strength make normalization part model architecture perform normalization train mini batch batch normalization allow use much higher learn rat less careful initialization also act regularizer case eliminate need dropout apply state art image classification model batch normalization achieve accuracy time fewer train step beat original model significant margin use ensemble batch normalize network improve upon best publish result imagenet classification reach top validation error test error exceed accuracy human raters',\n",
              " 'real world image recognition often challenge variability visual style include object textures light condition filter effect etc although variations deem implicitly handle train data deeper network recent advance image style transfer suggest also possible explicitly manipulate style information extend idea general visual recognition problems present batch instance normalization bin explicitly normalize unnecessary style image consider certain style feature play essential role discriminative task bin learn selectively normalize disturb style preserve useful style propose normalization module easily incorporate exist network architectures residual network surprisingly improve recognition performance various scenarios furthermore experiment verify bin effectively adapt completely different task like object classification style transfer control trade preserve remove style variations bin implement line code use popular deep learn frameworks',\n",
              " 'base non local prior distributions propose bayesian model selection bms procedure boundary detection sequence data multiple systematic mean change bms method effectively suppress non boundary spike point large instantaneous change speed algorithm reduce multiple change point series single change point detection problems establish consistency estimate number locations change point various prior distributions extensive simulation study conduct compare bms exist methods approach illustrate application magnetic resonance image guide radiation therapy data',\n",
              " 'long live autonomous agent able respond online novel instance task familiar domain act online require fast responses term rapid convergence especially task instance short duration applications involve interactions humans requirements problematic many establish methods learn act domains agent know task instance draw family relate task albeit without access label give instance choose act process policy reuse library rather policy learn scratch policy reuse agent prior knowledge class task form library policies learn sample task instance offline train phase formalise problem policy reuse present algorithm efficiently respond novel task instance reuse policy library exist policies choice base observe signal correlate policy performance achieve pose problem bayesian choice problem correspond notion optimal response computation response many case intractable therefore reduce computation cost posterior follow bayesian optimisation approach define set policy selection function balance exploration policy library exploitation previously try policies together model expect performance policy library correspond task instance validate method several simulate domains interactive short duration episodic task show rapid convergence unknown task variations',\n",
              " 'introduce temper geodesic markov chain monte carlo mcmc algorithm initialize pose graph optimization problems arise various scenarios sfm structure motion slam simultaneous localization map mcmc first kind unite global non convex optimization spherical manifold quaternions posterior sample order provide reliable initial pose uncertainty estimate informative quality solutions devise theoretical convergence guarantee extensively evaluate method synthetic real benchmarks besides elegance formulation theory show method robust miss data noise estimate uncertainties capture intuitive properties data',\n",
              " 'propose data efficient gaussian process base bayesian approach semi supervise learn problem graph propose model show extremely competitive performance compare state art graph neural network semi supervise learn benchmark experiment outperform neural network active learn experiment label scarce furthermore model require validation data set early stop control fit model view instance empirical distribution regression weight locally network connectivity motivate intuitive construction model bayesian linear model interpretation node feature filter operator relate graph laplacian method easily implement adapt shelf scalable variational inference algorithms gaussian process',\n",
              " 'attention network multimodal learn provide efficient way utilize give visual information selectively however computational cost learn attention distributions every pair multimodal input channel prohibitively expensive solve problem attention build two separate attention distributions modality neglect interaction multimodal input paper propose bilinear attention network ban find bilinear attention distributions utilize give vision language information seamlessly ban consider bilinear interactions among two group input channel low rank bilinear pool extract joint representations pair channel furthermore propose variant multimodal residual network exploit eight attention map ban efficiently quantitatively qualitatively evaluate model visual question answer vqa flickr entities datasets show ban significantly outperform previous methods achieve new state arts datasets',\n",
              " 'stochastic gradient method minimize objective function compose large number differentiable function solve stochastic optimization problem moderate accuracy block coordinate descent update bcd method hand handle problems multiple block variables update one time block variables easier update individually together bcd lower per iteration cost paper introduce method combine feature bcd problems many components objective multiple block variables specifically block stochastic gradient bsg method propose solve convex nonconvex program iteration bsg approximate gradient differentiable part objective randomly sample small set data sample function sum term objective use sample update block variables either deterministic randomly shuffle order convergence convex nonconvex case establish different sense convex case propose method order convergence rate method nonconvex case convergence establish term expect violation first order optimality condition propose method numerically test problems include stochastic least square logistic regression convex well low rank tensor recovery bilinear logistic regression nonconvex',\n",
              " 'propose sparse low rank tensor regression model relate univariate outcome feature tensor unit rank tensor decomposition coefficient tensor assume sparse structure parsimonious highly interpretable imply outcome relate feature distinct pathways may involve subsets feature dimension take divide conquer strategy simplify task set sparse unit rank tensor regression problems make computation efficient scalable unit rank tensor regression propose stagewise estimation procedure efficiently trace entire solution path show step size go zero stagewise solution paths converge exactly correspond regularize regression superior performance approach demonstrate various real world synthetic examples',\n",
              " 'approximate probability density tractable manner central task bayesian statistics variational inference popular technique achieve tractability choose relatively simple variational approximation borrow ideas classic boost framework recent approach attempt emph boost replace selection single density iteratively construct mixture densities order guarantee convergence previous work impose stringent assumptions require significant effort practitioners specifically require custom implementation greedy step call lmo every probabilistic model respect unnatural variational family truncate distributions work fix issue novel theoretical algorithmic insights theoretical side show boost satisfy relax smoothness assumption sufficient convergence functional frank wolfe algorithm furthermore rephrase lmo problem propose maximize residual elbo relbo replace standard elbo optimization theoretical enhancements allow black box implementation boost subroutine finally present stop criterion draw duality gap classic analyse exhaustive experiment illustrate usefulness theoretical algorithmic contributions',\n",
              " 'top visual attention mechanisms use extensively image caption visual question answer vqa enable deeper image understand fine grain analysis even multiple step reason work propose combine bottom top attention mechanism enable attention calculate level object salient image regions natural basis attention consider within approach bottom mechanism base faster cnn propose image regions associate feature vector top mechanism determine feature weight apply approach image caption result mscoco test server establish new state art task achieve cider spice bleu score respectively demonstrate broad applicability method apply approach vqa obtain first place vqa challenge',\n",
              " 'paper address mode collapse generative adversarial network gans view modes geometric structure data distribution metric space geometric lens embed subsamples dataset arbitrary metric space space preserve pairwise distance distribution metric embed determine dimensionality latent space automatically also enable construct mixture gaussians draw latent space random vectors use gaussian mixture model tandem simple augmentation objective function train gans every major step method support theoretical analysis experiment real synthetic data confirm generator able produce sample spread modes avoid unwanted sample outperform several recent gin variants number metrics offer new feature',\n",
              " 'reason play essential role visual question answer vqa multi step dynamic reason often necessary answer complex question example question place next bus right picture talk compound object bus right generate relation',\n",
              " 'present learn base approach compute solutions certain hard problems approach combine deep learn techniques useful algorithmic elements classic heuristics central component graph convolutional network train estimate likelihood vertex graph whether vertex part optimal solution network design train synthesize diverse set solutions enable rapid exploration solution space via tree search present approach evaluate four canonical hard problems five datasets include benchmark satisfiability problems real social network graph hundred thousand nod experimental result demonstrate present approach substantially outperform recent deep learn work perform par highly optimize state art heuristic solvers hard problems experiment indicate approach generalize across datasets scale graph order magnitude larger use train',\n",
              " 'present novel newton type method distribute optimization particularly well suit stochastic optimization learn problems quadratic objectives method enjoy linear rate convergence provably emph improve data size require essentially constant number iterations reasonable assumptions provide theoretical empirical evidence advantage method compare approach one shoot parameter average admm',\n",
              " 'bilinear model show achieve impressive performance wide range visual task semantic segmentation fine grain recognition face recognition however bilinear feature high dimensional typically order hundreds thousands million make impractical subsequent analysis propose two compact bilinear representations discriminative power full bilinear representation thousand dimension compact representations allow back propagation classification errors enable end end optimization visual recognition system compact bilinear representations derive novel kernelized analysis bilinear pool provide insights discriminative power bilinear pool platform research compact pool methods experimentation illustrate utility propose representations image classification shoot learn across several datasets',\n",
              " 'new class dependent random measure call compound random measure propose use normalize versions random measure priors bayesian nonparametric mixture model consider tractability allow properties compound random measure normalize compound random measure derive particular show compound random measure construct gamma sigma stable generalize gamma process marginals also derive several form laplace exponent characterize dependence evy copula correlation function slice sampler augment olya urn scheme sampler describe posterior inference normalize compound random measure use mix measure nonparametric mixture model data example discuss',\n",
              " 'adversarial learn embed deep network learn disentangle transferable representations domain adaptation exist adversarial domain adaptation methods may struggle align different domains multimodal distributions native classification problems paper present conditional adversarial domain adaptation principled framework condition adversarial adaptation model discriminative information convey classifier predictions conditional domain adversarial network cdans design two novel condition strategies multilinear condition capture cross covariance feature representations classifier predictions improve discriminability entropy condition control uncertainty classifier predictions guarantee transferability experiment testify propose approach exceed state art result five benchmark datasets',\n",
              " 'connectionist temporal classification ctc objective function end end sequence learn adopt dynamic program algorithms directly learn map sequence ctc show promise result many sequence learn applications include speech recognition scene text recognition however ctc tend produce highly peaky overconfident distributions symptom overfitting remedy propose regularization method base maximum conditional entropy penalize peaky distributions encourage exploration also introduce entropy base prune method dramatically reduce number ctc feasible paths rule unreasonable alignments experiment scene text recognition show propose methods consistently improve ctc baseline without need adjust train settings code make publicly available https github com liuhu bigeye enctc crnn',\n",
              " 'introduce principled approach unsupervised structure learn deep neural network propose new interpretation depth inter layer connectivity conditional independencies input distribution encode hierarchically network structure thus depth network determine inherently propose method cast problem neural network structure learn problem bayesian network structure learn instead directly learn discriminative structure learn generative graph construct stochastic inverse construct discriminative graph prove conditional dependency relations among latent variables generative graph preserve class conditional discriminative graph demonstrate image classification benchmarks deepest layer convolutional dense common network replace significantly smaller learn structure maintain classification accuracy state art test benchmarks structure learn algorithm require small computational cost run efficiently standard desktop cpu',\n",
              " 'present unsupervised visual feature learn algorithm drive context base pixel prediction analogy auto encoders propose context encoders convolutional neural network train generate content arbitrary image region condition surround order succeed task context encoders need understand content entire image well produce plausible hypothesis miss part train context encoders experiment standard pixel wise reconstruction loss well reconstruction plus adversarial loss latter produce much sharper result better handle multiple modes output find context encoder learn representation capture appearance also semantics visual structure quantitatively demonstrate effectiveness learn feature cnn pre train classification detection segmentation task furthermore context encoders use semantic inpainting task either stand alone initialization non parametric methods',\n",
              " 'adapt ideas underlie success deep learn continuous action domain present actor critic model free algorithm base deterministic policy gradient operate continuous action space use learn algorithm network architecture hyper parameters algorithm robustly solve simulate physics task include classic problems cartpole swing dexterous manipulation legged locomotion car drive algorithm able find policies whose performance competitive find plan algorithm full access dynamics domain derivatives demonstrate many task algorithm learn policies end end directly raw pixel input',\n",
              " 'goal precipitation nowcasting predict future rainfall intensity local region relatively short period time previous study examine crucial challenge weather forecast problem machine learn perspective paper formulate precipitation nowcasting spatiotemporal sequence forecast problem input prediction target spatiotemporal sequence extend fully connect lstm lstm convolutional structure input state state state transition propose convolutional lstm convlstm use build end end trainable model precipitation nowcasting problem experiment show convlstm network capture spatiotemporal correlations better consistently outperform lstm state art operational rover algorithm precipitation nowcasting',\n",
              " 'holistic indoor scene understand refer jointly recover object bound box room layout iii camera pose exist methods either ineffective tackle problem partially paper propose end end model simultaneously solve three task real time give single rgb image essence propose method improve prediction parametrizing target box instead directly estimate target cooperative train across different modules contrast train modules individually specifically parametrize object bound box predictions several modules camera pose object attribute propose method provide two major advantage parametrization help maintain consistency image world thus largely reduce prediction variances coordinate constraints impose parametrization train different modules simultaneously call constraints cooperative losses enable joint train inference employ three cooperative losses bound box projections physical constraints estimate geometrically consistent physically plausible scene experiment sun rgb dataset show propose method significantly outperform prior approach layout estimation object detection camera pose estimation holistic scene understand',\n",
              " 'visual attention derive cognitive neuroscience facilitate human perception pertinent subset sensory data recently significant efforts make exploit attention scheme advance computer vision systems visual track often challenge track target object undergo large appearance change attention map facilitate visual track selectively pay attention temporal robust feature exist track detection approach mainly use additional attention modules generate feature weight classifiers equip mechanisms paper propose reciprocative learn algorithm exploit visual attention train deep classifiers propose algorithm consist fee forward backward operations generate attention map serve regularization term couple original classification loss function train deep classifier learn attend regions target object robust appearance change extensive experiment large scale benchmark datasets show propose attentive track method perform favorably state art approach',\n",
              " 'despite efficacy variety computer vision task deep neural network dnns vulnerable adversarial attack limit applications security critical systems recent work show possibility generate imperceptibly perturb image input adversarial examples fool well train dnn classifiers make arbitrary predictions address problem propose train recipe name deep defense core idea integrate adversarial perturbation base regularizer classification objective obtain model learn resist potential attack directly precisely whole optimization problem solve like train recursive network experimental result demonstrate method outperform train adversarial parseval regularizations large margins various datasets include mnist cifar imagenet different dnn architectures code model reproduce result available https github com ziangyan deepdefense pytorch',\n",
              " 'various semantic attribute segmentation mask geometric feature keypoints materials encode per point probe function geometries give collection relate shape consider jointly analyze probe function different shape discover common latent structure use neural network even absence correspondence information network train point cloud representations shape geometry associate semantic function point cloud function express share semantic understand shape coordinate way example segmentation task function indicator function arbitrary set shape part particular combination involve know network network able produce small dictionary basis function shape dictionary whose span include semantic function provide shape even though shape independent discretizations functional correspondences provide network able generate latent base consistent order reflect share semantic structure among shape demonstrate effectiveness technique various segmentation keypoint selection applications',\n",
              " 'replace output layer deep neural net typically softmax function novel interpolate function propose end end train test algorithms new architecture compare classical neural net softmax function output activation surrogate interpolate function output activation combine advantage deep manifold learn new framework demonstrate follow major advantage first better applicable case insufficient train data second significantly improve generalization accuracy wide variety network algorithm implement pytorch code available https github com baowangmath dnn datadependentactivation',\n",
              " 'deeper neural network difficult train present residual learn framework ease train network substantially deeper use previously explicitly reformulate layer learn residual function reference layer input instead learn unreferenced function provide comprehensive empirical evidence show residual network easier optimize gain accuracy considerably increase depth imagenet dataset evaluate residual net depth layer deeper vgg net still lower complexity ensemble residual net achieve error imagenet test set result place ilsvrc classification task also present analysis cifar layer depth representations central importance many visual recognition task solely due extremely deep representations obtain relative improvement coco object detection dataset deep residual net foundations submissions ilsvrc coco competitions also place task imagenet detection imagenet localization coco detection coco segmentation',\n",
              " 'rapid growth image video data web hash extensively study image video search recent years benefit recent advance deep learn deep hash methods achieve promise result image retrieval however limitations previous deep hash methods semantic information fully exploit paper develop deep supervise discrete hash algorithm base assumption learn binary cod ideal classification pairwise label information classification information use learn hash cod within one stream framework constrain output last layer binary cod directly rarely investigate deep hash algorithm discrete nature hash cod alternate minimization method use optimize objective function experimental result show method outperform current state art methods benchmark datasets',\n",
              " 'hash one popular powerful approximate nearest neighbor search techniques large scale image retrieval traditional hash methods first represent image shelf visual feature produce hash cod separate stage however shelf visual feature may optimally compatible hash code learn procedure may result sub optimal hash cod recently deep hash methods propose simultaneously learn image feature hash cod use deep neural network show superior performance traditional hash methods deep hash methods give supervise information form pairwise label triplet label current state art deep hash method dpsh cite feature base pairwise label perform image feature learn hash code learn simultaneously maximize likelihood pairwise similarities inspire dpsh cite feature propose triplet label base deep hash method aim maximize likelihood give triplet label experimental result show method outperform baselines cifar nus wide datasets include state art method dpsh cite feature previous triplet label base deep hash methods',\n",
              " 'accurate exposure key capture high quality photos computational photography especially mobile phone limit size camera modules inspire luminosity mask usually apply professional photographers paper develop novel algorithm learn local exposures deep reinforcement adversarial learn specific segment image sub image reflect variations dynamic range exposures accord raw low level feature base sub image local exposure sub image automatically learn virtue policy network sequentially reward learn globally design strike balance overall exposures aesthetic evaluation function approximate discriminator generative adversarial network reinforcement learn adversarial learn train collaboratively asynchronous deterministic policy gradient generative loss approximation simply algorithmic architecture also prove feasibility leverage discriminator value function employ local exposure retouch raw input image respectively thus deliver multiple retouch image different exposures fuse exposure blend extensive experiment verify algorithms superior state art methods term quantitative accuracy visual illustration',\n",
              " 'deep learn see remarkable developments last years many inspire neuroscience however main learn mechanism behind advance error backpropagation appear odds neurobiology introduce multilayer neuronal network model simplify dendritic compartments error drive synaptic plasticity adapt network towards global desire output contrast previous work model require separate phase synaptic learn drive local dendritic prediction errors continuously time errors originate apical dendrites occur due mismatch predictive input lateral interneurons activity actual top feedback use simple dendritic compartments different cell type model represent error normal activity within pyramidal neuron demonstrate learn capabilities model regression classification task show analytically approximate error backpropagation algorithm moreover framework consistent recent observations learn brain areas architecture cortical microcircuits overall introduce novel view learn dendritic cortical circuit brain may solve long stand synaptic credit assignment problem',\n",
              " 'recent work show convolutional network substantially deeper accurate efficient train contain shorter connections layer close input close output paper embrace observation introduce dense convolutional network densenet connect layer every layer fee forward fashion whereas traditional convolutional network layer connections one layer subsequent layer network direct connections layer feature map precede layer use input feature map use input subsequent layer densenets several compel advantage alleviate vanish gradient problem strengthen feature propagation encourage feature reuse substantially reduce number parameters evaluate propose architecture four highly competitive object recognition benchmark task cifar cifar svhn imagenet densenets obtain significant improvements state art whilst require less computation achieve high performance code pre train model available https github com liuzhuang densenet',\n",
              " 'exist methods interactive image retrieval demonstrate merit integrate user feedback improve retrieval result however current systems rely restrict form user feedback binary relevance responses feedback base fix set relative attribute limit impact paper introduce new approach interactive image search enable users provide feedback via natural language allow natural effective interaction formulate task dialog base interactive image retrieval reinforcement learn problem reward dialog system improve rank target image dialog turn mitigate cumbersome costly process collect human machine conversations dialog system learn train system user simulator train describe differences target candidate image efficacy approach demonstrate footwear retrieval application experiment simulate real world data show propose learn framework achieve better accuracy supervise reinforcement learn baselines user feedback base natural language rather pre specify attribute lead effective retrieval result natural expressive communication interface',\n",
              " 'deep neural network dnns recently show state art performance semantic segmentation task however still suffer problems poor boundary localization spatial fragment predictions difficulties lie requirement make dense predictions long path model since detail hard keep data go deeper layer instead work decompose difficult task two relative simple sub task seed detection require predict initial predictions without need wholeness preciseness similarity estimation measure possibility two nod belong class without need know class use one branch network one sub task apply cascade random walk base hierarchical semantics approximate complex diffusion process propagate seed information whole image accord estimate similarities propose difnet consistently produce improvements baseline model depth equivalent number parameters also achieve promise performance pascal voc pascal context dataset ourdifnet train end end without complex loss function',\n",
              " 'paper present keypointnet end end geometric reason framework learn optimal set category specific keypoints along detectors predict keypoints single input image demonstrate framework pose estimation task propose differentiable pose objective seek optimal set keypoints recover relative pose two view object network automatically discover consistent set keypoints across viewpoints single object well across object instance give object class importantly find end end approach use grind truth keypoint annotations outperform fully supervise baseline use neural network architecture pose estimation task discover keypoints across car chair plane categories shapenet visualize https keypoints github',\n",
              " 'channel prune one predominant approach deep model compression exist prune methods either train scratch sparsity constraints channel minimize reconstruction error pre train feature map compress ones strategies suffer limitations former kind computationally expensive difficult converge whilst latter kind optimize reconstruction error ignore discriminative power channel overcome drawbacks investigate simple yet effective method call discrimination aware channel prune choose channel really contribute discriminative power end introduce additional losses network increase discriminative power intermediate layer select discriminative channel layer consider additional loss reconstruction error last propose greedy algorithm conduct channel selection parameter optimization iterative way extensive experiment demonstrate effectiveness method example ilsvrc prune resnet reduction channel even outperform original model top accuracy',\n",
              " 'propose novel wasserstein method distillation mechanism yield joint learn word embeddings topics propose method base fact euclidean distance word embeddings may employ underlie distance wasserstein topic model word distributions topics optimal transport word distributions document embeddings word learn unify framework learn topic model leverage distil grind distance matrix update topic distributions smoothly calculate correspond optimal transport strategy provide update word embeddings robust guidance improve algorithm convergence application focus patient admission record propose method embed cod diseases procedures learn topics admissions obtain superior performance clinically meaningful disease network construction mortality prediction function admission cod procedure recommendation',\n",
              " 'simple way improve performance almost machine learn algorithm train many different model data average predictions unfortunately make predictions use whole ensemble model cumbersome may computationally expensive allow deployment large number users especially individual model large neural net caruana collaborators show possible compress knowledge ensemble single model much easier deploy develop approach use different compression technique achieve surprise result mnist show significantly improve acoustic model heavily use commercial system distil knowledge ensemble model single model also introduce new type ensemble compose one full model many specialist model learn distinguish fine grain class full model confuse unlike mixture experts specialist model train rapidly parallel',\n",
              " 'stochastic convex optimization algorithms popular way train machine learn model large scale data scale train process model crucial popular algorithm stochastic gradient descent sgd serial method surprisingly hard parallelize paper propose efficient distribute stochastic optimization method combine adaptivity variance reduction techniques analysis yield linear speedup number machine constant memory footprint logarithmic number communication round critically approach black box reduction parallelize serial online learn algorithm streamline prior analysis allow leverage significant progress make design adaptive algorithms particular achieve optimal convergence rat without prior knowledge smoothness parameters yield robust algorithm reduce need hyperparameter tune implement algorithm spark distribute framework exhibit dramatic performance gain large scale logistic regression problems',\n",
              " 'zero shoot learn zsl aim recognize unseen object class without train sample regard form transfer learn see class unseen ones make possible learn projection feature space semantic space attribute space key zsl thus learn projection function robust often large domain gap see unseen class paper propose novel zsl model term domain invariant projection learn dipl model two novel components domain invariant feature self reconstruction task introduce see unseen class data result simple linear formulation cast zsl min min optimization problem solve problem non trivial novel iterative algorithm formulate solver rigorous theoretic algorithm analysis provide align two domains via learn projection share semantic structure among see unseen class explore via form superclasses semantic space extensive experiment show model outperform state art alternatives significant margins',\n",
              " 'propose dropmax stochastic version softmax classifier iteration drop non target class accord dropout probabilities adaptively decide instance specifically overlay binary mask variables class output probabilities input adaptively learn via variational inference stochastic regularization effect build ensemble classifier exponentially many classifiers different decision boundaries moreover learn dropout rat non target class instance allow classifier focus classification confuse class validate model multiple public datasets classification obtain significantly improve accuracy regular softmax classifier baselines analysis learn dropout probabilities show model indeed select confuse class often perform classification',\n",
              " 'recent methods learn linear subspace data corrupt outliers base convex nuclear norm optimization require dimension subspace number outliers sufficiently small sharp contrast recently propose dual principal component pursuit dpcp method provably handle subspaces high dimension solve non convex optimization problem sphere however geometric analysis base quantities difficult interpret amenable statistical analysis paper provide refine geometric analysis new statistical analysis show dpcp tolerate many outliers square number inliers thus improve upon provably correct robust pca methods also propose scalable project sub gradient descent method dpcp psgd solve dpcp problem show admit linear convergence even though underlie optimization problem non convex non smooth experiment road plane detection point cloud data demonstrate dpcp psgd efficient traditional ransac algorithm one popular methods computer vision applications',\n",
              " 'boltzmann machine powerful distributions show effective prior binary latent variables variational autoencoders vaes however previous methods train discrete vaes use evidence lower bind tighter importance weight bind propose two approach relax boltzmann machine continuous distributions permit train importance weight bound relaxations base generalize overlap transformations gaussian integral trick experiment mnist omniglot datasets show relaxations outperform previous discrete vaes boltzmann priors implementation reproduce result available',\n",
              " 'train discrete latent variable model remain challenge pass gradient information discrete units difficult propose new class smooth transformations base mixture two overlap distributions show propose transformation use train binary latent model either direct undirected priors derive new variational bind efficiently train boltzmann machine priors use bind develop dvae generative model global discrete prior hierarchy convolutional continuous variables experiment several benchmarks show overlap transformations outperform recent continuous relaxations discrete latent variables include gumbel softmax maddison jang discrete variational autoencoders rolfe',\n",
              " 'consider minimization submodular function subject order constraints show potentially non convex optimization problem cast convex optimization problem space uni dimensional measure order constraints correspond first order stochastic dominance propose new discretization scheme lead simple efficient algorithms base zero first higher order oracles algorithms also lead improvements without isotonic constraints finally experiment show non convex loss function much robust outliers isotonic regression still solvable polynomial time',\n",
              " 'active search learn paradigm actively identify many members give class possible critical target scenario high throughput screen scientific discovery drug materials discovery settings specialize instrument often evaluate emph multiple point simultaneously however exist work active search focus sequential acquisition bridge gap address batch active search theoretical practical perspective first derive bayesian optimal policy problem prove lower bind performance gap sequential batch optimal policies cost parallelization also propose novel efficient batch policies inspire state art sequential policies develop aggressive prune technique dramatically speed computation conduct thorough experiment data three application domains citation network material science drug discovery test propose policies total wide range batch size result demonstrate empirical performance gap match theoretical bind nonmyopic policies usually significantly outperform myopic alternatives diversity important consideration batch policy design',\n",
              " 'stochastic gradient hard thresholding methods recently show work favorably solve large scale empirical risk minimization problems sparsity rank constraint despite improve iteration complexity full gradient methods gradient evaluation hard thresholding complexity exist stochastic algorithms usually scale linearly data size could still expensive data huge hard thresholding step could expensive singular value decomposition rank constrain problems address deficiencies propose efficient hybrid stochastic gradient hard thresholding hsg method provably show sample size independent gradient evaluation hard thresholding complexity bound specifically prove stochastic gradient evaluation complexity hsg scale linearly inverse sub optimality hard thresholding complexity scale logarithmically apply heavy ball acceleration technique propose accelerate variant hsg show improve factor dependence restrict condition number numerical result confirm theoretical affirmation demonstrate computational efficiency propose methods',\n",
              " 'learn low dimensional embeddings knowledge graph powerful approach use predict unobserved miss edge entities however open challenge area develop techniques beyond simple edge prediction handle complex logical query might involve multiple unobserved edge entities variables instance give incomplete biological knowledge graph might want predict drug likely target proteins involve diseases query require reason possible proteins might interact diseases introduce framework efficiently make predictions conjunctive logical query flexible tractable subset first order logic incomplete knowledge graph approach embed graph nod low dimensional space represent logical operators learn geometric operations translation rotation embed space perform logical operations within low dimensional embed space approach achieve time complexity linear number query variables compare exponential complexity require naive enumeration base approach demonstrate utility framework two application study real world datasets millions relations predict logical relationships network drug gene disease interactions graph base representation social interactions derive popular web forum',\n",
              " 'paper compare different type recurrent units recurrent neural network rnns especially focus sophisticate units implement gate mechanism long short term memory lstm unit recently propose gate recurrent unit gru evaluate recurrent units task polyphonic music model speech signal model experiment reveal advance recurrent units indeed better traditional recurrent units tanh units also find gru comparable lstm',\n",
              " 'societies often rely human experts take wide variety decisions affect members jail release decisions take judge stop frisk decisions take police officer accept reject decisions take academics context decision take expert typically choose uniformly random pool experts however decisions may imperfect due limit experience implicit bias faulty probabilistic reason improve accuracy fairness overall decision make process optimize assignment experts decisions',\n",
              " 'deep reinforcement learn drl algorithms successfully apply range challenge control task however methods typically suffer three core difficulties temporal credit assignment sparse reward lack effective exploration brittle convergence properties extremely sensitive hyperparameters collectively challenge severely limit applicability approach real world problems evolutionary algorithms eas class black box optimization techniques inspire natural evolution well suit address three challenge however eas typically suffer high sample complexity struggle solve problems require optimization large number parameters paper introduce evolutionary reinforcement learn erl hybrid algorithm leverage population provide diversify data train agent reinserts agent population periodically inject gradient information erl inherit ability temporal credit assignment fitness metric effective exploration diverse set policies stability population base approach complement policy drl ability leverage gradients higher sample efficiency faster learn experiment range challenge continuous control benchmarks demonstrate erl significantly outperform prior drl methods',\n",
              " 'paper propose novel method provide contrastive explanations justify classification input black box classifier deep neural network give input find minimally sufficiently present viz important object pixels image justify classification analogously minimally necessarily emph absent viz certain background pixels argue explanations natural humans use commonly domains health care criminology minimally critically emph absent important part explanation best knowledge explicitly identify current explanation methods explain predictions neural network validate approach three real datasets obtain diverse domains namely handwritten digits dataset mnist large procurement fraud dataset brain activity strength dataset three case witness power approach generate precise explanations also easy human experts understand evaluate',\n",
              " 'draw attention important yet largely overlook aspect evaluate fairness automate decision make systems namely risk welfare considerations propose family measure correspond long establish formulations cardinal social welfare economics justify rawlsian conception fairness behind veil ignorance convex formulation welfare base measure fairness allow integrate constraint convex loss minimization pipeline empirical analysis reveal interest trade off proposal prediction accuracy group discrimination dwork notion individual fairness furthermore perhaps importantly work provide heuristic justification empirical evidence suggest lower bind measure often lead bound inequality algorithmic outcomes hence present first computationally feasible mechanism bound individual level inequality',\n",
              " 'similarity search fundamental problem compute science various applications attract significant research attention especially large scale search high dimension motivate evidence biological science work develop novel approach similarity search fundamentally different exist methods typically reduce dimension data lessen computational complexity speed search approach project data even higher dimensional space ensure sparsity data output space objective improve precision speed specifically approach two key step firstly compute optimal sparse lift give input sample increase dimension data approximately preserve pairwise similarity secondly seek optimal lift operator best map input sample optimal sparse lift computationally step model optimization problems efficiently effectively solve frank wolfe algorithm simple approach report significantly improve result empirical evaluations exhibit high potentials solve practical problems',\n",
              " 'state art object detection network depend region proposal algorithms hypothesize object locations advance like sppnet fast cnn reduce run time detection network expose region proposal computation bottleneck work introduce region proposal network rpn share full image convolutional feature detection network thus enable nearly cost free region proposals rpn fully convolutional network simultaneously predict object bound objectness score position rpn train end end generate high quality region proposals use fast cnn detection merge rpn fast cnn single network share convolutional feature use recently popular terminology neural network attention mechanisms rpn component tell unify network look deep vgg model detection system frame rate fps include step gpu achieve state art object detection accuracy pascal voc coco datasets proposals per image ilsvrc coco competitions faster cnn rpn foundations place win entries several track code make publicly available',\n",
              " 'basic principles design convolutional neural network cnn structure predict object different level image level region level pixel level diverge generally network structure design specifically image classification directly use default backbone structure task include detection segmentation seldom backbone structure design consideration unify advantage network design pixel level region level predict task may require deep feature high resolution towards goal design fish like network call fishnet fishnet information resolutions preserve refine final task besides observe exist work still emph directly propagate gradient information deep layer shallow layer design better handle problem extensive experiment conduct demonstrate remarkable performance fishnet particular imagenet accuracy fishnet able surpass performance densenet resnet fewer parameters fishnet apply one modules win entry coco detection challenge code available https github com kevin ssy fishnet',\n",
              " 'paper present novel framework video image segmentation localization cast single optimization problem integrate information low level appearance cue high level localization cue weakly supervise manner propose framework leverage two representations different level exploit spatial relationship bound box superpixels linear constraints simultaneously discriminate foreground background bound box superpixel level different previous approach mainly rely discriminative cluster incorporate foreground model minimize histogram difference object across image frame exploit geometric relation superpixels bound box enable transfer segmentation cue improve localization output vice versa inclusion foreground model generalize discriminative framework video data background tend similar thus discriminative demonstrate effectiveness unify framework youtube object video dataset internet object discovery dataset pascal voc',\n",
              " 'typical way network data record measure interactions involve specify set core nod produce graph contain core together potentially larger set fringe nod link core interactions nod fringe however present result graph data example phone service provider may record call least one participants customer include call customer non customer pair non customers knowledge nod belong core crucial interpret dataset metadata unavailable many case either lose due difficulties data provenance network consist find data obtain settings counter surveillance lead algorithmic problem recover core set since core vertex cover essentially plant vertex cover problem arbitrary underlie graph develop framework analyze plant vertex cover problem base theory fix parameter tractability together algorithms recover core algorithms fast simple implement perform several baselines base core periphery structure various real world datasets',\n",
              " 'continuous word representation aka word embed basic build block many neural network base model use natural language process task although widely accept word similar semantics close embed space find word embeddings learn several task bias towards word frequency embeddings high frequency low frequency word lie different subregions embed space embed rare word popular word far even semantically similar make learn word embeddings ineffective especially rare word consequently limit performance neural network model order mitigate issue paper propose neat simple yet effective adversarial train method blur boundary embeddings high frequency word low frequency word conduct comprehensive study ten datasets across four natural language process task include word similarity language model machine translation text classification result show achieve higher performance baselines task',\n",
              " 'deep convolutional neural network demonstrate powerfulness variety applications however storage computational requirements largely restrict extensions mobile devices recently prune unimportant parameters use network compression acceleration consider spatial redundancy within filter cnn propose frequency domain dynamic prune scheme exploit spatial correlations frequency domain coefficients prune dynamically iteration different frequency band prune discriminatively give different importance accuracy experimental result demonstrate propose scheme outperform previous spatial domain counterparts large margin specifically achieve compression ratio theoretical inference speed resnet accuracy even better reference model cifar',\n",
              " 'real time automatic speech recognition asr mobile embed devices great interest many years present real time speech recognition smartphones embed systems employ recurrent neural network rnn base acoustic model rnn base language model beam search decode acoustic model end end train connectionist temporal classification ctc loss rnn implementation embed devices suffer excessive dram access parameter size neural network usually exceed cache memory parameters use time step remedy problem employ multi time step parallelization approach compute multiple output sample time parameters fetch dram since number dram access reduce proportion number parallelization step achieve high process speed however conventional rnns long short term memory lstm gate recurrent unit gru permit multi time step parallelization construct acoustic model combine simple recurrent units srus depth wise dimensional convolution layer multi time step parallelization character word piece model develop acoustic model correspond rnn base language model use beam search decode achieve competitive wer wsj corpus use entire model size around achieve real time speed use single core arm without gpu special hardware',\n",
              " 'conditional density estimation cde model deal estimate conditional distributions condition impose distribution input model cde challenge task fundamental trade model complexity representational capacity overfitting work propose extend model input latent variables use gaussian process map augment input onto sample conditional distribution bayesian approach allow model small datasets also provide machinery apply big data use stochastic variational inference approach use model densities even sparse data regions allow share learn structure condition illustrate effectiveness wide reach applicability model variety real world problems spatio temporal density estimation taxi drop off non gaussian noise model shoot learn omniglot image',\n",
              " 'present generative framework generalize zero shoot learn train test class necessarily disjoint build upon variational autoencoder base architecture consist probabilistic encoder probabilistic conditional decoder model generate novel exemplars see unseen class give respective class attribute exemplars subsequently use train shelf classification model one key aspects encoder decoder architecture feedback drive mechanism discriminator multivariate regressor learn map generate exemplars correspond class attribute vectors lead improve generator model ability generate leverage examples unseen class train classification model naturally help mitigate bias towards predict see class generalize zero shoot learn settings comprehensive set experiment show model outperform several state art methods several benchmark datasets standard well generalize zero shoot learn',\n",
              " 'technical challenge deep learn recognize target class without see data zero shoot learn leverage semantic representations attribute class prototypes bridge source target class exist standard zero shoot learn methods may prone overfitting see data source class blind semantic representations target class paper study generalize zero shoot learn assume accessible target class unseen data train prediction unseen data make search source target class propose novel deep calibration network dcn approach towards generalize zero shoot learn paradigm enable simultaneous calibration deep network confidence source class uncertainty target class approach map visual feature image semantic representations class prototypes common embed space compatibility see data source target class maximize show superior accuracy approach state art benchmark datasets generalize zero shoot learn include awa cub sun apy',\n",
              " 'graph match receive persistent attention decades formulate quadratic assignment problem qap show large family function define separable function approximate discrete graph match continuous domain asymptotically vary approximation control parameters also study properties global optimality devise convex concave preserve extensions widely use lawler qap form theoretical find show potential derive new algorithms techniques graph match deliver solvers base two specific instance separable function state art performance method verify popular benchmarks',\n",
              " 'probability estimation one fundamental task statistics machine learn however standard methods probability estimation discrete object handle object structure satisfactory manner paper derive general bayesian network formulation probability estimation leaf label tree enable flexible approximations generalize beyond observations show efficient algorithms learn bayesian network easily extend probability estimation challenge structure space experiment synthetic real data show methods greatly outperform current practice use empirical distribution well previous effort probability estimation tree',\n",
              " 'responses generate neural conversational model tend lack informativeness diversity present adversarial information maximization aim adversarial learn framework address two relate distinct problems foster response diversity leverage adversarial train allow distributional match synthetic real responses improve informativeness framework explicitly optimize variational lower bind pairwise mutual information query response empirical result automatic human evaluations demonstrate methods significantly boost informativeness diversity',\n",
              " 'propose new framework estimate generative model via adversarial net simultaneously train two model generative model capture data distribution discriminative model estimate probability sample come train data rather train procedure maximize probability make mistake framework correspond minimax two player game space arbitrary function unique solution exist recover train data distribution equal everywhere case define multilayer perceptrons entire system train backpropagation need markov chain unroll approximate inference network either train generation sample experiment demonstrate potential framework qualitative quantitatively evaluation generate sample',\n",
              " 'recent deep learn base approach show promise result challenge task inpainting large miss regions image methods generate visually plausible image structure textures often create distort structure blurry textures inconsistent surround areas mainly due ineffectiveness convolutional neural network explicitly borrow copy information distant spatial locations hand traditional texture patch synthesis approach particularly suitable need borrow textures surround regions motivate observations propose new deep generative model base approach synthesize novel image structure also explicitly utilize surround image feature reference network train make better predictions model fee forward fully convolutional neural network process image multiple hole arbitrary locations variable size test time experiment multiple datasets include face celeba celeba textures dtd natural image imagenet place demonstrate propose approach generate higher quality inpainting result exist ones code demo model available https github com jiahuiyu generative inpainting',\n",
              " 'introduce generative neural machine translation gnmt latent variable architecture design model semantics source target sentence modify encoder decoder translation model add latent variable language agnostic representation encourage learn mean sentence gnmt achieve competitive bleu score pure translation task superior miss word source sentence augment model facilitate multilingual translation semi supervise learn without add parameters framework significantly reduce overfitting limit pair data available effective translate pair languages see train',\n",
              " 'introduce genetic gate network simple neural network combine gate vector compose binary genetic genes hide layer network method take advantage gradient free optimization gradient base optimization methods former effective problems multiple local minima latter quickly find local minima addition multiple chromosomes define different model make easy construct multiple model effectively apply problems require multiple model show apply typical reinforcement learn algorithms achieve large improvement sample efficiency performance',\n",
              " 'monte carlo sample high dimensional low sample settings important many machine learn task improve current methods sample euclidean space avoid independence instead consider ways couple sample show fundamental connections optimal transport theory lead novel sample algorithms provide new theoretical ground exist strategies compare new strategies prior methods improve sample efficiency include qmc study discrepancy explore find empirically observe benefit sample scheme reinforcement learn generative model',\n",
              " 'propose new type generative model high dimensional data learn manifold geometry data rather density generate point evenly along manifold contrast exist generative model represent data density strongly affect noise artifacts data collection demonstrate approach correct sample bias artifacts thus improve several downstream data analysis task cluster classification finally demonstrate approach especially useful biology despite advent single cell technologies rare subpopulations gene interaction relationships affect bias sample show sugar generate hypothetical populations able reveal intrinsic pattern mutual information relationships genes single cell rna sequence dataset hematopoiesis',\n",
              " 'distribute compute environment consider empirical risk minimization problem propose distribute communication efficient newton type optimization method every iteration worker locally find approximate newton ant direction send main driver main driver average ant directions receive workers form globally improve ant giant direction giant highly communication efficient naturally exploit trade off local computations global communications local computations result fewer overall round communications theoretically show giant enjoy improve convergence rate compare first order methods exist distribute newton type methods sharp contrast many exist distribute newton type methods well popular first order methods highly advantageous practical feature giant involve one tune parameter conduct large scale experiment computer cluster empirically demonstrate superior performance giant',\n",
              " 'exist deep convolutional neural network cnns classification global average first order pool gap become standard module summarize activations last convolution layer final representation prediction recent research show integration higher order pool hop methods clearly improve performance deep cnns however gap exist hop methods assume unimodal distributions fully capture statistics convolutional activations limit representation ability deep cnns especially sample complex content overcome limitation paper propose global gate mixture second order pool sop method improve representation ability deep cnns end introduce sparsity constrain gate mechanism propose novel parametric sop component mixture model give bank sop candidates method adaptively choose top candidates input sample sparsity constrain gate module perform weight sum output select candidates representation sample propose sop flexibly accommodate large number personalize sop candidates efficient way lead richer representations deep network sop end end train potential characterize complex multi modal distributions propose method evaluate two large scale image benchmarks downsampled imagenet place experimental result show sop superior counterparts achieve competitive performance source code available http www peihuali org sop',\n",
              " 'large scale network model use neurons static nonlinearities produce analog output despite fact information process brain predominantly carry dynamic neurons produce discrete pulse call spike research spike base computation impede lack efficient supervise learn algorithm spike neural network present gradient descent method optimize spike network model introduce differentiable formulation spike dynamics derive exact gradient calculation demonstration train recurrent spike network two dynamic task one require optimize fast millisecond spike base interactions efficient encode information delay memory task extend duration second result show gradient descent approach indeed optimize network dynamics time scale individual spike well behavioral time scale conclusion method yield general purpose supervise learn algorithm spike neural network facilitate investigations spike base computations',\n",
              " 'lot learn task require deal graph data contain rich relation information among elements model physics systems learn molecular fingerprint predict protein interface classify diseases demand model learn graph input domains learn non structural data like texts image reason extract structure like dependency tree sentence scene graph image important research topic also need graph reason model graph neural network gnns neural model capture dependence graph via message pass nod graph recent years variants gnns graph convolutional network gcn graph attention network gat graph recurrent network grn demonstrate grind break performances many deep learn task survey propose general design pipeline gnn model discuss variants component systematically categorize applications propose four open problems future research',\n",
              " 'convert input binary code hash algorithm widely use approximate nearest neighbor search large scale image set due computation storage efficiency deep hash improve retrieval quality combine hash cod deep neural network however major difficulty deep hash lie discrete constraints impose network output generally make optimization hard work adopt greedy principle tackle hard problem iteratively update network toward probable optimal discrete solution iteration hash cod layer design implement approach strictly use sign function forward propagation maintain discrete constraints back propagation gradients transmit intactly front layer avoid vanish gradients addition theoretical derivation provide new perspective visualize understand effectiveness efficiency algorithm experiment benchmark datasets show scheme outperform state art hash methods supervise unsupervised task',\n",
              " 'batch normalization milestone technique development deep learn enable various network train however normalize along batch dimension introduce problems error increase rapidly batch size become smaller cause inaccurate batch statistics estimation limit usage train larger model transfer feature computer vision task include detection segmentation video require small batch constrain memory consumption paper present group normalization simple alternative divide channel group compute within group mean variance normalization computation independent batch size accuracy stable wide range batch size resnet train imagenet lower error counterpart use batch size use typical batch size comparably good outperform normalization variants moreover naturally transfer pre train fine tune outperform base counterparts object detection segmentation coco video classification kinetics show effectively replace powerful variety task easily implement line code modern libraries',\n",
              " 'consider high dimensional linear regression problem goal efficiently recover unknown vector beta noisy linear observations beta instead adopt regularization base assume underlie vectors beta rational entries denominator call rationality assumption propose new polynomial time algorithm task base seminal lenstra lenstra lovasz lll lattice basis reduction algorithm establish rationality assumption algorithm recover exactly vector beta large class distributions iid entries non zero noise prove successful small noise even learner access one observation furthermore prove case gaussian white noise log sufficiently large algorithm tolerate nearly optimal information theoretic level noise',\n",
              " 'recent advance deep learn show excite promise fill large hole natural image semantically plausible context aware detail impact fundamental image manipulation task object removal learn base methods significantly effective capture high level feature prior techniques handle low resolution input due memory limitations difficulty train even slightly larger image inpainted regions would appear blurry unpleasant boundaries become visible propose multi scale neural patch synthesis approach base joint optimization image content texture constraints preserve contextual structure also produce high frequency detail match adapt patch similar mid layer feature correlations deep classification network evaluate method imagenet paris streetview datasets achieve state art inpainting accuracy show approach produce sharper coherent result prior methods especially high resolution image',\n",
              " 'quantization promise technique reduce model size memory footprint massive computation operations recurrent neural network rnns embed devices limit resources although extreme low bite quantization achieve impressive success convolutional neural network still suffer huge accuracy degradation rnns low bite precision paper first investigate accuracy degradation rnn model different quantization scheme distribution tensor value full precision model observation reveal due difference distributions weight activations different quantization methods suitable different part model base observation propose hitnet hybrid ternary recurrent neural network bridge accuracy gap full precision model quantize model hitnet develop hybrid quantization method quantize weight activations moreover introduce slop factor motivate prior work boltzmann machine activation function close accuracy gap full precision model quantize model overall hitnet quantize rnn model ternary value outperform state art quantization methods rnn model significantly test typical rnn model long short term memory lstm gate recurrent units gru result outperform previous work significantly example improve perplexity per word ppw ternary lstm penn tree bank ptb corpus state art result best knowledge full precision model ternary gru full precision model',\n",
              " 'stochastic gradient descent sgd popular algorithm achieve state art performance variety machine learn task several researchers recently propose scheme parallelize sgd require performance destroy memory lock synchronization work aim show use novel theoretical analysis algorithms implementation sgd implement without lock present update scheme call hogwild allow processors access share memory possibility overwrite work show associate optimization problem sparse mean gradient update modify small part decision variable hogwild achieve nearly optimal rate convergence demonstrate experimentally hogwild outperform alternative scheme use lock order magnitude',\n",
              " 'batch normalization batchnorm widely adopt technique enable faster stable train deep neural network dnns despite pervasiveness exact reason batchnorm effectiveness still poorly understand popular belief effectiveness stem control change layer input distributions train reduce call internal covariate shift work demonstrate distributional stability layer input little success batchnorm instead uncover fundamental impact batchnorm train process make optimization landscape significantly smoother smoothness induce predictive stable behavior gradients allow faster train',\n",
              " 'identify study two common failure modes early train deep relu net give rigorous proof occur avoid fully connect convolutional residual architectures show first failure mode explode vanish mean activation length avoid initialize weight symmetric distribution variance fan resnets correctly scale residual modules prove second failure mode exponentially large variance activation length never occur residual net first failure mode avoid contrast fully connect net prove failure mode happen avoid keep constant sum reciprocals layer widths demonstrate empirically effectiveness theoretical result predict network able start train particular note many popular initializations fail criteria whereas correct initialization architecture allow much deeper network train',\n",
              " 'many structure data fit applications require solution optimization problem involve sum potentially large number measurements incremental gradient algorithms offer inexpensive iterations sample subset term sum methods make great progress initially often slow approach solution contrast full gradient methods achieve steady convergence expense evaluate full objective gradient iteration explore hybrid methods exhibit benefit approach rate convergence analysis show control sample size incremental gradient algorithm possible maintain steady convergence rat full gradient methods detail practical quasi newton implementation base approach numerical experiment illustrate potential benefit',\n",
              " 'dominant object detection approach treat recognition region separately overlook crucial semantic correlations object one scene paradigm lead substantial performance drop face heavy long tail problems sample available rare class plenty confuse categories exist exploit diverse human commonsense knowledge reason large scale object categories reach semantic coherency within one image particularly present hybrid knowledge rout modules hkrm incorporate reason rout two kinds knowledge form explicit knowledge module structure constraints summarize linguistic knowledge share attribute relationships concepts implicit knowledge module depict implicit constraints common spatial layouts function region region graph modules individualize adapt coordinate visual pattern image guide specific knowledge form hkrm light weight general purpose extensible easily incorporate multiple knowledge endow detection network ability global semantic reason experiment large scale object detection benchmarks show hkrm obtain around improvement visualgenome categories ade term map',\n",
              " 'generate long coherent report describe medical image pose challenge bridge visual pattern informative human linguistic descriptions propose novel hybrid retrieval generation reinforce agent hrgr agent reconcile traditional retrieval base approach populate human prior knowledge modern learn base approach achieve structure robust diverse report generation hrgr agent employ hierarchical decision make procedure sentence high level retrieval policy module choose either retrieve template sentence shelf template database invoke low level generation module generate new sentence hrgr agent update via reinforcement learn guide sentence level word level reward experiment show approach achieve state art result two medical report datasets generate well balance structure sentence robust coverage heterogeneous medical report content addition model achieve highest detection precision medical abnormality terminologies improve human evaluation performance',\n",
              " 'paper propose generative multi column network image inpainting network synthesize different image components parallel manner within one stage better characterize global structure design confidence drive reconstruction loss implicit diversify mrf regularization adopt enhance local detail multi column network combine reconstruction mrf loss propagate local global information derive context target inpainting regions extensive experiment challenge street view face natural object scenes manifest method produce visual compel result even without previously common post process',\n",
              " 'deep image translation methods recently show excellent result output high quality image cover multiple modes data distribution also increase interest disentangle internal representations learn deep methods improve performance achieve finer control paper bridge two objectives introduce concept cross domain disentanglement aim separate internal representation three part share part contain information domains exclusive part hand contain factor variation particular domain achieve bidirectional image translation base generative adversarial network cross domain autoencoders novel network component model offer multiple advantage output diverse sample cover multiple modes distributions domains perform domain specific image transfer interpolation cross domain retrieval without need label data pair image compare model state art multi modal image translation achieve better result translation challenge datasets well cross domain retrieval realistic datasets',\n",
              " 'work aim solve large collection task use single reinforcement learn agent single set parameters key challenge handle increase amount data extend train time develop new distribute agent impala importance weight actor learner architecture use resources efficiently single machine train also scale thousands machine without sacrifice data efficiency resource utilisation achieve stable learn high throughput combine decouple act learn novel policy correction method call trace demonstrate effectiveness impala multi task reinforcement learn dmlab set task deepmind lab environment beattie atari available atari game arcade learn environment bellemare result show impala able achieve better performance previous agents less data crucially exhibit positive transfer task result multi task approach',\n",
              " 'provide simple efficient way compute low variance gradients continuous random variables reparameterization trick become technique choice train variety latent variable model however applicable number important continuous distributions introduce alternative approach compute reparameterization gradients base implicit differentiation demonstrate broader applicability apply gamma beta dirichlet von mises distributions use classic reparameterization trick experiment show propose approach faster accurate exist gradient estimators distributions',\n",
              " 'generative adversarial network gans powerful generative model suffer train instability recently propose wasserstein gin wgan make progress toward stable train gans sometimes still generate low quality sample fail converge find problems often due use weight clip wgan enforce lipschitz constraint critic lead undesired behavior propose alternative clip weight penalize norm gradient critic respect input propose method perform better standard wgan enable stable train wide variety gin architectures almost hyperparameter tune include layer resnets language model discrete data also achieve high quality generations cifar lsun bedrooms',\n",
              " 'paper describe infogan information theoretic extension generative adversarial network able learn disentangle representations completely unsupervised manner infogan generative adversarial network also maximize mutual information small subset latent variables observation derive lower bind mutual information objective optimize efficiently show train procedure interpret variation wake sleep algorithm specifically infogan successfully disentangle write style digit shape mnist dataset pose light render image background digits central digit svhn dataset also discover visual concepts include hair style presence absence eyeglasses emotions celeba face dataset experiment show infogan learn interpretable representations competitive representations learn exist fully supervise methods',\n",
              " 'give two candidate model set target observations address problem measure relative goodness fit two model propose two new statistical test nonparametric computationally efficient runtime complexity linear sample size interpretable unique advantage test produce set examples informative feature indicate regions data domain one model fit significantly better real world problem compare gin model test power new test match state art test relative goodness fit one order magnitude faster',\n",
              " 'hypergraph partition important problem machine learn computer vision network analytics widely use method hypergraph partition rely minimize normalize sum cost partition hyperedges across cluster algorithmic solutions base approach assume different partition hyperedge incur cost however assumption fail leverage fact different subsets vertices within hyperedge may different structural importance hence propose new hypergraph cluster technique term inhomogeneous hypergraph partition assign different cost different hyperedge cut prove inhomogeneous partition produce quadratic approximation optimal solution inhomogeneous cost satisfy submodularity constraints moreover demonstrate inhomogenous partition offer significant performance improvements applications structure learn rank subspace segmentation motif cluster',\n",
              " 'work introduce interactive structure learn framework unify many different interactive learn task present generalization query committee active learn algorithm set study consistency rate convergence theoretically empirically without noise',\n",
              " 'two semimetrics probability distributions propose give sum differences expectations analytic function evaluate spatial frequency locations feature feature choose maximize distinguishability distributions optimize lower bind test power statistical test use feature result parsimonious interpretable indication two distributions differ locally empirical estimate test power criterion converge increase sample size ensure quality return feature real world benchmarks high dimensional text image data linear time test use propose semimetrics achieve comparable performance state art quadratic time maximum mean discrepancy test return human interpretable feature explain test result',\n",
              " 'deep neural network highly expressive model recently achieve state art performance speech visual recognition task expressiveness reason succeed also cause learn uninterpretable solutions could counter intuitive properties paper report two properties first find distinction individual high level units random linear combinations high level units accord various methods unit analysis suggest space rather individual units contain semantic information high layer neural network second find deep neural network learn input output mappings fairly discontinuous significant extend cause network misclassify image apply certain imperceptible perturbation find maximize network prediction error addition specific nature perturbations random artifact learn perturbation cause different network train different subset dataset misclassify input',\n",
              " 'present novel introspective variational autoencoder introvae model synthesize high resolution photographic image introvae capable self evaluate quality generate sample improve accordingly inference generator model jointly train introspective way one hand generator require reconstruct input image noisy output inference model normal vaes hand inference model encourage classify generate real sample generator try fool gans two famous generative frameworks integrate simple yet efficient single stream architecture train single stage introvae preserve advantage vaes stable train nice latent manifold unlike hybrid model vaes gans introvae require extra discriminators inference model serve discriminator distinguish generate real sample experiment demonstrate method produce high resolution photo realistic image celeba image comparable better state art gans',\n",
              " 'decompose evidence lower bind show existence term measure total correlation latent variables use motivate beta tcvae total correlation variational autoencoder algorithm refinement plug replacement beta vae learn disentangle representations require additional hyperparameters train propose principled classifier free measure disentanglement call mutual information gap mig perform extensive quantitative qualitative experiment restrict non restrict settings show strong relation total correlation disentanglement model train use framework',\n",
              " 'consider problem active feature acquisition goal sequentially select subset feature order achieve maximum prediction performance cost effective way test time work formulate active feature acquisition jointly learn problem train classifier environment agent decide either collect new feature test time cost sensitive manner also introduce novel encode scheme represent acquire subsets feature propose order invariant set encode feature level also significantly reduce search space agent evaluate model carefully design synthetic dataset active feature acquisition well several medical datasets framework show meaningful feature acquisition process diagnosis comply human knowledge outperform baselines term prediction performance well feature acquisition cost',\n",
              " 'convolutional neural network cnns recently achieve great success single image super resolution sisr however methods tend produce smooth output miss textural detail solve problems propose super resolution cliquenet srcliquenet reconstruct high resolution image better textural detail wavelet domain propose srcliquenet firstly extract set feature map low resolution image clique block group send set feature map clique sample module reconstruct image clique sample module consist four sub net predict high resolution wavelet coefficients four sub band since consider edge feature properties four sub band four sub net connect others learn coefficients four sub band jointly finally apply inverse discrete wavelet transform idwt output four sub net end clique sample module increase resolution reconstruct image extensive quantitative qualitative experiment benchmark datasets show method achieve superior performance state art methods',\n",
              " 'indispensable component batch normalization successfully improve train deep neural network dnns mini batch normalize distribution internal representation hide layer however effectiveness would diminish scenario micro batch less sample mini batch since estimate statistics mini batch reliable insufficient sample limit room train larger model segmentation detection video relate problems require small batch constrain memory consumption paper present novel normalization method call kalman normalization improve accelerate train dnns particularly context micro batch specifically unlike exist solutions treat hide layer isolate system treat layer network whole system estimate statistics certain layer consider distributions precede layer mimic merit kalman filter resnet train imagenet lower error counterpart use batch size even use typical batch size still maintain advantage variants suffer performance degradation moreover naturally generalize many exist normalization variants obtain gain equip group normalization group kalman normalization gkn outperform variants large scale object detection segmentation task coco',\n",
              " 'knowledge distillation aim train lightweight classifier suitable provide accurate inference constrain resources multi label learn instead directly consume feature label pair classifier train teacher high capacity model whose train may resource hungry accuracy classifier train way usually suboptimal difficult learn true data distribution teacher alternative method adversarially train classifier discriminator two player game akin generative adversarial network gin ensure classifier learn true data distribution equilibrium game however may take excessively long time two player game reach equilibrium due high variance gradient update address limitations propose three player game name kdgan consist classifier teacher discriminator classifier teacher learn via distillation losses adversarially train discriminator via adversarial losses simultaneously optimize distillation adversarial losses classifier learn true data distribution equilibrium approximate discrete distribution learn classifier teacher concrete distribution concrete distribution generate continuous sample obtain low variance gradient update speed train extensive experiment use real datasets confirm superiority kdgan accuracy train speed',\n",
              " 'suggest new loss learn deep embeddings key characteristics new loss absence tunable parameters good result obtain across range datasets problems loss compute estimate two distribution similarities positive match negative non match point pair compute probability positive pair lower similarity score negative pair base probability estimate show operations perform simple piecewise differentiable manner use histograms soft assignment operations make propose loss suitable learn deep embeddings use stochastic optimization experiment reveal favourable result compare recently propose loss function',\n",
              " 'present framework learn disentangle interpretable jointly continuous discrete representations unsupervised manner augment continuous latent distribution variational autoencoders relax discrete distribution control amount information encode latent unit show continuous categorical factor variation discover automatically data experiment show framework disentangle continuous discrete generative factor various datasets outperform current disentangle methods discrete generative factor prominent',\n",
              " 'progress deep learn spawn great successes many engineer applications prime example convolutional neural network type feedforward neural network approach sometimes even surpass human accuracy variety visual recognition task however show neural network recent extensions struggle recognition task dependent visual feature must detect long spatial range introduce visual challenge pathfinder describe novel recurrent neural network architecture call horizontal gate recurrent unit hgru learn intrinsic horizontal connections within across feature columns demonstrate single hgru layer match outperform test feedforward hierarchical baselines include state art architectures order magnitude parameters',\n",
              " 'consider problem learn optimal reserve price repeat auction non myopic bidders may bid strategically order gain future round even single round auction truthful previous algorithms empirical price provide non trivial regret round set general introduce algorithms obtain small regret non myopic bidders either market large bidder appear constant fraction round bidders impatient discount future utility factor mildly bound away one approach carefully control information reveal bidder build techniques differentially private online learn well recent line work jointly differentially private algorithms',\n",
              " 'machine learn become widely use practice need new methods build complex intelligent systems integrate learn exist software domain knowledge encode rule case study present system learn parse newtonian physics problems textbooks system nut bolt learn pipeline process incorporate exist code pre learn machine learn model human engineer rule jointly train entire pipeline prevent propagation errors use combination label unlabelled data approach achieve good performance parse task outperform simple pipeline variants finally also show nut bolt use achieve improvements relation extraction task end task answer newtonian physics problems',\n",
              " 'address problem learn semantic representation question measure similarity pair continuous distance metric work naturally extend word mover distance wmd represent text document normal distributions instead bag embed word learn metric measure dissimilarity two question minimum amount distance intent hide representation one question need travel match intent another question first learn repeat reformulate question infer intents normal distributions deep generative model variational auto encoder semantic similarity pair learn discriminatively optimal transport distance metric wasserstein novel variational siamese framework among know model read sentence individually propose framework achieve competitive result quora duplicate question dataset work shed light deep generative model approximate distributions semantic representations effectively measure semantic similarity meaningful distance metrics information theory',\n",
              " 'propose simple yet effective approach spatiotemporal feature learn use deep dimensional convolutional network convnets train large scale supervise video dataset find three fold convnets suitable spatiotemporal feature learn compare convnets homogeneous architecture small convolution kernels layer among best perform architectures convnets learn feature namely convolutional simple linear classifier outperform state art methods different benchmarks comparable current best methods benchmarks addition feature compact achieve accuracy ucf dataset dimension also efficient compute due fast inference convnets finally conceptually simple easy train use',\n",
              " 'goal predict future video frame give sequence input frame despite large amount video data remain challenge task high dimensionality video frame address challenge propose decompositional disentangle predictive auto encoder ddpae framework combine structure probabilistic model deep network automatically decompose high dimensional video aim predict components disentangle component low dimensional temporal dynamics easier predict crucially appropriately specify generative model video frame ddpae able learn latent decomposition disentanglement without explicit supervision move mnist dataset show ddpae able recover underlie components individual digits disentanglement appearance location would intuitively demonstrate ddpae apply bounce ball dataset involve complex interactions multiple object predict video frame directly pixels recover physical state without explicit supervision',\n",
              " 'human scene understand use variety visual non visual cue perform inference object type pose relations physics rich universal cue exploit enhance scene understand integrate physical cue stability learn process use reinforce approach couple physics engine apply problem produce bound box pose object scene first show apply physics supervision exist scene understand model increase performance produce stable predictions allow train equivalent performance level fewer annotate train examples present novel architecture scene parse name prim cnn learn predict bound box well size translation rotation physics supervision prim cnn outperform exist scene understand approach problem finally show apply physics supervision unlabeled real image improve real domain transfer model train synthetic data',\n",
              " 'navigate unstructured environments basic capability intelligent creatures thus fundamental interest study development artificial intelligence long range navigation complex cognitive task rely develop internal representation space ground recognisable landmarks robust visual process simultaneously support continuous self localisation representation goal go build upon recent research apply deep reinforcement learn maze navigation problems present end end deep reinforcement learn approach apply city scale recognise successful navigation rely integration general policies locale specific knowledge propose dual pathway architecture allow locale specific feature encapsulate still enable transfer multiple cities key contribution paper interactive navigation environment use google street view photographic content worldwide coverage baselines demonstrate deep reinforcement learn agents learn navigate multiple cities traverse target destinations may kilometres away video summarize research show train agent diverse city environments well transfer task available https sit google com view learn navigate cities nip',\n",
              " 'single image humans able perceive full shape object exploit learn shape priors everyday life contemporary single image reconstruction algorithms aim solve task similar fashion often end priors highly bias train class present algorithm generalizable reconstruction genre design capture generic class agnostic shape priors achieve inference network train procedure combine representations visible surface depth silhouette spherical shape representations visible non visible surface voxel base representations principled manner exploit causal structure shape give rise image experiment demonstrate genre perform well single view shape reconstruction generalize diverse novel object categories see train',\n",
              " 'paper introduce versatile filter construct efficient convolutional neural network consider demand efficient deep learn techniques run cost effective hardware number methods develop learn compact neural network work aim slim filter different ways investigate small sparse binarized filter contrast treat filter additive perspective series secondary filter derive primary filter secondary filter inherit primary filter without occupy storage unfold computation could significantly enhance capability filter integrate information extract different receptive field besides spatial versatile filter additionally investigate versatile filter channel perspective new techniques general upgrade filter exist cnns experimental result benchmark datasets neural network demonstrate cnns construct versatile filter able achieve comparable accuracy original filter require less memory flop',\n",
              " 'unsupervised learn generative adversarial network gans prove hugely successful regular gans hypothesize discriminator classifier sigmoid cross entropy loss function however find loss function may lead vanish gradients problem learn process overcome problem propose paper least square generative adversarial network lsgans adopt least square loss function discriminator show minimize objective function lsgan yield minimize pearson chi divergence two benefit lsgans regular gans first lsgans able generate higher quality image regular gans second lsgans perform stable learn process evaluate lsgans five scene datasets experimental result show image generate lsgans better quality ones generate regular gans also conduct two comparison experiment lsgans regular gans illustrate stability lsgans',\n",
              " 'suppose design matrix linear regression problem give response point hide unless explicitly request goal sample small number responses produce weight vector whose sum square loss point epsilon time minimum small jointly sample diverse subsets point crucial one method call volume sample unique desirable property weight vector produce unbiased estimate optimum therefore natural ask method offer optimal unbiased estimate term number responses need achieve epsilon loss approximation',\n",
              " 'object relationships critical content image understand scene graph provide structure description capture properties image however reason relationships object challenge recent work attempt solve problem generate scene graph image paper present novel method improve scene graph generation explicitly model inter dependency among entire object instance design simple effective relational embed module enable model jointly represent connections among relate object rather focus object isolation novel method significantly benefit two main part scene graph generation task object classification relationship classification use top basic faster cnn model achieve state art result visual genome benchmark push performance introduce global context encode module geometrical layout encode module validate final model linknet extensive ablation study demonstrate efficacy scene graph generation',\n",
              " 'several large scale deployments differential privacy use collect statistical information users however deployments periodically recollect data recompute statistics use algorithms design single use result systems provide meaningful privacy guarantee long time scale moreover exist techniques mitigate effect apply local model differential privacy systems use',\n",
              " 'recurrent network spike neurons rsnns underlie astound compute learn capabilities brain compute learn capabilities rsnn model remain poor least comparison anns address two possible reason one rsnns brain randomly connect design accord simple rule start learn tabula rasa network rather rsnns brain optimize task evolution development prior experience detail optimization process largely unknown functional contribution approximate powerful optimization methods backpropagation time bptt',\n",
              " 'deep neural network suffer fit catastrophic forget train small data one natural remedy problem data augmentation recently show effective however previous work either assume intra class variances always generalize new class employ naive generation methods hallucinate finite examples without model latent distributions work propose covariance preserve adversarial augmentation network overcome exist limit low shoot learn specifically novel generative adversarial network design model latent distribution novel class give relate base counterparts since direct estimation novel class inductively bias explicitly preserve covariance information variability base examples generation process empirical result show model generate realistic yet diverse examples lead substantial improvements imagenet benchmark state art',\n",
              " 'learn examples remain key challenge machine learn despite recent advance important domains vision language standard supervise deep learn paradigm offer satisfactory solution learn new concepts rapidly little data work employ ideas metric learn base deep neural feature recent advance augment neural network external memories framework learn network map small label support set unlabelled example label obviate need fine tune adapt new class type define one shoot learn problems vision use omniglot imagenet language task algorithm improve one shoot accuracy imagenet omniglot compare compete approach also demonstrate usefulness model language model introduce one shoot task penn treebank',\n",
              " 'fine grain visual classification fgvc important computer vision problem involve small diversity within different class often require expert annotators collect data utilize notion small visual diversity revisit maximum entropy learn context fine grain classification provide train routine maximize entropy output probability distribution train convolutional neural network fgvc task provide theoretical well empirical justification approach achieve state art performance across variety classification task fgvc potentially extend fine tune task method robust different hyperparameter value amount train data amount train label noise hence valuable tool many similar problems',\n",
              " 'propose novel flexible anchor mechanism name metaanchor object detection frameworks unlike many previous detectors model anchor via predefined manner metaanchor anchor function could dynamically generate arbitrary customize prior box take advantage weight prediction metaanchor able work anchor base object detection systems retinanet compare predefined anchor scheme empirically find metaanchor robust anchor settings bound box distributions addition also show potential transfer task experiment coco detection task show metaanchor consistently outperform counterparts various scenarios',\n",
              " 'paper propose conceptually simple general framework call metagan shoot learn problems state art shoot classification model integrate metagan principled straightforward way introduce adversarial generator condition task augment vanilla shoot classification model ability discriminate real fake data argue gin base approach help shoot classifiers learn sharper decision boundary could generalize better show metagan framework extend supervise shoot learn model naturally cope unsupervised data different previous work semi supervise shoot learn algorithms deal semi supervision sample level task level give theoretical justifications strength metagan validate effectiveness metagan challenge shoot image classification benchmarks',\n",
              " 'goal reinforcement learn algorithms estimate optimise value function however unlike supervise learn teacher oracle available provide true value function instead majority reinforcement learn algorithms estimate optimise proxy value function proxy typically base sample bootstrapped approximation true value function know return particular choice return one chief components determine nature algorithm rate future reward discount value bootstrapped even nature reward well know decisions crucial overall success algorithms discuss gradient base meta learn algorithm able adapt nature return online whilst interact learn environment apply game atari environment million frame algorithm achieve new state art performance',\n",
              " 'train model generalize new domains test time problem fundamental importance machine learn work encode notion domain generalization use novel regularization function pose problem find regularization function learn learn meta learn framework objective domain generalization explicitly model learn regularizer make model train one domain perform well another domain experimental validations computer vision natural language datasets indicate method learn regularizers achieve good cross domain generalization',\n",
              " 'consider problem sample constrain distributions pose significant challenge non asymptotic analysis algorithmic design propose unify framework inspire classical mirror descent derive novel first order sample scheme prove general target distribution strongly convex potential framework imply existence first order algorithm achieve epsilon convergence suggest state art epsilon vastly improve important latent dirichlet allocation lda application mind specialize algorithm sample dirichlet posteriors derive first non asymptotic epsilon rate first order sample extend framework mini batch set prove convergence rat stochastic gradients available finally report promise experimental result lda real datasets',\n",
              " 'complete data matrix become ubiquitous problem modern data science motivations recommender systems computer vision network inference name one typical assumption low rank general model assume column correspond one several low rank matrices paper generalize model call mixture matrix completion mmc case entry correspond one several low rank matrices mmc accurate model recommender systems bring flexibility completion cluster problems make four fundamental contributions new model first show mmc theoretically possible well pose second give precise information theoretic identifiability condition third derive sample complexity mmc finally give practical algorithm mmc performance comparable state art simpler relate problems synthetic real data',\n",
              " 'model interpretability increasingly important component practical machine learn common form interpretability systems example base local global explanations one main challenge interpretability design explanation systems capture aspects explanation type order develop thorough understand model address challenge novel model call maple use local linear model techniques along dual interpretation random forest supervise neighborhood approach feature selection method maple two fundamental advantage exist interpretability systems first effective black box explanation system maple highly accurate predictive model provide faithful self explanations thus sidestep typical accuracy interpretability trade specifically demonstrate several uci datasets maple least accurate random forest produce faithful local explanations lime popular interpretability system second maple provide example base local explanations detect global pattern allow diagnose limitations local explanations',\n",
              " 'propose novel class network model temporal dyadic interaction data objective capture important feature often observe social interactions sparsity degree heterogeneity community structure reciprocity use mutually excite hawk process model interactions direct pair individuals intensity process allow interactions arise responses opposite interactions reciprocity due share interest individuals community structure sparsity degree heterogeneity build non time dependent part intensity function compound random measure follow todeschini conduct experiment real world temporal interaction data show propose model outperform compete approach link prediction lead interpretable parameters',\n",
              " 'scale model capacity vital success deep learn typical network necessary compute resources train time grow dramatically model size conditional computation promise way increase number parameters relatively small increase resources propose train algorithm flexibly choose neural modules base data process decomposition modules learn end end contrast exist approach train rely regularization enforce diversity module use apply modular network image recognition language model task achieve superior performance compare several baselines introspection reveal modules specialize interpretable contexts',\n",
              " 'ensembles randomize decision tree usually refer random forest widely use classification regression task machine learn statistics random forest achieve competitive predictive performance computationally efficient train test make excellent candidates real world prediction task popular random forest variants breiman random forest extremely randomize tree operate batch train data online methods greater demand exist online random forest however require train data batch counterpart achieve comparable predictive performance work use mondrian process roy teh construct ensembles random decision tree call mondrian forest mondrian forest grow incremental online fashion remarkably distribution online mondrian forest batch mondrian forest mondrian forest achieve competitive predictive performance comparable exist online random forest periodically train batch random forest order magnitude faster thus represent better computation accuracy tradeoff',\n",
              " 'contextual bandit literature traditionally focus algorithms address exploration exploitation tradeoff particular greedy algorithms exploit current estimate without exploration may sub optimal general however exploration free greedy algorithms desirable practical settings exploration may costly unethical clinical trials surprisingly find simple greedy algorithm rate optimal achieve asymptotically optimal regret sufficient randomness observe contexts covariates prove always case two arm bandit general class context distributions satisfy condition term covariate diversity furthermore even absent condition show greedy algorithm rate optimal positive probability thus standard bandit algorithms may unnecessarily explore motivate result introduce greedy first new algorithm use observe contexts reward determine whether follow greedy algorithm explore prove algorithm rate optimal without additional assumptions context distribution number arm extensive simulations demonstrate greedy first successfully reduce exploration outperform exist exploration base contextual bandit algorithms thompson sample upper confidence bind ucb',\n",
              " 'paper address general problem blind echo retrieval give sensors measure discrete time domain mixtures delay attenuate copy unknown source signal echo location weight recover problem broad applications field sonars seismology ultrasounds room acoustics belong broader class blind channel identification problems intensively study signal process exist methods proceed two step blind estimation sparse discrete time filter echo information retrieval peak pick precision methods fundamentally limit rate signal sample estimate echo locations necessary grid since true locations never match sample grid weight estimation precision also strongly limit call basis mismatch problem compress sense propose radically different approach problem build top framework finite rate innovation sample approach operate directly parameter space echo locations weight enable near exact blind grid echo retrieval discrete time measurements show outperform conventional methods several order magnitudes precision',\n",
              " 'paper study generalization performance multi class classification obtain shaper data dependent generalization error bind fast convergence rate substantially improve state art bound exist data dependent generalization analysis theoretical analysis motivate devise two effective multi class kernel learn algorithms statistical guarantee experimental result show propose methods significantly outperform exist multi class classification methods',\n",
              " 'occurrence multiple diseases among general population important problem patients risk complications represent large share health care expenditure learn predict time event probabilities patients challenge problem risk events correlate compete risk often patients experience individual events interest fraction actually observe data introduce paper survival model flexibility leverage common representation relate events design correct strong imbalance observe outcomes procedure sequential outcome specific survival distributions form components nonparametric multivariate estimators combine ensemble way ensure accurate predictions outcome type simultaneously algorithm general represent first boost like method time event data multiple outcomes demonstrate performance algorithm synthetic real data',\n",
              " 'multi task learn multiple task solve jointly share inductive bias multi task learn inherently multi objective problem different task may conflict necessitate trade common compromise optimize proxy objective minimize weight linear combination per task losses however workaround valid task compete rarely case paper explicitly cast multi task learn multi objective optimization overall objective find pareto optimal solution end use algorithms develop gradient base multi objective optimization literature algorithms directly applicable large scale learn problems since scale poorly dimensionality gradients number task therefore propose upper bind multi objective loss show optimize efficiently prove optimize upper bind yield pareto optimal solution realistic assumptions apply method variety multi task deep learn problems include digit classification scene understand joint semantic segmentation instance segmentation depth estimation multi label classification method produce higher perform model recent multi task learn formulations per task train',\n",
              " 'numerous deep learn applications benefit multi task learn multiple regression classification objectives paper make observation performance systems strongly dependent relative weight task loss tune weight hand difficult expensive process make multi task learn prohibitive practice propose principled approach multi task deep learn weigh multiple loss function consider homoscedastic uncertainty task allow simultaneously learn various quantities different units scale classification regression settings demonstrate model learn per pixel depth regression semantic instance segmentation monocular input image perhaps surprisingly show model learn multi task weight outperform separate model train individually task',\n",
              " 'multivariate time series usually contain large number miss value hinder application advance analysis methods multivariate time series data conventional approach address challenge miss value include mean zero imputation case deletion matrix factorization base imputation incapable model temporal dependencies nature complex distribution multivariate time series paper treat problem miss value imputation data generation inspire success generative adversarial network gin image generation propose learn overall distribution multivariate time series dataset gin use generate miss value sample different image data time series data usually incomplete due nature data record process modify gate recurrent unit employ gin model temporal irregularity incomplete time series experiment two multivariate time series datasets show propose model outperform baselines term accuracy imputation experimental result also show simple model impute data achieve state art result prediction task demonstrate benefit model downstream applications',\n",
              " 'bayesian optimisation refer class methods global optimisation function accessible via point evaluations typically use settings expensive evaluate common use case machine learn model selection possible analytically model generalisation performance statistical model resort noisy expensive train validation procedures choose best model conventional methods focus euclidean categorical domains context model selection permit tune scalar hyper parameters machine learn algorithms however surge interest deep learn increase demand tune neural network architectures work develop nasbot gaussian process base framework neural architecture search accomplish develop distance metric space neural network architectures compute efficiently via optimal transport program distance might independent interest deep learn community may find applications outside demonstrate nasbot outperform alternatives architecture search several cross validation base model selection task multi layer perceptrons convolutional neural network',\n",
              " 'non local methods exploit self similarity natural signal well study example image analysis restoration exist approach however rely nearest neighbor knn match fix feature space main hurdle optimize feature space application performance non differentiability knn selection rule overcome propose continuous deterministic relaxation knn selection maintain differentiability pairwise distance retain original knn limit temperature parameter approach zero exploit relaxation propose neural nearest neighbor block block novel non local process layer leverage principle self similarity use build block modern neural network architectures show effectiveness set reason task correspondence classification well image restoration include image denoising single image super resolution outperform strong convolutional neural network cnn baselines recent non local model rely knn selection hand choose feature space',\n",
              " 'extend capabilities neural network couple external memory resources interact attentional process combine system analogous turing machine von neumann architecture differentiable end end allow efficiently train gradient descent preliminary result demonstrate neural turing machine infer simple algorithms copy sort associative recall input output examples',\n",
              " 'marry two powerful ideas deep representation learn visual recognition language understand symbolic program execution reason neural symbolic visual question answer vqa system first recover structural scene representation image program trace question execute program scene representation obtain answer incorporate symbolic structure prior knowledge offer three unique advantage first execute program symbolic space robust long program trace model solve complex reason task better achieve accuracy clevr dataset second model data memory efficient perform well learn small number train data also encode image compact representation require less storage exist methods offline question answer third symbolic program execution offer full transparency reason process thus able interpret diagnose execution step',\n",
              " 'incremental gradient algorithm hybrid stochastic gradient descent hsgd enjoy merit stochastic full gradient methods finite sum minimization problem however exist rate convergence analysis hsgd make replacement sample wrs restrict convex problems clear whether hsgd still carry advantage common practice without replacement sample wors non convex problems paper affirmatively answer open question show wors convex non convex problems still possible hsgd constant step size match full gradient descent rate convergence maintain comparable sample size independent incremental first order oracle complexity stochastic gradient descent special class finite sum problems linear prediction model convergence result improve case extensive numerical result confirm theoretical affirmation demonstrate favorable efficiency wors base hsgd',\n",
              " 'identify fundamental source error learn form dynamic program function approximation delusional bias arise approximation architecture limit class expressible greedy policies since standard update make globally uncoordinated action choices respect expressible policy class inconsistent even conflict value estimate result lead pathological behaviour estimation instability even divergence solve problem introduce new notion policy consistency define local backup process ensure global consistency use information set set record constraints policies consistent back value prove model base model free algorithms use backup remove delusional bias yield first know algorithms guarantee optimal result general condition algorithms furthermore require polynomially many information set potentially exponential support finally suggest practical heuristics value iteration learn attempt reduce delusional bias',\n",
              " 'nonlocal neural network propose show effective several computer vision task nonlocal operations directly capture long range dependencies feature space paper study nature diffusion damp effect nonlocal network spectrum analysis weight matrices well train network propose new formulation nonlocal block new block learn nonlocal interactions also stable dynamics thus allow deeper nonlocal structure moreover interpret formulation general nonlocal model perspective make connections propose nonlocal network nonlocal model nonlocal diffusion process markov jump process',\n",
              " 'many classic methods show non local self similarity natural image effective prior image restoration however remain unclear challenge make use intrinsic property via deep network paper propose non local recurrent network nlrn first attempt incorporate non local operations recurrent neural network rnn image restoration main contributions work unlike exist methods measure self similarity isolate manner propose non local module flexibly integrate exist deep network end end train capture deep feature correlation location neighborhood fully employ rnn structure parameter efficiency allow deep feature correlation propagate along adjacent recurrent state new design boost robustness inaccurate correlation estimation due severely degrade image show essential maintain confine neighborhood compute deep feature correlation give degrade image contrast exist practice deploy whole image extensive experiment image denoising super resolution task conduct thank recurrent non local operations correlation propagation propose nlrn achieve superior result state art methods many fewer parameters',\n",
              " 'paper consider parallelization applications whose objective express maximize non monotone submodular function cardinality constraint main result algorithm whose approximation arbitrarily close log adaptive round size grind set exponential speedup parallel run time previously study algorithm constrain non monotone submodular maximization beyond provable guarantee algorithm perform well practice specifically experiment traffic monitor personalize data summarization applications show algorithm find solutions whose value competitive state art algorithms run exponentially fewer parallel iterations',\n",
              " 'bayesian learn build assumption model space contain true reflection data generate mechanism assumption problematic particularly complex data environments present bayesian nonparametric approach learn make use statistical model assume model true approach provably better properties use parametric model admit monte carlo sample scheme afford massive scalability modern computer architectures model base aspect learn particularly attractive regularize nonparametric inference sample size small also correct approximate approach variational bay demonstrate approach number examples include classifiers bayesian random forest',\n",
              " 'problem estimate unknown discrete distribution sample fundamental tenet statistical learn past decade attract significant research effort solve variety divergence measure surprisingly equally important problem estimate unknown markov chain sample still far understand consider two problems relate min max risk expect loss estimate unknown state markov chain sequential sample predict conditional distribution next sample respect divergence estimate transition matrix respect natural loss induce general divergence measure',\n",
              " 'study computational tractability pac reinforcement learn rich observations present new provably sample efficient algorithms environments deterministic hide state dynamics stochastic rich observations methods operate oracle model computation access policy value function class exclusively standard optimization primitives therefore represent computationally efficient alternatives prior algorithms require enumeration stochastic hide state dynamics prove know sample efficient algorithm olive implement oracle model also present several examples illustrate fundamental challenge tractable pac reinforcement learn general settings',\n",
              " 'paper provide theoretical understand word embed dimensionality motivate unitary invariance word embed propose pairwise inner product pip loss novel metric dissimilarity word embeddings use techniques matrix perturbation theory reveal fundamental bias variance trade dimensionality selection word embeddings bias variance trade shed light many empirical observations previously unexplained example existence optimal dimensionality moreover new insights discoveries like word embeddings robust fit reveal optimize bias variance trade pip loss explicitly answer open question dimensionality selection word embed',\n",
              " 'consider problem online learn linear contextual bandits set also strong individual fairness constraints govern unknown similarity metric constraints demand select similar action individuals approximately equal probability dhprz may odds optimize reward thus model settings profit social policy tension assume learn unknown mahalanobis similarity metric weak feedback identify fairness violations quantify extent intend represent interventions regulator know unfairness see nevertheless enunciate quantitative fairness metric individuals main result algorithm adversarial context set number fairness violations depend logarithmically obtain optimal sqrt regret bind best fair policy',\n",
              " 'consider problem maximize submodular function give access approximate version submodular function heavily study wide variety discipline since use model many real world phenomena amenable optimization however many case phenomena observe approximately submodular approximation guarantee cease hold describe technique call sample mean approximation yield strong guarantee maximization submodular function approximate surrogates cardinality intersection matroid constraints particular show tight guarantee maximization cardinality constraint approximation intersection matroids',\n",
              " 'accurately answer question give image require combine observations general knowledge effortless humans reason general knowledge remain algorithmic challenge advance research direction novel reason correct answer jointly consider entities show challenge fvqa dataset lead improvement accuracy around compare state art',\n",
              " 'capacity neural network absorb information limit number parameters conditional computation part network active per example basis propose theory way dramatically increase model capacity without proportional increase computation practice however significant algorithmic performance challenge work address challenge finally realize promise conditional computation achieve greater improvements model capacity minor losses computational efficiency modern gpu cluster introduce sparsely gate mixture experts layer moe consist thousands fee forward sub network trainable gate network determine sparse combination experts use example apply moe task language model machine translation model capacity critical absorb vast quantities knowledge available train corpora present model architectures moe billion parameters apply convolutionally stack lstm layer large language model machine translation benchmarks model achieve significantly better result state art lower computational cost',\n",
              " 'modern visual question answer vqa model show rely heavily superficial correlations question answer word learn train overwhelmingly report type room kitchen sport play tennis irrespective image alarmingly shortcoming often well reflect evaluation strong priors exist test distributions however vqa system fail grind question image content would likely perform poorly real world settings',\n",
              " 'people belong multiple communities word belong multiple topics book cover multiple genres overlap cluster commonplace many exist overlap cluster methods model person word book non negative weight combination exemplars belong solely one community small noise geometrically person point cone whose corner exemplars basic form encompass widely use mix membership stochastic blockmodel network degree correct variants well topic model lda show simple one class svm yield provably consistent parameter inference model scale large datasets experimental result several simulate real datasets show algorithm call svm cone accurate scalable',\n",
              " 'propose study new model reinforcement learn rich observations generalize contextual bandits sequential decision make model require agent take action base observations feature goal achieve long term performance competitive large set policies avoid barriers sample efficient learn associate large observation space general pomdps focus problems summarize small number hide state long term reward predictable reactive function class set design analyze new reinforcement learn algorithm least square value elimination exploration prove algorithm learn near optimal behavior number episodes polynomial relevant parameters logarithmic number policies independent size observation space result provide theoretical justification reinforcement learn function approximation',\n",
              " 'generative adversarial network gans technique learn generative model complex data distributions sample despite remarkable advance generate realistic image major shortcoming gans fact tend produce sample little diversity even train diverse datasets phenomenon know mode collapse focus much recent work study principled approach handle mode collapse call pack main idea modify discriminator make decisions base multiple sample class either real artificially generate draw analysis tool binary hypothesis test particular seminal result blackwell prove fundamental connection pack mode collapse show pack naturally penalize generators mode collapse thereby favor generator distributions less mode collapse train process numerical experiment benchmark datasets suggest pack provide significant improvements',\n",
              " 'existence evasion attack test phase machine learn algorithms represent significant challenge deployment understand attack carry add imperceptible perturbations input generate adversarial examples find effective defenses detectors prove difficult paper step away attack defense arm race seek understand limit learn presence evasion adversary particular extend probably approximately correct pac learn framework account presence adversary first define corrupt hypothesis class arise standard binary hypothesis class presence evasion adversary derive vapnik chervonenkis dimension denote adversarial dimension show sample complexity upper bound fundamental theorem statistical learn extend case evasion adversaries sample complexity control adversarial dimension explicitly derive adversarial dimension halfspace classifiers presence sample wise norm constrain adversary type commonly study evasion attack show standard dimension close open question finally prove adversarial dimension either larger smaller standard dimension depend hypothesis class adversary make interest object study right',\n",
              " 'propose parsimonious quantile regression framework learn dynamic tail behaviors financial asset return model capture well time vary characteristic asymmetrical heavy tail property financial time series combine merit popular sequential neural network model lstm novel parametric quantile function construct represent conditional distribution asset return model also capture individually serial dependences higher moments rather volatility across wide range asset class sample forecast conditional quantiles var model outperform garch family propose approach suffer issue quantile cross expose ill posedness compare parametric probability density function approach',\n",
              " 'image caption model become increasingly successful describe content image restrict domains however model function wild example assistants people impair vision much larger number variety visual concepts must understand address problem teach image caption model new visual concepts label image object detection datasets since image label object class interpret partial caption formulate problem learn partially specify sequence data propose novel algorithm train sequence model recurrent neural network partially specify sequence represent use finite state automata context image caption method lift restriction previously require image caption model train pair image sentence corpora otherwise require specialize model architectures take advantage alternative data modalities apply approach exist neural caption model achieve state art result novel object caption task use coco dataset show train caption model describe new visual concepts open image dataset maintain competitive coco evaluation score',\n",
              " 'increase need run convolutional neural network cnn model mobile devices limit compute power memory resource encourage study efficient model design number efficient architectures propose recent years example mobilenet shufflenet mobilenetv however model heavily dependent depthwise separable convolution lack efficient implementation deep learn frameworks study propose efficient architecture name peleenet build conventional convolution instead imagenet ilsvrc dataset propose peleenet achieve higher accuracy time faster speed mobilenet mobilenetv nvidia meanwhile peleenet model size mobilenet propose real time object detection system combine peleenet single shoot multibox detector ssd method optimize architecture fast speed propose detection system name pelee achieve map mean average precision pascal voc map coco dataset speed fps iphone fps nvidia result coco outperform yolov consideration higher precision time lower computational cost time smaller model size code model open source',\n",
              " 'present simple general framework feature learn point cloud key success cnns convolution operator capable leverage spatially local correlation data represent densely grids image however point cloud irregular unordered thus direct convolve kernels feature associate point result desert shape information variant order address problems propose learn transformation input point use simultaneously weight input feature associate point permute latent potentially canonical order element wise product sum operations typical convolution operator apply transform feature propose method generalization typical cnns learn feature point cloud thus call pointcnn experiment show pointcnn achieve par better performance state art methods multiple challenge benchmark datasets task',\n",
              " 'introduce spike slab deep learn fully bayesian alternative dropout improve generalizability deep relu network new type regularization enable provable recovery smooth input output map unknown level smoothness indeed show posterior distribution concentrate near minimax rate alpha holder smooth map perform well know smoothness level alpha ahead time result shed light architecture design deep neural network namely choice depth width sparsity level network attribute typically depend unknown smoothness order optimal obviate constraint fully bay construction aside show overfit sense posterior concentrate smaller network fewer optimal number nod link result provide new theoretical justifications deep relu network bayesian point view',\n",
              " 'classical anomaly detection principally concern point base anomalies anomalies occur single point time yet many real world anomalies range base mean occur period time motivate observation present new mathematical model evaluate accuracy time series classification algorithms model expand well know precision recall metrics measure range simultaneously enable customization support domain specific preferences',\n",
              " 'formulate private learn model study intrinsic tradeoff privacy query complexity sequential learn model involve learner aim determine scalar value sequentially query external database receive binary responses meantime adversary observe learner query though responses try infer value objective learner obtain accurate estimate use small number query simultaneously protect privacy make provably difficult learn adversary main result provide tight upper lower bound learner query complexity function desire level privacy estimation accuracy also construct explicit query strategies whose complexity optimal additive constant',\n",
              " 'learn solve complex sequence task leverage transfer avoid catastrophic forget remain key obstacle achieve human level intelligence progressive network approach represent step forward direction immune forget leverage prior knowledge via lateral connections previously learn feature evaluate architecture extensively wide variety reinforcement learn task atari maze game show outperform common baselines base pretraining finetuning use novel sensitivity measure demonstrate transfer occur low level sensory high level control layer learn policy',\n",
              " 'propose prototypical network problem shoot classification classifier must generalize new class see train set give small number examples new class prototypical network learn metric space classification perform compute distance prototype representations class compare recent approach shoot learn reflect simpler inductive bias beneficial limit data regime achieve excellent result provide analysis show simple design decisions yield substantial improvements recent approach involve complicate architectural choices meta learn extend prototypical network zero shoot learn achieve state art result bird dataset',\n",
              " 'propose new family policy gradient methods reinforcement learn alternate sample data interaction environment optimize surrogate objective function use stochastic gradient ascent whereas standard policy gradient methods perform one gradient update per data sample propose novel objective function enable multiple epochs minibatch update new methods call proximal policy optimization ppo benefit trust region policy optimization trpo much simpler implement general better sample complexity empirically experiment test ppo collection benchmark task include simulate robotic locomotion atari game play show ppo outperform online policy gradient methods overall strike favorable balance sample complexity simplicity wall time',\n",
              " 'parallel implementations stochastic gradient descent sgd receive significant research attention thank excellent scalability properties algorithm efficiency context train deep neural network fundamental barrier parallelize large scale sgd fact cost communicate gradient update nod large consequently lossy compression heuristics propose nod communicate quantize gradients although effective practice heuristics always provably converge clear whether optimal paper propose quantize sgd qsgd family compression scheme allow compression gradient update node guarantee convergence standard assumptions qsgd allow user trade compression convergence time communicate sublinear number bits per iteration model dimension achieve asymptotically optimal communication cost complement theoretical result empirical data show qsgd significantly reduce communication cost competitive standard uncompress techniques variety real task particular experiment show gradient quantization apply train deep neural network image classification automate speech recognition lead significant reductions communication cost end end train time instance gpus able train resnet network imagenet faster full accuracy note show exist generic parameter settings know network architectures preserve slightly improve full accuracy use quantization',\n",
              " 'introduce new convex optimization problem term quadratic decomposable submodular function minimization problem closely relate decomposable submodular function minimization arise many learn graph hypergraphs settings graph base semi supervise learn pagerank approach problem via new dual strategy describe objective may optimize via random coordinate descent rcd methods projections onto con also establish linear convergence rate rcd algorithm develop efficient projection algorithms provable performance guarantee numerical experiment semi supervise learn hypergraphs confirm efficiency propose algorithm demonstrate significant improvements prediction accuracy respect state art methods',\n",
              " 'study consistency properties machine learn methods base minimize convex surrogates extend recent framework osokin quantitative analysis consistency properties case inconsistent surrogates key technical contribution consist new lower bind calibration function quadratic surrogate non trivial always zero inconsistent case new bind allow quantify level inconsistency set show learn inconsistent surrogates guarantee sample complexity optimization difficulty apply theory two concrete case multi class classification tree structure loss rank mean average precision loss result show approximation computation trade off cause inconsistent surrogates potential benefit',\n",
              " 'paucity videos current action classification datasets ucf hmdb make difficult identify good video architectures methods obtain similar performance exist small scale benchmarks paper evaluate state art architectures light new kinetics human action video dataset kinetics two order magnitude data human action class clip per class collect realistic challenge youtube videos provide analysis current architectures fare task action classification dataset much performance improve smaller benchmark datasets pre train kinetics also introduce new two stream inflate convnet base convnet inflation filter pool kernels deep image classification convnets expand make possible learn seamless spatio temporal feature extractors video leverage successful imagenet architecture design even parameters show pre train kinetics model considerably improve upon state art action classification reach hmdb ucf',\n",
              " 'computable stein discrepancies deploy variety applications range sampler selection posterior inference approximate bayesian inference goodness fit test exist convergence determine stein discrepancies admit strong theoretical guarantee suffer computational cost grow quadratically sample size linear time stein discrepancies propose goodness fit test exhibit avoidable degradations test power even power explicitly optimize address shortcomings introduce feature stein discrepancies sds new family quality measure cheaply approximate use importance sample show construct sds provably determine convergence sample target develop high accuracy approximations random sds sds computable near linear time experiment sampler selection approximate posterior inference goodness fit test sds perform well better quadratic time ksds order magnitude faster compute',\n",
              " 'generative recurrent neural network quickly train unsupervised manner model popular reinforcement learn environments compress spatio temporal representations world model extract feature feed compact simple policies train evolution achieve state art result various environments also train agent entirely inside environment generate internal world model transfer policy back actual environment interactive version paper available https worldmodels github',\n",
              " 'despite impressive performance deep neural network dnns typically underperform gradient boost tree gbts many tabular dataset learn task propose apply different regularization coefficient weight might boost performance dnns allow make use relevant input however lead intractable number hyperparameters introduce regularization learn network rlns overcome challenge introduce efficient hyperparameter tune scheme minimize new counterfactual loss result show rlns significantly improve dnns tabular datasets achieve comparable result gbts best performance achieve ensemble combine gbts rlns rlns produce extremely sparse network eliminate network edge input feature thus provide interpretable model reveal importance network assign different input rlns could efficiently learn single network datasets comprise tabular unstructured data set medical image accompany electronic health record open source implementation rln find https github com irashavitt regularizationnetworks',\n",
              " 'normalization techniques play important role support efficient often effective train deep neural network conventional methods explicitly normalize activations suggest add loss term instead new loss term encourage variance activations stable vary one random mini batch next prove encourage activations distribute around distinct modes also show input mixture two gaussians new loss would either join two together separate optimally lda sense depend prior probabilities finally able link new regularization term batchnorm method provide regularization perspective experiment demonstrate improvement accuracy batchnorm technique cnns fully connect network',\n",
              " 'artificial intelligence model limit ability solve new task faster without forget previously acquire knowledge recently emerge paradigm continual learn aim solve issue model learn various task sequential fashion work novel approach continual learn propose search best neural architecture come task via sophisticatedly design reinforcement learn strategies name reinforce continual learn method good performance prevent catastrophic forget also fit new task well experiment sequential classification task variants mnist cifar datasets demonstrate propose approach outperform exist continual learn alternatives deep network',\n",
              " 'statistical leverage score emerge fundamental tool matrix sketch column sample applications low rank approximation regression random feature learn quadrature yet nature quantity barely understand borrow ideas orthogonal polynomial literature introduce regularize christoffel function associate positive definite kernel uncover variational formulation leverage score kernel methods allow elucidate relationships choose kernel well population density main result quantitatively describe decrease relation leverage score population density broad class kernels euclidean space numerical simulations support find',\n",
              " 'multiplicative noise include dropout widely use regularize deep neural network dnns show effective wide range architectures task information perspective consider inject multiplicative noise dnn train network solve task noisy information pathways lead observation multiplicative noise tend increase correlation feature increase signal noise ratio information pathways however high feature correlation undesirable increase redundancy representations work propose non correlate multiplicative noise ncmn exploit batch normalization remove correlation effect simple yet effective way show ncmn significantly improve performance standard multiplicative noise image classification task provide better alternative dropout batch normalize network additionally present unify view ncmn shake shake regularization explain performance gain latter',\n",
              " 'study problem policy policy evaluation oppe contrast prior work consider estimate individual policy value average policy value accurately draw inspiration recent work causal reason propose new finite sample generalization error bind value estimate mdp model use upper bind objective develop learn algorithm mdp model balance representation show approach yield substantially lower mse common synthetic benchmarks hiv treatment simulation domain',\n",
              " 'estimate individual treatment effect ite challenge problem causal inference due miss counterfactuals selection bias exist ite estimation methods mainly focus balance distributions control treat group ignore local similarity information helpful paper propose local similarity preserve individual treatment effect site estimation method base deep representation learn site preserve local similarity balance data distributions simultaneously focus several hard sample mini batch experimental result synthetic three real world datasets demonstrate advantage propose site method compare state art ite estimation methods',\n",
              " 'propose structure adaptive variant state art stochastic variance reduce gradient algorithm katyusha regularize empirical risk minimization propose method able exploit intrinsic low dimensional structure solution sparsity low rank enforce non smooth regularization achieve even faster convergence rate provable algorithmic improvement do restart katyusha algorithm accord restrict strong convexity constants demonstrate effectiveness approach via numerical experiment',\n",
              " 'convolutional network core state art computer vision solutions wide variety task since deep convolutional network start become mainstream yield substantial gain various benchmarks although increase model size computational cost tend translate immediate quality gain task long enough label data provide train computational efficiency low parameter count still enable factor various use case mobile vision big data scenarios explore ways scale network ways aim utilize add computation efficiently possible suitably factorize convolutions aggressive regularization benchmark methods ilsvrc classification challenge validation set demonstrate substantial gain state art top top error single frame evaluation use network computational cost billion multiply add per inference use less million parameters ensemble model multi crop evaluation report top error validation set error test set top error validation set',\n",
              " 'introduce new approach decomposable submodular function minimization dsfm exploit incidence relations incidence relations describe variables effectively influence component function properly utilize allow improve convergence rat dsfm solvers main result include precise parametrization dsfm problem base incidence relations development new scalable alternative projections parallel coordinate descent methods accompany rigorous analysis convergence rat',\n",
              " 'multi task learn mtl appeal deep learn regularization paper tackle specific mtl context denote primary mtl ultimate goal improve performance give primary task leverage several auxiliary task main methodological contribution introduce rock new generic multi modal fusion block deep learn tailor primary mtl context rock architecture base residual connection make forward prediction explicitly impact intermediate auxiliary representations auxiliary predictor architecture also specifically design primary mtl context incorporate intensive pool operators maximize complementarity intermediate representations extensive experiment nyuv dataset object detection scene classification depth prediction surface normal estimation auxiliary task validate relevance approach superiority flat mtl approach method outperform state art object detection model nyuv dataset large margin also able handle large scale heterogeneous input real synthetic image miss annotation modalities',\n",
              " 'note consider normalize gradient descent ngd natural modification classical gradient descent optimization problems serious shortcoming non convex problems may take arbitrarily long escape neighborhood saddle point issue make convergence arbitrarily slow particularly high dimensional non convex problems relative number saddle point often large paper focus continuous time descent show contrary standard ngd escape saddle point quickly particular show ngd almost never converge saddle point time require ngd escape ball radius saddle point sqrt kappa kappa condition number hessian application result global convergence time bind establish ngd mild assumptions',\n",
              " 'object detection performance measure canonical pascal voc dataset plateaued last years best perform methods complex ensemble systems typically combine multiple low level image feature high level context paper propose simple scalable detection algorithm improve mean average precision map relative previous best result voc achieve map approach combine two key insights one apply high capacity convolutional neural network cnns bottom region proposals order localize segment object label train data scarce supervise pre train auxiliary task follow domain specific fine tune yield significant performance boost since combine region proposals cnns call method cnn regions cnn feature also compare cnn overfeat recently propose slide window detector base similar cnn architecture find cnn outperform overfeat large margin class ilsvrc detection dataset source code complete system available http www berkeley edu rbg rcnn',\n",
              " 'singular value decomposition principal component analysis one widely use techniques dimensionality reduction successful efficiently computable nevertheless plague well know well document sensitivity outliers recent work consider set point arbitrarily corrupt components yet applications svd pca robust collaborative filter bioinformatics malicious agents defective genes simply corrupt contaminate experiment may effectively yield entire point completely corrupt present efficient convex optimization base algorithm call outlier pursuit mild assumptions uncorrupted point satisfy standard generative assumption pca problems recover exact optimal low dimensional subspace identify corrupt point identification corrupt point conform low dimensional approximation paramount interest bioinformatics financial applications beyond techniques involve matrix decomposition use nuclear norm minimization however result setup approach necessarily differ considerably exist line work matrix completion matrix decomposition since develop approach recover correct column space uncorrupted matrix rather exact matrix problem one seek recover structure rather exact initial matrices techniques develop thus far rely certificate optimality fail present important extension methods allow treatment problems',\n",
              " 'paper propose stochastic recursive gradient algorithm sarah well practical variant sarah novel approach finite sum minimization problems different vanilla sgd modern stochastic methods svrg sag saga sarah admit simple recursive framework update stochastic gradient estimate compare sag saga sarah require storage past gradients linear convergence rate sarah prove strong convexity assumption also prove linear convergence rate strongly convex case inner loop sarah property svrg possess numerical experiment demonstrate efficiency algorithm',\n",
              " 'semantic scene completion predict volumetric occupancy object category scene help intelligent agents understand interact surround work propose disentangle framework sequentially carry semantic segmentation reprojection semantic scene completion three stage framework three advantage explicit semantic segmentation significantly boost performance flexible fusion ways sensor data bring good extensibility progress subtask promote holistic performance experimental result show regardless input single depth rgb framework generate high quality semantic scene completion outperform state art approach synthetic real datasets',\n",
              " 'propose novel randomize first order optimization method sega sketch gradient method progressively throughout iterations build variance reduce estimate gradient random linear measurements sketch gradient provide iteration oracle iteration sega update current estimate gradient sketch project operation use information provide latest sketch subsequently use compute unbiased estimate true gradient random relaxation procedure unbiased estimate use perform gradient step unlike standard subspace descent methods coordinate descent sega use optimization problems non separable proximal term provide general convergence analysis prove linear convergence strongly convex objectives special case coordinate sketch sega enhance various techniques importance sample minibatching acceleration rate small constant factor identical best know rate coordinate descent',\n",
              " 'recently adversarial erase weakly supervise object attention deeply study due capability localize integral object regions however strategy raise one key problem attention regions gradually expand non object regions train iterations continue significantly decrease quality produce attention map tackle issue well promote quality object attention introduce simple yet effective self erase network seenet prohibit attentions spread unexpected background regions particular seenet leverage two self erase strategies encourage network use reliable object background cue learn attention way integral object regions effectively highlight without include much background regions test quality generate attention map employ mine object regions heuristic cue learn semantic segmentation model experiment pascal voc well demonstrate superiority seenet state art methods',\n",
              " 'introduce approach convert mono audio record video camera spatial audio representation distribution sound full view sphere spatial audio important component immersive video view spatial audio microphones still rare current video production system consist end end trainable neural network separate individual sound source localize view sphere condition multi modal analysis audio video frame introduce several datasets include one film one collect wild youtube consist videos upload spatial audio train grind truth spatial audio serve self supervision mix mono track form input network use approach show possible infer spatial localization sound base synchronize video mono audio track',\n",
              " 'paper focus semantic scene completion task produce complete voxel representation volumetric occupancy semantic label scene single view depth map observation previous work consider scene completion semantic label depth map separately however observe two problems tightly intertwine leverage couple nature two task introduce semantic scene completion network sscnet end end convolutional network take single depth image input simultaneously output occupancy semantic label voxels camera view frustum network use dilation base context module efficiently expand receptive field enable context learn train network construct suncg manually create large scale dataset synthetic scenes dense volumetric annotations experiment demonstrate joint model outperform methods address task isolation outperform alternative approach semantic scene completion task',\n",
              " 'role semantics zero shoot learn consider effectiveness previous approach analyze accord form supervision provide learn semantics independently others supervise semantic subspace explain train class thus former able constrain whole space lack ability model semantic correlations latter address issue leave part semantic space unsupervised complementarity exploit new convolutional neural network cnn framework propose use semantics constraints recognition although cnn train classification transfer ability encourage learn hide semantic layer together semantic code classification two form semantic constraints introduce first loss base regularizer introduce generalization constraint semantic predictor second codeword regularizer favor semantic class mappings consistent prior semantic knowledge allow learn data significant improvements state art achieve several datasets',\n",
              " 'duplicate removal critical step accomplish reasonable amount predictions prevalent proposal base object detection frameworks albeit simple effective previous algorithms utilize greedy process without make sufficient use properties input data work design new two stage framework effectively select appropriate proposal candidate object first stage suppress easy negative object proposals second stage select true positives reduce proposal set two stag share network structure encoder decoder form recurrent neural network rnn global attention context gate encoder scan proposal candidates sequential manner capture global context information feed decoder extract optimal proposals extensive experiment propose method outperform alternatives large margin',\n",
              " 'present shapenet richly annotate large scale repository shape represent cad model object shapenet contain model multitude semantic categories organize wordnet taxonomy collection datasets provide many semantic annotations model consistent rigid alignments part bilateral symmetry plan physical size keywords well plan annotations annotations make available public web base interface enable data visualization object attribute promote data drive geometric analysis provide large scale quantitative benchmark research computer graphics vision time technical report shapenet index model model classify categories wordnet synsets report describe shapenet effort whole provide detail currently available datasets summarize future plan',\n",
              " 'softmax output activation function model categorical probability distributions many applications deep learn however recent study reveal softmax bottleneck representational capacity neural network language model softmax bottleneck paper propose output activation function break softmax bottleneck without additional parameters analyze softmax bottleneck perspective output set log softmax identify cause softmax bottleneck basis analysis propose sigsoftmax compose multiplication exponential function sigmoid function sigsoftmax break softmax bottleneck experiment language model demonstrate sigsoftmax mixture sigsoftmax outperform softmax mixture softmax respectively',\n",
              " 'model free reinforcement learn aim offer shelf solutions control dynamical systems without require model system dynamics introduce model free random search algorithm train static linear policies continuous control problems common evaluation methodology show method match state art sample efficiency benchmark mujoco locomotion task nonetheless rigorous evaluation reveal assessment performance benchmarks optimistic evaluate performance method hundreds random seed many different hyperparameter configurations benchmark task extensive evaluation possible small computational footprint method simulations highlight high variability performance benchmark task indicate commonly use estimations sample efficiency adequately evaluate performance algorithms result stress need new baselines benchmarks evaluation methodology algorithms',\n",
              " 'configure deep spike neural network snns excite research avenue low power spike event base computation however spike generation function non differentiable therefore directly compatible standard error backpropagation algorithm paper introduce new general backpropagation mechanism learn synaptic weight axonal delay overcome problem non differentiability spike function use temporal credit assignment policy backpropagating error precede layer describe release gpu accelerate software implementation method allow train fully connect convolutional neural network cnn architectures use software compare method exist snn base learn approach standard ann snn conversion techniques show method achieve state art performance snn mnist nmnist dvs gesture tidigits datasets',\n",
              " 'describe new software framework fast train generalize linear model framework name snap machine learn snap combine recent advance machine learn systems algorithms nest manner reflect hierarchical architecture modern compute systems prove theoretically hierarchical system accelerate train distribute environments intra node communication cheaper inter node communication additionally provide review implementation snap term gpu acceleration pipelining communication pattern software architecture highlight aspects critical achieve high performance evaluate performance snap single node multi node environments quantify benefit hierarchical scheme data stream functionality compare widely use machine learn software frameworks finally present logistic regression benchmark criteo terabyte click log dataset show snap achieve test loss order magnitude faster previously report result include obtain use tensorflow scikit learn',\n",
              " 'despite remarkable advance image synthesis research exist work often fail manipulate image context large geometric transformations synthesize person image condition arbitrary pose one representative examples generation quality largely rely capability identify model arbitrary transformations different body part current generative model often build local convolutions overlook key challenge heavy occlusions different view dramatic appearance change distinct geometric change happen part cause arbitrary pose manipulations paper aim resolve challenge induce geometric variability spatial displacements via new soft gate warp generative adversarial network warp gin compose two stag first synthesize target part segmentation map give target pose depict region level spatial layouts guide image synthesis higher level structure constraints warp gin equip soft gate warp block learn feature level map render textures original image generate segmentation map warp gin capable control different transformation degrees give distinct target pose moreover propose warp block light weight flexible enough inject network human perceptual study quantitative evaluations demonstrate superiority warp gin significantly outperform exist methods two large datasets',\n",
              " 'tremendous recent progress equilibrium find algorithms zero sum imperfect information extensive form game puzzle gap theory practice first order methods significantly better theoretical convergence rat counterfactual regret minimization cfr variant despite cfr variants favor practice experiment first order methods conduct small medium size game methods complicate implement set cfr variants enhance extensively decade perform well practice paper show particular first order method state art variant excessive gap technique instantiate dilate entropy distance function efficiently solve large real world problems competitively cfr variants show large endgames encounter libratus poker recently beat top human poker specialist professionals limit texas hold show experimental result variant excessive gap technique well prior version introduce numerically friendly implementation smooth best response computation associate first order methods extensive form game solve present knowledge first gpu implementation first order method extensive form game present comparisons several excessive gap technique cfr variants',\n",
              " 'significant interest able predict crimes happen example aid efficient task police protective measure aim model temporal spatial dependencies often exhibit violent crimes order make predictions temporal variation crimes typically follow pattern familiar time series analysis spatial pattern irregular vary smoothly across area instead find spatially disjoint regions exhibit correlate crime pattern indeterminate inter region correlation structure along low count discrete nature count serious crimes motivate propose forecast tool particular propose model crime count region use integer value first order autoregressive process take bayesian nonparametric approach flexibly discover cluster region specific time series describe account covariates within framework approach adjust seasonality demonstrate approach analysis weekly report violent crimes washington forecast outperform standard methods additionally provide useful tool prediction intervals',\n",
              " 'present splinenets practical novel approach use condition convolutional neural network cnns splinenets continuous generalizations neural decision graph dramatically reduce runtime complexity computation cost cnns maintain even increase accuracy function splinenets dynamic condition input hierarchical condition computational path splinenets employ unify loss function desire level smoothness network decision parameters allow sparse activation subset nod individual sample particular embed infinitely many function weight filter smooth low dimensional manifold parameterized compact splines index position parameter instead sample categorical distribution pick branch sample choose continuous position pick function weight show maximize mutual information spline position class label network optimally utilize specialize classification task experiment show approach significantly increase accuracy resnets negligible cost speed match precision level resnet level splinenet',\n",
              " 'study stochastic composite mirror descent class scalable algorithms able exploit geometry composite structure problem consider convex strongly convex objectives non smooth loss function establish high probability convergence rat optimal logarithmic factor apply derive computational error bound study generalization performance multi pass stochastic gradient descent sgd non parametric set high probability generalization bound enjoy logarithmical dependency number pass provide step size sequence square summable improve exist bound expectation polynomial dependency therefore give strong justification ability multi pass sgd overcome overfitting analysis remove boundedness assumptions subgradients often impose literature numerical result report support theoretical find',\n",
              " 'study finite sum nonconvex optimization problems objective function average nonconvex function propose new stochastic gradient descent algorithm base nest variance reduction compare conventional stochastic variance reduce gradient svrg algorithm use two reference point construct semi stochastic gradient diminish variance iteration algorithm use nest reference point build semi stochastic gradient reduce variance iteration smooth nonconvex function propose algorithm converge epsilon approximate first order stationary point nabla mathbf leq epsilon within tilde land epsilon epsilon land epsilon number stochastic gradient evaluations improve best know gradient complexity svrg epsilon scsg land epsilon epsilon land epsilon gradient dominate function algorithm also achieve better gradient complexity state art algorithms thorough experimental result different nonconvex optimization problems back theory',\n",
              " 'study problem identify best action sequential decision make set reward distributions arm exhibit non trivial dependence structure govern underlie causal model domain agent deploy set play arm correspond intervene set variables set specific value paper show whenever underlie causal model take account decision make process standard strategies simultaneously intervene variables subsets variables may general lead suboptimal policies regardless number interventions perform agent environment formally acknowledge phenomenon investigate structural properties imply underlie causal model lead complete characterization relationships arm distributions leverage characterization build new algorithm take input causal structure find minimal sound complete set qualify arm agent play maximize expect reward empirically demonstrate new strategy learn optimal policy lead order magnitude faster convergence rat compare causal insensitive counterparts',\n",
              " 'convolutional neural network cnns inherently subject invariable filter aggregate local input topological structure cause cnns allow manage data euclidean grid like structure image ones non euclidean graph structure traffic network broaden reach cnns develop structure aware convolution eliminate invariance yield unify mechanism deal euclidean non euclidean structure data technically filter structure aware convolution generalize univariate function capable aggregate local input diverse topological structure since infinite parameters require determine univariate function parameterize filter number learnable parameters context function approximation theory replace classical convolution cnns structure aware convolution structure aware convolutional neural network sacnns readily establish extensive experiment eleven datasets strongly evidence sacnns outperform current model various machine learn task include image classification cluster text categorization skeleton base action recognition molecular activity detection taxi flow prediction',\n",
              " 'generalization performance central goal machine learn particularly learn representations large neural network common strategy improve generalization use regularizers typically norm constrain parameters regularize hide layer neural network architecture however straightforward effective layer wise suggestions without theoretical guarantee improve performance work theoretically empirically analyze one model call supervise auto encoder neural network predict input reconstruction error target jointly provide novel generalization result linear auto encoders prove uniform stability base inclusion reconstruction error particularly improvement simplistic regularization norms even advance regularizations use auxiliary task empirically demonstrate across array architectures different number hide units activation function supervise auto encoder compare correspond standard neural network never harm performance significantly improve generalization',\n",
              " 'beyond local convolution network explore harness various external human knowledge endow network capability semantic global reason rather use separate graphical model crf constraints model broader dependencies propose new symbolic graph reason sgr layer perform reason group symbolic nod whose output explicitly represent different properties semantic prior knowledge graph cooperate local convolutions sgr constitute three modules primal local semantic vote module feature symbolic nod generate vote local representations graph reason module propagate information knowledge graph achieve global semantic coherency dual semantic local map module learn new associations evolve symbolic nod local representations accordingly enhance local feature sgr layer inject convolution layer instantiate distinct prior graph extensive experiment show incorporate sgr significantly improve plain convnets three semantic segmentation task one image classification task analyse show sgr layer learn share symbolic representations domains datasets different label set give universal knowledge graph demonstrate superior generalization capability',\n",
              " 'ability transfer reinforcement learn key towards build agent general artificial intelligence paper consider problem learn simultaneously transfer across environments task probably importantly learn sparse environment task pair possible combinations propose novel compositional neural network architecture depict meta rule compose policies environment task embeddings notably one main challenge learn embeddings jointly meta rule propose new train methods disentangle embeddings make distinctive signatures environments task effective build block compose policies experiment gridworld thor agent take input egocentric view show approach give rise high success rat environment task pair learn',\n",
              " 'shoot learn become essential produce model generalize examples work identify metric scale metric task condition important improve performance shoot algorithms analysis reveal simple metric scale completely change nature shoot algorithm parameter update metric scale provide improvements accuracy certain metrics mini imagenet way shoot classification task propose simple effective way condition learner task sample set result learn task dependent metric space moreover propose empirically test practical end end optimization procedure base auxiliary task train learn task dependent metric space result shoot learn model base task dependent scale metric achieve state art mini imagenet confirm result another shoot dataset introduce paper base cifar',\n",
              " 'several applications reinforcement learn suffer instability due high variance especially prevalent high dimensional domains regularization commonly use technique machine learn reduce variance cost introduce bias exist regularization techniques focus spatial perceptual regularization yet reinforcement learn due nature bellman equation opportunity also exploit temporal regularization base smoothness value estimate trajectories paper explore class methods temporal regularization formally characterize bias induce technique use markov chain concepts illustrate various characteristics temporal regularization via sequence simple discrete continuous mdps show technique provide improvement even high dimensional atari game',\n",
              " 'paper address problem manipulate image use natural language description task aim semantically modify visual attribute object image accord text describe new visual appearance although exist methods synthesize image new attribute fully preserve text irrelevant content original image paper propose text adaptive generative adversarial network tagan generate semantically manipulate image preserve text irrelevant content key method text adaptive discriminator create word level local discriminators accord input text classify fine grain attribute independently discriminator generator learn generate image regions correspond give text modify experimental result show method outperform exist methods cub oxford datasets result mostly prefer user study extensive analysis show method able effectively disentangle visual attribute produce please output',\n",
              " 'deep learn model often parameters observations still perform well sometimes describe paradox work show experimentally despite huge number parameters deep neural network compress data losslessly even take cost encode parameters account compression viewpoint originally motivate use variational methods neural network however show variational methods provide surprisingly poor compression bound despite explicitly build minimize bound might explain relatively poor practical performance variational methods deep learn better encode methods import minimum description length mdl toolbox yield much better compression value deep network',\n",
              " 'consider classification problem label unlabeled data available show linear classifiers define convex margin base surrogate losses decrease impossible construct emph semi supervise approach able guarantee improvement supervise classifier measure surrogate loss label unlabeled data convex margin base loss function also increase demonstrate safe improvements emph possible',\n",
              " 'real world learn systems practical limitations quality quantity train datasets collect consider system choose subset possible train examples still allow learn accurate generalizable model help address question draw inspiration highly efficient practical learn system human child use head mount cameras eye gaze trackers model foveated vision collect first person egocentric image represent highly accurate approximation train data toddlers visual systems collect everyday naturalistic learn contexts use state art computer vision learn model convolutional neural network help characterize structure data find child data produce significantly better object model egocentric data experience adults exactly environment use cnns model tool investigate properties child data may enable rapid learn find child data exhibit unique combination quality diversity many similar large high quality object view also greater number diversity rare view novel methodology analyze visual train data use children may reveal insights improve machine learn also may suggest new experimental tool better understand infant learn developmental psychology',\n",
              " 'long short term memory lstm network type recurrent neural network complex computational unit successfully apply variety sequence model task paper develop tree long short term memory treelstm neural network model base lstm design predict tree rather linear sequence treelstm define probability sentence estimate generation probability dependency tree time step node generate base representation generate sub tree enhance model power treelstm explicitly represent correlations leave right dependents application model msr sentence completion challenge achieve result beyond current state art also report result dependency parse reranking achieve competitive performance',\n",
              " 'many image image translation problems ambiguous single input image may correspond multiple possible output work aim model emph distribution possible output conditional generative model set ambiguity map distil low dimensional latent vector randomly sample test time generator learn map give input combine latent code output explicitly encourage connection output latent code invertible help prevent many one map latent code output train also know problem mode collapse produce diverse result explore several variants approach employ different train objectives network architectures methods inject latent code propose method encourage bijective consistency latent encode output modes present systematic comparison method variants perceptual realism diversity',\n",
              " 'wide adoption dnns give birth unrelenting compute requirements force datacenter operators adopt domain specific accelerators train accelerators typically employ densely pack full precision float point arithmetic maximize performance per area ongoing research efforts seek increase performance density replace float point fix point arithmetic however significant roadblock attempt fix point narrow dynamic range insufficient dnn train convergence identify block float point bfp promise alternative representation since exhibit wide dynamic range enable majority dnn operations perform fix point logic unfortunately bfp alone introduce several limitations preclude direct applicability work introduce hbfp hybrid bfp approach perform dot products bfp operations float point hbfp deliver best worlds high accuracy float point superior hardware density fix point wide variety model show hbfp match float point accuracy enable hardware implementations deliver higher throughput',\n",
              " 'leverage temporal dimension key question video analysis recent work suggest efficient approach video feature learn factorize convolutions separate components respectively spatial temporal convolutions temporal convolution however come implicit assumption feature map across time step well align feature locations aggregate assumption may overly strong practical applications especially action recognition motion serve crucial cue work propose new cnn architecture trajectorynet incorporate trajectory convolution new operation integrate feature along temporal dimension replace exist temporal convolution operation explicitly take account change content cause deformation motion allow visual feature aggregate along motion paths trajectories two large scale action recognition datasets namely something something kinetics propose network architecture achieve notable improvement strong baselines',\n",
              " 'program translation important tool migrate legacy code one language ecosystem build different language work first employ deep neural network toward tackle problem observe program translation modular procedure sub tree source tree translate correspond target sub tree step capture intuition design tree tree neural network translate source tree target one meanwhile develop attention mechanism tree tree model decoder expand one non terminal target tree attention mechanism locate correspond sub tree source tree guide expansion decoder evaluate program translation capability tree tree model several state art approach compare neural translation model observe approach consistently better baselines margin point approach improve previous state art program translation approach margin point translation real world project',\n",
              " 'describe iterative procedure optimize policies guarantee monotonic improvement make several approximations theoretically justify procedure develop practical algorithm call trust region policy optimization trpo algorithm similar natural policy gradient methods effective optimize large nonlinear policies neural network experiment demonstrate robust performance wide variety task learn simulate robotic swim hop walk gaits play atari game use image screen input despite approximations deviate theory trpo tend give monotonic improvement little tune hyperparameters',\n",
              " 'uncertainty sample popular active learn algorithm use reduce amount data require learn classifier observe practice converge different parameters depend initialization sometimes even better parameters standard train data work give theoretical explanation phenomenon show uncertainty sample convex logistic loss interpret perform precondition stochastic gradient step population zero one loss experiment synthetic real datasets support connection',\n",
              " 'attention mechanism effective focus deep learn model relevant feature interpret however attentions may unreliable since network generate often train weakly supervise manner overcome limitation introduce notion input dependent uncertainty attention mechanism generate attention feature vary degrees noise base give input learn larger variance instance uncertain learn uncertainty aware attention mechanism use variational inference validate various risk prediction task electronic health record model significantly outperform exist attention model analysis learn attentions show model generate attentions comply clinicians interpretation provide richer interpretation via learn variance evaluation accuracy uncertainty calibration prediction performance know decision show yield network high reliability well',\n",
              " 'real world applications education effective teacher adaptively choose next example teach base learner current state however exist work algorithmic machine teach focus batch set adaptivity play role paper study case teach consistent version space learners interactive set time step teacher provide example learner perform update teacher observe learner new state highlight adaptivity speed teach process consider exist model version space learners worst case model learner pick next hypothesis randomly version space preference base model learner pick hypothesis accord global preference inspire human teach propose new model learner pick hypotheses accord local preference define current hypothesis show model exhibit several desirable properties adaptivity play key role learner transition hypotheses smooth interpretable develop adaptive teach algorithms demonstrate result via simulation user study',\n",
              " 'recent success human action recognition deep learn methods mostly adopt supervise learn paradigm require significant amount manually label data achieve good performance however label collection expensive time consume process work propose unsupervised learn framework exploit unlabeled data learn video representations different previous work video representation learn unsupervised learn task predict motion multiple target view use video representation source view learn extrapolate cross view motion representation capture view invariant motion dynamics discriminative action addition propose view adversarial train method enhance learn view invariant feature demonstrate effectiveness learn representations action recognition multiple datasets',\n",
              " 'recent years supervise learn convolutional network cnns see huge adoption computer vision applications comparatively unsupervised learn cnns receive less attention work hope help bridge gap success cnns supervise learn unsupervised learn introduce class cnns call deep convolutional generative adversarial network dcgans certain architectural constraints demonstrate strong candidate unsupervised learn train various image datasets show convince evidence deep convolutional adversarial pair learn hierarchy representations object part scenes generator discriminator additionally use learn feature novel task demonstrate applicability general image representations',\n",
              " 'recurrent neural network powerful tool understand model computation representation populations neurons continuous variable rate model network analyze apply extensively purpose however neurons fire action potentials discrete nature spike important feature neural circuit dynamics despite significant advance train recurrently connect spike neural network remain challenge present procedure train recurrently connect spike network generate dynamical pattern autonomously produce complex temporal output base integrate network input model physiological data procedure make use continuous variable network identify target train input spike model neurons surprisingly able construct spike network duplicate task perform continuous variable network relatively minor expansion number neurons approach provide novel view significance appropriate use fire rate model useful approach build model spike network use address important question representation computation neural systems',\n",
              " 'decision tree random forest well establish model offer good predictive performance also provide rich feature importance information practitioners often employ variable importance methods rely impurity base information methods remain poorly characterize theoretical perspective provide novel insights performance methods derive finite sample performance guarantee high dimensional set various model assumptions demonstrate effectiveness impurity base methods via extensive set simulations',\n",
              " 'one core problems modern statistics approximate difficult compute probability densities problem especially important bayesian statistics frame inference unknown quantities calculation involve posterior density paper review variational inference method machine learn approximate probability densities optimization use many applications tend faster classical methods markov chain monte carlo sample idea behind first posit family densities find member family close target closeness measure kullback leibler divergence review ideas behind mean field variational inference discuss special case apply exponential family model present full example bayesian mixture gaussians derive variant use stochastic optimization scale massive data discuss modern research highlight important open problems powerful yet well understand hope write paper catalyze statistical research class algorithms',\n",
              " 'introduce variability maintain coherence core task learn generate utterances conversation standard neural encoder decoder model extensions use conditional variational autoencoder often result either trivial digressive responses overcome explore novel approach inject variability neural encoder decoder via use external memory mixture model namely variational memory encoder decoder vmed associate memory read mode latent mixture distribution timestep model capture variability observe sequential data natural conversations empirically compare propose model recent approach various conversational datasets result show vmed consistently achieve significant improvement others metric base qualitative evaluations',\n",
              " 'model neural machine translation often discriminative family encoderdecoders learn conditional distribution target sentence give source sentence paper propose variational model learn conditional distribution neural machine translation variational encoderdecoder model train end end different vanilla encoder decoder model generate target translations hide representations source sentence alone variational model introduce continuous latent variable explicitly model underlie semantics source sentence guide generation target translations order perform efficient posterior inference large scale train build neural posterior approximator condition source target side equip reparameterization technique estimate variational lower bind experiment chinese english english german translation task show propose variational neural machine translation achieve significant improvements vanilla neural machine translation baselines',\n",
              " 'deep reinforcement learn successfully solve many challenge control task real world applicability limit inability ensure safety learn policies propose approach verifiable reinforcement learn train decision tree policies represent complex policies since nonparametric yet efficiently verify use exist techniques since highly structure challenge decision tree policies difficult train propose viper algorithm combine ideas model compression imitation learn learn decision tree policies guide dnn policy call oracle function show substantially outperform two baselines use viper learn provably robust decision tree policy variant atari pong symbolic state space learn decision tree policy toy game base pong provably never lose iii learn provably stable decision tree policy cart pole case decision tree policy achieve performance equal original dnn policy',\n",
              " 'adversarial learn base video prediction methods suffer image blur since commonly use adversarial regression loss pair work rather competitive way collaboration yield compromise blur effect meantime often rely single pass architecture predictor inadequate explicitly capture forthcoming uncertainty work involve two key insights video prediction approach stochastic process sample collection proposals conform possible frame distribution follow time stamp one select final prediction couple combine loss function dedicatedly design sub network encourage work collaborative way combine two insights propose two stage network call vpss textbf ideo textbf rediction via textbf elective textbf ampling specifically emph sample module produce collection high quality proposals facilitate multiple choice adversarial learn scheme yield diverse frame proposal set subsequently emph selection module select high possibility candidates proposals combine produce final prediction extensive experiment diverse challenge datasets demonstrate effectiveness propose video prediction approach yield diverse proposals accurate prediction result',\n",
              " 'study problem video video synthesis whose goal learn map function input source video sequence semantic segmentation mask output photorealistic video precisely depict content source video image counterpart image image translation problem popular topic video video synthesis problem less explore literature without model temporal dynamics directly apply exist image synthesis approach input video often result temporally incoherent videos low visual quality paper propose video video synthesis approach generative adversarial learn framework carefully design generators discriminators couple spatio temporal adversarial objective achieve high resolution photorealistic temporally coherent video result diverse set input format include segmentation mask sketch pose experiment multiple benchmarks show advantage method compare strong baselines particular model capable synthesize resolution videos street scenes second long significantly advance state art video synthesis finally apply method future video prediction outperform several compete systems code model result available website https github com nvidia vid vid please use adobe reader see embed videos paper',\n",
              " 'recently learn discriminative feature improve recognition performances gradually become primary goal deep learn numerous remarkable work emerge paper propose novel yet extremely simple method virtual softmax enhance discriminative property learn feature inject dynamic virtual negative class original softmax inject virtual class aim enlarge inter class margin compress intra class distribution strengthen decision boundary constraint although seem weird optimize additional virtual class show method derive intuitive clear motivation indeed encourage feature compact separable paper empirically experimentally demonstrate superiority virtual softmax improve performances variety object classification face verification task',\n",
              " 'humans routinely retrace path novel environment forward backwards despite uncertainty motion paper present approach give demonstration path first network generate path equip second network observe world decide act order retrace path noisy actuation change environment two network optimize end end train time evaluate method two realistic simulators perform path follow forward backwards experiment show approach outperform classical approach solve task well number baselines',\n",
              " 'recent progress deep generative model lead tremendous breakthroughs image generation able synthesize photorealistic image exist model lack understand underlie world different previous work build datasets model present new generative model visual object network vons synthesize natural image object disentangle representation inspire classic graphics render pipelines unravel image formation process three conditionally independent factor shape viewpoint texture present end end adversarial learn framework jointly model shape texture model first learn synthesize shape indistinguishable real shape render object sketch silhouette depth map shape sample viewpoint finally learn add realistic textures sketch generate realistic image von generate image realistic state art image synthesis methods also enable many operations change viewpoint generate image shape texture edit linear interpolation texture shape space transfer appearance across different object viewpoints',\n",
              " 'propose wasserstein auto encoder wae new algorithm build generative model data distribution wae minimize penalize form wasserstein distance model distribution target distribution lead different regularizer one use variational auto encoder vae regularizer encourage encode train distribution match prior compare algorithm several techniques show generalization adversarial auto encoders aae experiment show wae share many properties vaes stable train encoder decoder architecture nice latent manifold structure generate sample better quality measure fid score',\n",
              " 'paper introduce wasserstein variational inference new form approximate bayesian inference base optimal transport theory wasserstein variational inference use new family divergences include divergences wasserstein distance special case gradients wasserstein variational loss obtain backpropagating sinkhorn iterations technique result stable likelihood free train method use implicit distributions probabilistic program use wasserstein variational inference framework introduce several new form autoencoders test robustness performance exist variational autoencoding techniques',\n",
              " 'introduce new efficient principled backpropagation compatible algorithm learn probability distribution weight neural network call bay backprop regularise weight minimise compression cost know variational free energy expect lower bind marginal likelihood show principled kind regularisation yield comparable performance dropout mnist classification demonstrate learn uncertainty weight use improve generalisation non linear regression problems weight uncertainty use drive exploration exploitation trade reinforcement learn',\n",
              " 'infer intent observe behavior study extensively within frameworks bayesian inverse plan inverse reinforcement learn methods infer goal reward function best explain action observe agent typically human demonstrator another agent use infer intent predict imitate assist human user however central assumption inverse reinforcement learn demonstrator close optimal model suboptimal behavior exist typically assume suboptimal action result type random noise know cognitive bias like temporal inconsistency paper take alternative approach model suboptimal behavior result internal model misspecification reason user action might deviate near optimal action user incorrect set beliefs rule dynamics govern action affect environment insight demonstrate action may suboptimal real world may actually near optimal respect user internal model dynamics estimate internal beliefs observe behavior arrive new method infer intent demonstrate simulation user study participants approach enable accurately model human intent use variety applications include offer assistance share autonomy framework infer human preferences',\n",
              " 'give rigorous analysis statistical behavior gradients randomly initialize fully connect network relu activations result show empirical variance square entries input output jacobian exponential simple architecture dependent constant beta give sum reciprocals hide layer widths beta large gradients compute initialization vary wildly approach complement mean field theory analysis random network point view rigorously compute finite width corrections statistics gradients edge chaos',\n",
              " 'humans make repeat choices among options imperfectly know reward outcomes important problem psychology neuroscience often study use multi arm bandits also frequently study machine learn present data human stationary bandit experiment vary average abundance variability reward availability mean variance reward rate distributions surprisingly find subject significantly underestimate prior mean reward rat base self report end game reward expectation non choose arm previously human learn bandit task find well capture bayesian ideal learn model dynamic belief model dbm albeit incorrect generative assumption temporal structure humans assume reward rat change time even though actually fix find pessimism bias bandit task well capture prior mean dbm fit human choices poorly capture prior mean fix belief model fbm alternative bayesian model correctly assume reward rat constants pessimism bias also incompletely capture simple reinforcement learn model commonly use neuroscience psychology term fit initial value seem sub optimal thus mysterious humans underestimate prior reward expectation simulations show underestimate prior mean help maximize long term gain observer assume volatility reward rat stable utilize softmax decision policy instead optimal one obtainable dynamic program raise intrigue possibility brain underestimate reward rat compensate incorrect non stationarity assumption generative model simplify decision policy',\n",
              " 'object orient representations reinforcement learn show promise transfer learn previous research introduce propositional object orient framework provably efficient learn bound respect sample complexity however framework limitations term class task efficiently learn paper introduce novel deictic object orient framework provably efficient learn bound solve broader range task additionally show framework capable zero shoot transfer transition dynamics across task demonstrate empirically taxi sokoban domains']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1k-wgW1c-o0",
        "outputId": "4b80ec62-051f-4f2b-c8fd-715b1ee8a349"
      },
      "source": [
        "len(data_list2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "292"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6XaOYkIHbPl"
      },
      "source": [
        "#Converting every data point into a fixed length so at the time of calculating the embeddings we won't have any issue\n",
        "\n",
        "data_list_fin2=[]\n",
        "for i in data_list2:\n",
        "  data_list_fin2.append(i[:2000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_1pSp5PxNBc",
        "outputId": "9909dd80-f5f6-42a3-e84e-985894577bc0"
      },
      "source": [
        "!pip install sentence_transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence_transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/9d/abacb6f7bb63df39285c55bb51b6403a7fd93ac2aea48b01f6215175446c/sentence-transformers-1.1.1.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.6MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 25.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (4.0.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 38.6MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 32.6MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (20.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence_transformers) (8.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.1.1-cp37-none-any.whl size=123338 sha256=386b40a9d116077cec706dbdbb66cf50e3fa5dfc87d0b821c69b446ea3f81990\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/89/29/45e45adc162b50f97f71801e8b07947c9cfe2b3ae7dbf37896\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 sentence-transformers-1.1.1 sentencepiece-0.1.95 tokenizers-0.10.2 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1w3KQgZCvea"
      },
      "source": [
        "# from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "\n",
        "\n",
        "\n",
        "# #Compute embeddings\n",
        "# embeddings = model.encode(data_list_fin, convert_to_tensor=False)\n",
        "\n",
        "\n",
        "# from sklearn.cluster import KMeans\n",
        "# num_clusters = 7\n",
        "# # Define kmeans model\n",
        "# clustering_model = KMeans(n_clusters=num_clusters)\n",
        "# # Fit the embedding with kmeans clustering.\n",
        "# clustering_model.fit(embeddings)\n",
        "# # Get the cluster id assigned to each news headline.\n",
        "# clusterr = clustering_model.get_params\n",
        "# cluster_assignment = clustering_model.labels_\n",
        "\n",
        "# for i in range(len(data_list_fin)):\n",
        "#   print(data_list_fin[i])\n",
        "#   print('\\t')\n",
        "#   print(cluster_assignment[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahU6itw6C_zZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "d3c25a21d21b42adb251c3499d2445e7",
            "cfe6e8ccad2a460c8e46bd90bfedef28",
            "fa15e15fb5174723ab5616fd589925db",
            "e963cc87aacc4e958697780504b741b9",
            "ac2f2d7a5949452aac3a4d2a9f34e04a",
            "b3bed97c2fca419ab002a30d5ad5d14c",
            "7251f864e70242bda6d241e20d1bd07e",
            "3a978baf429845739af411ce7111f160"
          ]
        },
        "outputId": "6293284f-58c2-4fd3-e1d8-479fcd59b4c9"
      },
      "source": [
        "# I have trained a model (SBERT) on the same kind of dataset. We can import it and name it as \"model\" and comment the next two lines but not the last one. \n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "\n",
        "#Compute embeddings\n",
        "embeddings_test = model.encode(data_list_fin2, convert_to_tensor=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3c25a21d21b42adb251c3499d2445e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=305584576.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz24tPBSdiDE",
        "outputId": "1df4d8d8-1b69-44c4-87f1-7b1dad14837e"
      },
      "source": [
        "len(embeddings_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "292"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "d747b244fd0e43b08bf9bef8d72c1b5d"
          ]
        },
        "id": "GCNeLgexjD0z",
        "outputId": "9e1335bf-f273-493a-b0f9-f290b90c2594"
      },
      "source": [
        "# from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# model = SentenceTransformer('paraphrase-distilroberta-base-v1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d747b244fd0e43b08bf9bef8d72c1b5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=305584576.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HpvNmOKUZM_"
      },
      "source": [
        "# First save the embeddings_test and then import it again, so the code will not break afterwards\n",
        "\n",
        "np.savetxt(\"embeddings_test.csv\", embeddings_test, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I95Xp9wZVcKd"
      },
      "source": [
        "# now importing embeddings (created before hand 3270) and embeddings_test \n",
        "\n",
        "embeddings = pd.read_csv(\"embeddings_final.csv\",header=None)\n",
        "#embeddings_test = pd.read_csv(\"embeddings_test.csv\",header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xdbOR5CXEL4"
      },
      "source": [
        "CALCULATIONS FOR NEW CLUSTERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pON-l9mZ6Btu"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PFeUZonDulc"
      },
      "source": [
        "#At first I just hard coded the sublist length, afterwards I will make it dynamic\n",
        "\n",
        "length = 7\n",
        "length2 = len(embeddings_test)\n",
        "cosine_list = []\n",
        "for i in range(length2):\n",
        "  cosine_list.append([])\n",
        "  for j in range(length):\n",
        "    cosine_list[i].append([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHFV3OUlDGhI"
      },
      "source": [
        "# listlengths = [len(filess_sum),len(filess_sum)+len(filess_doc),len(filess_sum)+len(filess_doc)+len(filess_sen),len(filess_sum)+len(filess_doc)+len(filess_sen)+len(filess_topi),len(filess_sum)+len(filess_doc)+len(filess_sen)+len(filess_topi)+len(filess_text),len(filess_sum)+len(filess_doc)+len(filess_sen)+len(filess_topi)+len(filess_text)+len(filess_relation),len(filess_sum)+len(filess_doc)+len(filess_sen)+len(filess_topi)+len(filess_text)+len(filess_relation)+len(filess_machine)]\n",
        "listlengths = [409, 890, 1361, 1650, 2379, 2678, 3295]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HihdTuFq3veI",
        "outputId": "9192d925-c417-462b-9d66-772227936028"
      },
      "source": [
        "len(data_list_fin2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "292"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfmp8sCDDcZ2",
        "outputId": "8b5ed3ed-1d36-4524-b057-bac5a9ad7cb0"
      },
      "source": [
        "len(embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3270"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5csmDvx7GzF",
        "outputId": "d8807e60-ee96-4b5e-90a0-68480926b1ce"
      },
      "source": [
        "'''\n",
        "We will compare the data points with the train data embeddings to see if any very different data points are present so just save it somewehre else so we will not feed\n",
        "it to the KMwans model and so we will code more to make separate clusters\n",
        "I put a threshold of 0.6 to check it, and it is a good threshold to keep.\n",
        "\n",
        "Afterwards we will see the count of every sublist, if any count is less than 10 we will consider it to be in a new cluster\n",
        "'''\n",
        "\n",
        "for i in range(len(cosine_list)):\n",
        "  for j in range(len(listlengths)):\n",
        "    \n",
        "    similarity = []\n",
        "    count = 0\n",
        "    if listlengths[j] == listlengths[0]:\n",
        "      n = listlengths[j]\n",
        "      similarity = cosine_similarity(embeddings_test[i].reshape(1,-1),embeddings.loc[0:n].values)\n",
        "      for k in similarity:\n",
        "        for l in k:\n",
        "          if l > 0.6:\n",
        "            count+=1\n",
        "      cosine_list[i][j].append(count)\n",
        "    \n",
        "    else:\n",
        "      similarity = cosine_similarity(embeddings_test[i].reshape(1,-1),embeddings.loc[n:listlengths[j]].values)\n",
        "      n = listlengths[j]\n",
        "      for k in similarity:\n",
        "        for l in k:\n",
        "          if l > 0.6:\n",
        "            count+=1\n",
        "      cosine_list[i][j].append(count)\n",
        "\n",
        "\n",
        "  \n",
        "        \n",
        "      \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#cosine_list[i][0].append(1)\n",
        "# creating a list\n",
        "\n",
        "total_list = []\n",
        "# Iterate each element in list\n",
        "# and add them in variale total\n",
        "# for i in cosine_list:\n",
        "#   total_list.append(sum(i))\n",
        "\n",
        "cosine_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[6], [4], [3], [3], [6], [1], [2]],\n",
              " [[17], [15], [3], [17], [6], [3], [16]],\n",
              " [[3], [3], [6], [1], [4], [1], [14]],\n",
              " [[37], [43], [8], [17], [36], [31], [46]],\n",
              " [[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[32], [32], [17], [30], [52], [28], [140]],\n",
              " [[58], [149], [47], [80], [100], [59], [136]],\n",
              " [[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[69], [29], [4], [55], [28], [18], [24]],\n",
              " [[7], [10], [1], [2], [1], [1], [0]],\n",
              " [[4], [1], [3], [2], [2], [8], [10]],\n",
              " [[27], [72], [20], [34], [21], [36], [22]],\n",
              " [[0], [0], [0], [0], [0], [1], [0]],\n",
              " [[57], [83], [25], [107], [84], [63], [147]],\n",
              " [[19], [58], [21], [28], [62], [72], [136]],\n",
              " [[36], [24], [19], [9], [51], [30], [54]],\n",
              " [[6], [0], [1], [7], [0], [0], [6]],\n",
              " [[1], [3], [2], [3], [2], [0], [1]],\n",
              " [[40], [111], [20], [58], [53], [70], [72]],\n",
              " [[7], [33], [9], [19], [16], [23], [126]],\n",
              " [[1], [0], [0], [0], [0], [0], [1]],\n",
              " [[17], [36], [20], [22], [61], [87], [23]],\n",
              " [[28], [42], [18], [41], [67], [32], [30]],\n",
              " [[0], [0], [0], [2], [0], [0], [0]],\n",
              " [[0], [1], [0], [0], [0], [0], [0]],\n",
              " [[12], [26], [4], [20], [9], [8], [24]],\n",
              " [[1], [1], [1], [2], [1], [1], [1]],\n",
              " [[1], [0], [0], [0], [0], [0], [0]],\n",
              " [[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[7], [6], [13], [2], [12], [6], [15]],\n",
              " [[28], [69], [28], [28], [45], [38], [169]],\n",
              " [[6], [14], [19], [3], [61], [5], [32]],\n",
              " [[94], [129], [61], [71], [281], [109], [418]],\n",
              " [[30], [50], [10], [69], [52], [77], [86]],\n",
              " [[18], [31], [9], [9], [43], [17], [82]],\n",
              " [[36], [84], [17], [40], [127], [59], [192]],\n",
              " [[35], [88], [28], [31], [69], [59], [186]],\n",
              " [[4], [3], [0], [14], [2], [0], [11]],\n",
              " [[3], [5], [4], [1], [1], [17], [13]],\n",
              " [[9], [10], [1], [12], [5], [2], [1]],\n",
              " [[115], [129], [57], [112], [109], [83], [208]],\n",
              " [[2], [2], [1], [1], [5], [1], [9]],\n",
              " [[25], [46], [17], [26], [23], [48], [30]],\n",
              " [[19], [28], [9], [32], [25], [10], [30]],\n",
              " [[34], [28], [9], [44], [89], [48], [137]],\n",
              " [[11], [8], [4], [1], [8], [11], [5]],\n",
              " [[50], [179], [58], [80], [169], [68], [124]],\n",
              " [[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[12], [14], [5], [15], [6], [6], [29]],\n",
              " [[0], [0], [0], [0], [0], [1], [1]],\n",
              " [[1], [5], [1], [0], [4], [0], [5]],\n",
              " [[2], [5], [0], [18], [3], [1], [8]],\n",
              " [[15], [34], [7], [17], [64], [18], [130]],\n",
              " [[58], [104], [20], [30], [242], [60], [105]],\n",
              " [[12], [39], [5], [18], [12], [13], [23]],\n",
              " [[50], [137], [48], [27], [102], [119], [112]],\n",
              " [[3], [0], [1], [1], [0], [0], [1]],\n",
              " [[4], [1], [4], [3], [7], [1], [17]],\n",
              " [[29], [53], [11], [30], [35], [35], [12]],\n",
              " [[8], [33], [11], [14], [57], [15], [22]],\n",
              " [[12], [56], [10], [22], [76], [15], [61]],\n",
              " [[16], [59], [4], [13], [10], [10], [1]],\n",
              " [[26], [61], [33], [17], [209], [50], [225]],\n",
              " [[42], [147], [92], [43], [136], [74], [159]],\n",
              " [[13], [12], [1], [0], [3], [11], [3]],\n",
              " [[0], [0], [0], [0], [0], [2], [0]],\n",
              " [[4], [3], [2], [2], [8], [2], [7]],\n",
              " [[13], [36], [10], [30], [30], [12], [45]],\n",
              " [[32], [82], [40], [22], [102], [34], [133]],\n",
              " [[9], [35], [22], [14], [17], [42], [40]],\n",
              " [[51], [133], [80], [61], [220], [67], [233]],\n",
              " [[7], [10], [1], [2], [5], [3], [2]],\n",
              " [[12], [12], [4], [9], [93], [17], [55]],\n",
              " [[13], [40], [12], [24], [19], [10], [19]],\n",
              " [[54], [59], [40], [60], [125], [81], [169]],\n",
              " [[90], [158], [47], [95], [210], [56], [146]],\n",
              " [[3], [11], [1], [13], [2], [5], [2]],\n",
              " [[3], [15], [1], [1], [1], [0], [5]],\n",
              " [[3], [3], [0], [12], [3], [0], [2]],\n",
              " [[1], [2], [0], [2], [3], [0], [18]],\n",
              " [[19], [45], [6], [49], [64], [24], [79]],\n",
              " [[2], [1], [0], [4], [11], [1], [8]],\n",
              " [[19], [15], [5], [7], [9], [10], [15]],\n",
              " [[11], [28], [11], [9], [11], [6], [17]],\n",
              " [[19], [31], [4], [16], [11], [37], [8]],\n",
              " [[0], [3], [3], [1], [5], [0], [9]],\n",
              " [[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[21], [57], [25], [19], [38], [43], [64]],\n",
              " [[4], [36], [6], [1], [11], [1], [2]],\n",
              " [[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[83], [109], [78], [50], [99], [45], [112]],\n",
              " [[1], [5], [1], [4], [9], [1], [8]],\n",
              " [[18], [74], [32], [11], [25], [8], [38]],\n",
              " [[108], [209], [77], [89], [208], [114], [272]],\n",
              " [[2], [3], [0], [0], [0], [0], [0]],\n",
              " [[46], [177], [60], [51], [213], [81], [249]],\n",
              " [[29], [78], [39], [31], [113], [46], [102]],\n",
              " [[21], [40], [13], [31], [368], [25], [210]],\n",
              " [[66], [147], [42], [101], [176], [110], [133]],\n",
              " [[25], [83], [19], [42], [29], [16], [39]],\n",
              " [[0], [5], [0], [1], [1], [1], [5]],\n",
              " [[1], [3], [1], [3], [0], [21], [1]],\n",
              " [[4], [9], [1], [20], [5], [13], [14]],\n",
              " [[39], [22], [25], [31], [55], [22], [49]],\n",
              " [[40], [88], [30], [75], [72], [73], [116]],\n",
              " [[7], [53], [18], [27], [106], [36], [101]],\n",
              " [[60], [125], [28], [59], [254], [75], [495]],\n",
              " [[26], [20], [6], [22], [12], [13], [73]],\n",
              " [[7], [11], [1], [34], [12], [10], [66]],\n",
              " [[42], [57], [30], [70], [67], [54], [64]],\n",
              " [[26], [26], [10], [18], [43], [16], [47]],\n",
              " [[22], [26], [16], [4], [68], [14], [20]],\n",
              " [[2], [3], [2], [7], [0], [4], [2]],\n",
              " [[14], [56], [8], [30], [11], [20], [9]],\n",
              " [[11], [9], [1], [2], [4], [22], [19]],\n",
              " [[36], [75], [8], [43], [124], [60], [216]],\n",
              " [[13], [24], [8], [29], [15], [9], [57]],\n",
              " [[11], [27], [13], [5], [80], [61], [25]],\n",
              " [[10], [22], [5], [8], [19], [4], [60]],\n",
              " [[0], [4], [1], [1], [0], [1], [1]],\n",
              " [[1], [2], [2], [3], [8], [5], [17]],\n",
              " [[3], [18], [3], [8], [19], [5], [35]],\n",
              " [[14], [15], [4], [15], [12], [17], [43]],\n",
              " [[41], [125], [22], [80], [47], [55], [104]],\n",
              " [[69], [87], [53], [41], [73], [94], [82]],\n",
              " [[1], [11], [4], [3], [19], [3], [6]],\n",
              " [[60], [72], [65], [30], [144], [111], [290]],\n",
              " [[57], [83], [45], [31], [105], [106], [222]],\n",
              " [[5], [12], [2], [23], [7], [14], [22]],\n",
              " [[2], [20], [16], [8], [32], [14], [15]],\n",
              " [[7], [49], [5], [10], [53], [11], [21]],\n",
              " [[36], [44], [27], [54], [46], [30], [76]],\n",
              " [[5], [2], [0], [2], [0], [1], [1]],\n",
              " [[0], [1], [0], [0], [1], [2], [0]],\n",
              " [[49], [67], [62], [27], [67], [42], [119]],\n",
              " [[66], [123], [74], [70], [275], [64], [210]],\n",
              " [[55], [109], [56], [61], [217], [95], [179]],\n",
              " [[13], [40], [9], [15], [17], [23], [47]],\n",
              " [[40], [79], [21], [51], [51], [74], [86]],\n",
              " [[0], [1], [1], [1], [6], [1], [1]],\n",
              " [[27], [57], [8], [37], [100], [28], [125]],\n",
              " [[0], [14], [2], [1], [2], [3], [6]],\n",
              " [[5], [10], [3], [2], [11], [10], [10]],\n",
              " [[3], [16], [1], [9], [4], [12], [3]],\n",
              " [[32], [51], [15], [12], [30], [18], [89]],\n",
              " [[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[8], [41], [5], [15], [12], [24], [42]],\n",
              " [[13], [50], [9], [28], [21], [24], [32]],\n",
              " [[1], [1], [2], [0], [3], [2], [0]],\n",
              " [[0], [1], [0], [0], [0], [0], [1]],\n",
              " [[38], [55], [18], [25], [37], [27], [41]],\n",
              " [[28], [66], [27], [27], [41], [17], [65]],\n",
              " [[4], [16], [3], [6], [38], [10], [22]],\n",
              " [[0], [2], [4], [0], [4], [0], [0]],\n",
              " [[0], [1], [0], [0], [0], [0], [0]],\n",
              " [[1], [1], [0], [0], [0], [0], [0]],\n",
              " [[45], [77], [20], [40], [39], [82], [27]],\n",
              " [[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[0], [0], [1], [0], [0], [0], [0]],\n",
              " [[31], [60], [30], [37], [90], [44], [93]],\n",
              " [[10], [19], [3], [9], [6], [7], [28]],\n",
              " [[48], [143], [54], [31], [96], [36], [76]],\n",
              " [[0], [0], [0], [1], [0], [0], [0]],\n",
              " [[0], [2], [0], [2], [0], [1], [0]],\n",
              " [[6], [2], [4], [0], [0], [2], [12]],\n",
              " [[37], [52], [13], [66], [34], [28], [130]],\n",
              " [[95], [126], [35], [104], [161], [53], [194]],\n",
              " [[44], [53], [10], [60], [18], [9], [15]],\n",
              " [[8], [8], [0], [19], [9], [12], [33]],\n",
              " [[22], [22], [20], [40], [23], [20], [16]],\n",
              " [[72], [144], [33], [77], [83], [82], [183]],\n",
              " [[0], [1], [0], [0], [1], [0], [1]],\n",
              " [[0], [1], [0], [1], [1], [2], [1]],\n",
              " [[31], [68], [34], [28], [112], [82], [89]],\n",
              " [[22], [119], [29], [23], [22], [14], [56]],\n",
              " [[5], [5], [2], [15], [4], [8], [6]],\n",
              " [[13], [22], [15], [4], [6], [12], [8]],\n",
              " [[7], [64], [21], [9], [11], [18], [33]],\n",
              " [[8], [13], [11], [18], [19], [5], [48]],\n",
              " [[20], [24], [12], [19], [25], [8], [86]],\n",
              " [[8], [13], [7], [4], [21], [9], [45]],\n",
              " [[3], [10], [1], [1], [5], [8], [11]],\n",
              " [[5], [19], [5], [10], [9], [4], [5]],\n",
              " [[32], [42], [20], [27], [29], [23], [53]],\n",
              " [[13], [6], [6], [5], [3], [3], [9]],\n",
              " [[3], [30], [5], [26], [8], [5], [48]],\n",
              " [[3], [8], [2], [3], [15], [10], [13]],\n",
              " [[6], [1], [0], [1], [0], [0], [3]],\n",
              " [[3], [5], [1], [19], [3], [2], [19]],\n",
              " [[7], [18], [2], [12], [3], [13], [10]],\n",
              " [[15], [19], [14], [32], [9], [18], [29]],\n",
              " [[0], [13], [11], [2], [14], [2], [4]],\n",
              " [[1], [0], [0], [0], [0], [0], [1]],\n",
              " [[18], [13], [3], [29], [7], [32], [23]],\n",
              " [[0], [0], [0], [0], [1], [1], [0]],\n",
              " [[145], [173], [67], [100], [267], [92], [383]],\n",
              " [[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[0], [0], [0], [6], [0], [0], [0]],\n",
              " [[23], [35], [28], [48], [9], [26], [35]],\n",
              " [[4], [6], [3], [2], [2], [1], [2]],\n",
              " [[1], [8], [0], [1], [13], [2], [10]],\n",
              " [[48], [76], [31], [89], [72], [43], [146]],\n",
              " [[48], [83], [25], [65], [78], [52], [106]],\n",
              " [[3], [8], [3], [1], [4], [0], [35]],\n",
              " [[44], [111], [17], [66], [18], [47], [21]],\n",
              " [[4], [2], [3], [1], [7], [3], [15]],\n",
              " [[4], [5], [0], [17], [4], [6], [6]],\n",
              " [[4], [8], [0], [15], [6], [7], [3]],\n",
              " [[0], [3], [0], [0], [2], [2], [7]],\n",
              " [[5], [24], [3], [4], [5], [5], [6]],\n",
              " [[1], [1], [1], [0], [1], [1], [1]],\n",
              " [[0], [0], [0], [2], [0], [0], [0]],\n",
              " [[22], [9], [2], [10], [8], [6], [2]],\n",
              " [[27], [57], [29], [33], [59], [90], [127]],\n",
              " [[71], [115], [71], [35], [172], [76], [157]],\n",
              " [[3], [3], [3], [4], [10], [1], [9]],\n",
              " [[0], [2], [2], [0], [0], [4], [2]],\n",
              " [[3], [21], [6], [5], [10], [7], [21]],\n",
              " [[7], [2], [2], [1], [20], [6], [20]],\n",
              " [[20], [58], [21], [41], [28], [44], [80]],\n",
              " [[2], [12], [3], [8], [0], [1], [2]],\n",
              " [[16], [24], [16], [5], [87], [14], [20]],\n",
              " [[1], [0], [4], [0], [0], [0], [0]],\n",
              " [[13], [19], [15], [15], [30], [23], [48]],\n",
              " [[2], [0], [0], [1], [1], [3], [0]],\n",
              " [[96], [185], [62], [66], [138], [57], [224]],\n",
              " [[0], [3], [3], [2], [1], [6], [1]],\n",
              " [[61], [103], [30], [57], [87], [40], [185]],\n",
              " [[0], [0], [2], [0], [0], [0], [0]],\n",
              " [[49], [107], [43], [41], [136], [37], [161]],\n",
              " [[17], [17], [2], [10], [1], [2], [3]],\n",
              " [[0], [2], [1], [0], [1], [0], [1]],\n",
              " [[4], [7], [1], [0], [3], [0], [0]],\n",
              " [[4], [5], [2], [1], [0], [3], [2]],\n",
              " [[10], [9], [6], [11], [6], [11], [20]],\n",
              " [[6], [0], [1], [0], [4], [0], [0]],\n",
              " [[33], [85], [23], [36], [39], [62], [17]],\n",
              " [[17], [78], [14], [20], [37], [28], [25]],\n",
              " [[1], [1], [0], [0], [1], [0], [0]],\n",
              " [[2], [3], [0], [3], [0], [0], [0]],\n",
              " [[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[24], [41], [17], [18], [13], [8], [17]],\n",
              " [[11], [24], [24], [10], [44], [17], [38]],\n",
              " [[7], [15], [6], [5], [3], [5], [1]],\n",
              " [[4], [4], [3], [3], [9], [1], [2]],\n",
              " [[4], [8], [0], [5], [4], [1], [2]],\n",
              " [[6], [6], [6], [6], [2], [1], [3]],\n",
              " [[18], [52], [30], [14], [15], [7], [14]],\n",
              " [[62], [89], [34], [47], [37], [31], [82]],\n",
              " [[19], [29], [28], [8], [13], [19], [40]],\n",
              " [[10], [16], [11], [21], [1], [15], [10]],\n",
              " [[1], [5], [2], [0], [1], [2], [0]],\n",
              " [[59], [63], [27], [39], [104], [46], [181]],\n",
              " [[6], [14], [10], [5], [13], [9], [7]],\n",
              " [[11], [56], [7], [15], [19], [24], [58]],\n",
              " [[22], [27], [14], [11], [10], [10], [29]],\n",
              " [[5], [5], [1], [13], [3], [0], [13]],\n",
              " [[28], [108], [25], [24], [64], [29], [33]],\n",
              " [[10], [9], [7], [10], [4], [4], [20]],\n",
              " [[5], [13], [2], [2], [18], [4], [14]],\n",
              " [[32], [90], [25], [72], [70], [42], [81]],\n",
              " [[13], [60], [10], [11], [35], [33], [98]],\n",
              " [[25], [84], [19], [56], [156], [76], [190]],\n",
              " [[0], [1], [2], [3], [6], [0], [9]],\n",
              " [[38], [47], [19], [26], [51], [25], [115]],\n",
              " [[11], [38], [5], [13], [33], [37], [243]],\n",
              " [[1], [0], [0], [0], [3], [0], [4]],\n",
              " [[7], [15], [3], [7], [9], [15], [7]],\n",
              " [[64], [72], [22], [104], [87], [78], [188]],\n",
              " [[0], [0], [0], [0], [1], [0], [2]],\n",
              " [[27], [37], [10], [7], [21], [15], [33]],\n",
              " [[0], [3], [2], [1], [3], [1], [2]],\n",
              " [[2], [6], [2], [3], [0], [3], [0]],\n",
              " [[4], [1], [1], [5], [0], [4], [0]],\n",
              " [[44], [49], [19], [42], [15], [9], [72]],\n",
              " [[72], [64], [38], [46], [94], [39], [123]],\n",
              " [[58], [89], [29], [51], [221], [77], [441]],\n",
              " [[1], [2], [0], [0], [0], [1], [0]],\n",
              " [[20], [45], [15], [22], [118], [11], [68]],\n",
              " [[54], [49], [13], [33], [114], [9], [54]],\n",
              " [[47], [81], [66], [20], [95], [29], [132]],\n",
              " [[0], [0], [0], [0], [0], [1], [0]],\n",
              " [[22], [34], [21], [31], [49], [45], [36]],\n",
              " [[21], [62], [32], [10], [108], [27], [25]],\n",
              " [[0], [2], [0], [0], [2], [1], [1]],\n",
              " [[11], [29], [3], [2], [6], [10], [18]],\n",
              " [[2], [0], [1], [1], [0], [1], [2]],\n",
              " [[1], [1], [0], [2], [0], [0], [1]],\n",
              " [[0], [0], [0], [0], [0], [0], [0]],\n",
              " [[3], [10], [3], [4], [6], [9], [18]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_1KGVC8qmQ6"
      },
      "source": [
        "data_list_fin3 = data_list_fin2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fyoDlQpKX6n",
        "outputId": "e0ef40c3-96f5-48a9-8f85-23b05ce06b0f"
      },
      "source": [
        "len(cosine_list) == len(data_list_fin3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI3nyeySas5u",
        "outputId": "1f81b720-65ac-4dc5-8023-ef37debfacc6"
      },
      "source": [
        "len(cosine_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "292"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqTy2Up8K2Wf"
      },
      "source": [
        "# data_list_fin2=[]\n",
        "# for i in data_list2:\n",
        "#   data_list_fin2.append(i[:2000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcBYRPxf8UHJ",
        "outputId": "2d5b063c-389a-4ec2-899e-32bae53614ad"
      },
      "source": [
        "'''\n",
        "Just to confirm, we will check the data points again to see if we did any duplication in the clustering data\n",
        "If there is we will append it to a list to be given to the kmeans clustering algorithm\n",
        "If there is not then we will make a new list and produce clusters out of it with the help Cosine similarity\n",
        "'''\n",
        "\n",
        "cluster_count = 7\n",
        "check_list = []\n",
        "check_list2 = []\n",
        "check_list3 = []\n",
        "mydata_list = []\n",
        "topic_new_clusters = []\n",
        "topic_old_clusters = []\n",
        "count = 0\n",
        "data_data = ['summarization','sentiment analysis','document classification','topic modeling','relation extraction','machine translation','text recognition','speech recognition']\n",
        "cosine_count_list = []\n",
        "cosine_count = 0\n",
        "for i in range(len(cosine_list)):\n",
        "\n",
        "\n",
        "\n",
        "  a = max(cosine_list[i])\n",
        "  \n",
        "  if a[0] < 10:\n",
        "    print(i)\n",
        "  \n",
        "\n",
        "    for j in data_data:\n",
        "      if j in data_list_fin3[i]:\n",
        "        print(data_list_fin2[i])\n",
        "        cosine_count_list.append(cosine_count)\n",
        "        \n",
        "        #data_list_fin3.pop(cosine_count)\n",
        "        break\n",
        "        \n",
        "        \n",
        "    cosine_count+=1\n",
        "  else:\n",
        "    cosine_count+=1\n",
        "def Remove(duplicate):\n",
        "    final_list = []\n",
        "    for num in duplicate:\n",
        "        if num not in final_list:\n",
        "            final_list.append(num)\n",
        "    return final_list\n",
        "     \n",
        "cosine_count_list = Remove(cosine_count_list)\n",
        "cosine_count_list = Reverse(cosine_count_list)\n",
        "for i in cosine_count_list:\n",
        " \n",
        "  cosine_list.pop(i)\n",
        "for i in cosine_count_list:\n",
        "  mydata_list.append(data_list_fin3[i])\n",
        "  topic_old_clusters.append(topic_list2[count])\n",
        "  check_list3.append(embeddings_test[i])\n",
        "  data_list_fin3.pop(i)\n",
        "\n",
        "listt = []\n",
        "for i in range(len(cosine_list)):\n",
        "  a = max(cosine_list[i])\n",
        "  if a[0] < 10:\n",
        "    \n",
        "    print(cluster_count)\n",
        "    check_list.append(embeddings_test[count])\n",
        "    check_list2.append(data_list_fin3[count])\n",
        "    topic_new_clusters.append(topic_list2[count])\n",
        "    listt.append(count)\n",
        "     \n",
        "\n",
        "    count+=1\n",
        "    cluster_count+=1\n",
        "  else:\n",
        "    check_list3.append(embeddings_test[count])\n",
        "    mydata_list.append(data_list_fin3[i])\n",
        "    topic_old_clusters.append(topic_list2[count])\n",
        "    print(cosine_list[i].index(a))\n",
        "    count+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "5\n",
            "8\n",
            "13\n",
            "17\n",
            "18\n",
            "21\n",
            "24\n",
            "25\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "43\n",
            "49\n",
            "51\n",
            "52\n",
            "58\n",
            "67\n",
            "68\n",
            "87\n",
            "88\n",
            "91\n",
            "93\n",
            "96\n",
            "102\n",
            "114\n",
            "121\n",
            "134\n",
            "135\n",
            "141\n",
            "147\n",
            "150\n",
            "151\n",
            "155\n",
            "156\n",
            "157\n",
            "159\n",
            "160\n",
            "164\n",
            "165\n",
            "173\n",
            "174\n",
            "189\n",
            "paper consider parallelization applications whose objective express maximize non monotone submodular function cardinality constraint main result algorithm whose approximation arbitrarily close log adaptive round size grind set exponential speedup parallel run time previously study algorithm constrain non monotone submodular maximization beyond provable guarantee algorithm perform well practice specifically experiment traffic monitor personalize data summarization applications show algorithm find solutions whose value competitive state art algorithms run exponentially fewer parallel iterations\n",
            "194\n",
            "196\n",
            "198\n",
            "199\n",
            "201\n",
            "210\n",
            "212\n",
            "213\n",
            "parallel implementations stochastic gradient descent sgd receive significant research attention thank excellent scalability properties algorithm efficiency context train deep neural network fundamental barrier parallelize large scale sgd fact cost communicate gradient update nod large consequently lossy compression heuristics propose nod communicate quantize gradients although effective practice heuristics always provably converge clear whether optimal paper propose quantize sgd qsgd family compression scheme allow compression gradient update node guarantee convergence standard assumptions qsgd allow user trade compression convergence time communicate sublinear number bits per iteration model dimension achieve asymptotically optimal communication cost complement theoretical result empirical data show qsgd significantly reduce communication cost competitive standard uncompress techniques variety real task particular experiment show gradient quantization apply train deep neural network image classification automate speech recognition lead significant reductions communication cost end end train time instance gpus able train resnet network imagenet faster full accuracy note show exist generic parameter settings know network architectures preserve slightly improve full accuracy use quantization\n",
            "218\n",
            "224\n",
            "226\n",
            "228\n",
            "230\n",
            "233\n",
            "234\n",
            "235\n",
            "237\n",
            "240\n",
            "241\n",
            "242\n",
            "246\n",
            "247\n",
            "248\n",
            "253\n",
            "265\n",
            "268\n",
            "271\n",
            "273\n",
            "274\n",
            "275\n",
            "279\n",
            "283\n",
            "286\n",
            "288\n",
            "289\n",
            "290\n",
            "7\n",
            "8\n",
            "0\n",
            "6\n",
            "6\n",
            "9\n",
            "6\n",
            "1\n",
            "10\n",
            "0\n",
            "1\n",
            "6\n",
            "1\n",
            "11\n",
            "6\n",
            "6\n",
            "6\n",
            "12\n",
            "13\n",
            "1\n",
            "6\n",
            "14\n",
            "5\n",
            "4\n",
            "15\n",
            "16\n",
            "1\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "6\n",
            "6\n",
            "4\n",
            "6\n",
            "6\n",
            "6\n",
            "6\n",
            "6\n",
            "3\n",
            "5\n",
            "3\n",
            "6\n",
            "21\n",
            "5\n",
            "3\n",
            "6\n",
            "0\n",
            "1\n",
            "22\n",
            "6\n",
            "23\n",
            "24\n",
            "3\n",
            "6\n",
            "4\n",
            "1\n",
            "1\n",
            "25\n",
            "6\n",
            "1\n",
            "4\n",
            "4\n",
            "1\n",
            "6\n",
            "6\n",
            "0\n",
            "26\n",
            "27\n",
            "6\n",
            "6\n",
            "5\n",
            "6\n",
            "1\n",
            "4\n",
            "1\n",
            "6\n",
            "4\n",
            "3\n",
            "1\n",
            "3\n",
            "6\n",
            "6\n",
            "4\n",
            "0\n",
            "1\n",
            "5\n",
            "28\n",
            "29\n",
            "6\n",
            "1\n",
            "30\n",
            "6\n",
            "31\n",
            "1\n",
            "6\n",
            "32\n",
            "6\n",
            "4\n",
            "4\n",
            "4\n",
            "1\n",
            "33\n",
            "5\n",
            "3\n",
            "4\n",
            "6\n",
            "4\n",
            "6\n",
            "6\n",
            "6\n",
            "3\n",
            "6\n",
            "4\n",
            "34\n",
            "1\n",
            "5\n",
            "6\n",
            "6\n",
            "4\n",
            "6\n",
            "35\n",
            "6\n",
            "6\n",
            "6\n",
            "1\n",
            "5\n",
            "4\n",
            "6\n",
            "6\n",
            "3\n",
            "4\n",
            "4\n",
            "6\n",
            "36\n",
            "37\n",
            "6\n",
            "4\n",
            "4\n",
            "6\n",
            "6\n",
            "38\n",
            "6\n",
            "1\n",
            "4\n",
            "1\n",
            "6\n",
            "39\n",
            "6\n",
            "1\n",
            "40\n",
            "41\n",
            "1\n",
            "1\n",
            "4\n",
            "42\n",
            "43\n",
            "44\n",
            "5\n",
            "45\n",
            "46\n",
            "6\n",
            "6\n",
            "1\n",
            "47\n",
            "48\n",
            "6\n",
            "6\n",
            "6\n",
            "3\n",
            "6\n",
            "3\n",
            "6\n",
            "49\n",
            "50\n",
            "4\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "6\n",
            "6\n",
            "6\n",
            "6\n",
            "1\n",
            "6\n",
            "0\n",
            "6\n",
            "4\n",
            "3\n",
            "1\n",
            "3\n",
            "4\n",
            "51\n",
            "5\n",
            "52\n",
            "6\n",
            "53\n",
            "54\n",
            "3\n",
            "55\n",
            "4\n",
            "6\n",
            "6\n",
            "6\n",
            "1\n",
            "6\n",
            "3\n",
            "3\n",
            "56\n",
            "1\n",
            "57\n",
            "0\n",
            "6\n",
            "4\n",
            "4\n",
            "58\n",
            "1\n",
            "4\n",
            "6\n",
            "1\n",
            "4\n",
            "59\n",
            "6\n",
            "60\n",
            "6\n",
            "61\n",
            "6\n",
            "62\n",
            "6\n",
            "0\n",
            "63\n",
            "64\n",
            "65\n",
            "6\n",
            "66\n",
            "1\n",
            "1\n",
            "67\n",
            "68\n",
            "69\n",
            "1\n",
            "4\n",
            "1\n",
            "70\n",
            "71\n",
            "72\n",
            "1\n",
            "1\n",
            "6\n",
            "3\n",
            "73\n",
            "6\n",
            "1\n",
            "6\n",
            "6\n",
            "3\n",
            "1\n",
            "6\n",
            "4\n",
            "1\n",
            "6\n",
            "6\n",
            "74\n",
            "6\n",
            "6\n",
            "75\n",
            "1\n",
            "6\n",
            "76\n",
            "1\n",
            "77\n",
            "78\n",
            "79\n",
            "6\n",
            "6\n",
            "6\n",
            "80\n",
            "4\n",
            "4\n",
            "6\n",
            "81\n",
            "4\n",
            "4\n",
            "82\n",
            "1\n",
            "83\n",
            "84\n",
            "85\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIUuaTKvw98Z",
        "outputId": "fb02a158-9433-4e82-ae3b-6e4d0be0272e"
      },
      "source": [
        "len(check_list2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kzs_3jtIUeU"
      },
      "source": [
        "mydata = check_list2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfWcCBnjFgl2",
        "outputId": "ed41a172-09d8-4dd0-f0dd-ed780e23f84e"
      },
      "source": [
        "#This is the data which is to be used to cluster together with the help of cosine similarity NOT KMeans\n",
        "\n",
        "mydata"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['paper address graph match problem follow recent work cite zaslavskiy path vestner analyze generalize idea concave relaxations introduce concepts emph conditionally concave emph probably conditionally concave energies polytopes show encapsulate many instance graph match problem include match euclidean graph graph surface prove local minima probably conditionally concave energies general match polytopes doubly stochastic high probability extreme point match polytope permutations',\n",
              " 'aim obtain interpretable expressive disentangle scene representation contain comprehensive structural textural information object previous scene representations learn neural network often uninterpretable limit single object lack knowledge work propose scene render network sdn address issue integrate disentangle representations semantics geometry appearance deep generative model scene encoder perform inverse graphics translate scene structure object wise representation decoder two components differentiable shape renderer neural texture generator disentanglement semantics geometry appearance support aware scene manipulation rotate move object freely keep consistent shape texture change object appearance without affect shape experiment demonstrate edit scheme base sdn superior counterpart',\n",
              " 'introduce game theoretic approach study recommendation systems strategic content providers systems fair stable show traditional approach fail satisfy requirements propose shapley mediator show shapley mediator satisfy fairness stability requirements run linear time economically efficient mechanism satisfy properties',\n",
              " 'recent work suggest enhance bloom filter use pre filter base apply machine learn determine function model data set bloom filter mean represent model learn bloom filter follow outcomes clarify guarantee associate structure show estimate size learn function must obtain order obtain improve performance provide simple method sandwich optimize learn bloom filter propose design analysis approach learn bloomier filter base model approach',\n",
              " 'bandit learn characterize tension long term exploration short term exploitation however recently note settings choices learn algorithm correspond important decisions individual people criminal recidivism prediction lend sequential drug trials exploration correspond explicitly sacrifice well one individual potential future benefit others settings one might like run greedy algorithm always make optimal decision individuals hand result catastrophic failure learn paper consider linear contextual bandit problem revisit performance greedy algorithm',\n",
              " 'present first accelerate randomize algorithm solve linear systems euclidean space one essential problem type matrix inversion problem particular algorithm specialize invert positive definite matrices way iterate approximate solutions generate algorithm positive definite matrices open way many applications field optimization machine learn application general theory develop first accelerate deterministic stochastic quasi newton update update lead provably aggressive approximations inverse hessian lead speed up classical non accelerate rule numerical experiment experiment empirical risk minimization show rule accelerate train machine learn model',\n",
              " 'stochastic gradient descent popular large scale optimization slow convergence asymptotically due inherent variance remedy problem introduce explicit variance reduction method stochastic gradient descent call stochastic variance reduce gradient svrg smooth strongly convex function prove method enjoy fast convergence rate stochastic dual coordinate ascent sdca stochastic average gradient sag however analysis significantly simpler intuitive moreover unlike sdca sag method require storage gradients thus easily applicable complex problems structure prediction problems neural network learn',\n",
              " 'machine learn model vulnerable adversarial examples small change image cause computer vision model make mistake identify school bus ostrich however still open question whether humans prone similar mistake address question leverage recent techniques transfer adversarial examples computer vision model know parameters architecture model unknown parameters architecture match initial process human visual system find adversarial examples strongly transfer across computer vision model influence classifications make time limit human observers',\n",
              " 'algorithmically construct multi output gaussian process priors satisfy linear differential equations approach attempt parametrize solutions equations use bner base successful push forward gaussian process along paramerization desire prior consider several examples physics geomathmatics control among full inhomogeneous system maxwell equations bring together stochastic learn computeralgebra novel way combine noisy observations precise algebraic computations',\n",
              " 'learn decision tree data difficult optimization problem widespread algorithm practice date base greedy growth tree structure recursively split nod possibly prune back final tree parameters decision function internal node approximately estimate minimize impurity measure give algorithm give input tree structure parameter value nod produce new tree smaller structure new parameter value provably lower leave unchanged misclassification error apply axis align oblique tree experiment show consistently outperform various algorithms highly scalable large datasets tree algorithm handle sparsity penalty learn sparse oblique tree structure subset original tree nonzero parameters combine best axis align oblique tree flexibility model correlate data low generalization error fast inference interpretable nod involve feature decision',\n",
              " 'ideas enjoy large impact deep learn convolution problem involve pixels spatial representations common intuition hold convolutional neural network may appropriate paper show strike counterexample intuition via seemingly trivial coordinate transform problem simply require learn map coordinate cartesian space coordinate one hot pixel space although convolutional network would seem appropriate task show fail spectacularly demonstrate carefully analyze failure first toy problem point simple fix become obvious call solution coordconv work give convolution access input coordinate use extra coordinate channel without sacrifice computational parametric efficiency ordinary convolution coordconv allow network learn either complete translation invariance vary degrees translation dependence require end task coordconv solve coordinate transform problem perfect generalization time faster time fewer parameters convolution stark contrast raise question extent inability convolution persist insidiously inside task subtly hamper performance within complete answer question require investigation show preliminary evidence swap convolution coordconv improve model diverse set task use coordconv gin produce less mode collapse transform high level spatial latents pixels become easier learn faster cnn detection model train mnist detection show better iou use coordconv reinforcement learn domain agents play atari game benefit significantly use coordconv layer',\n",
              " 'policy gradient methods widely use control reinforcement learn particularly continuous action set host theoretically sound algorithms propose policy set due existence policy gradient theorem provide simplify form gradient policy learn however behaviour policy necessarily attempt learn follow optimal policy give task existence theorem elusive work solve open problem provide first policy policy gradient theorem key derivation use emphatic weight develop new actor critic algorithm call actor critic emphatic weight ace approximate simplify gradients provide theorem demonstrate simple counterexample previous policy policy gradient methods particularly offpac dpg converge wrong solution whereas ace find optimal solution',\n",
              " 'paper try organize machine teach coherent set ideas idea present vary along dimension collection dimension form problem space machine teach exist teach problems characterize space hope organization allow gain deeper understand individual teach problems discover connections among identify gap field',\n",
              " 'goal orient dialog give attention due numerous applications artificial intelligence goal orient dialogue task occur questioner ask action orient question answerer respond intent let questioner know correct action take ask adequate question deep learn reinforcement learn recently apply however approach struggle find competent recurrent neural questioner owe complexity learn series sentence motivate theory mind propose answerer questioner mind aqm novel information theoretic algorithm goal orient dialog aqm questioner ask infer base approximate probabilistic model answerer questioner figure answerer intention via select plausible question explicitly calculate information gain candidate intentions possible answer question test framework two goal orient visual dialog task mnist count dialog guesswhat experiment aqm outperform comparative algorithms large margin',\n",
              " 'attention network multimodal learn provide efficient way utilize give visual information selectively however computational cost learn attention distributions every pair multimodal input channel prohibitively expensive solve problem attention build two separate attention distributions modality neglect interaction multimodal input paper propose bilinear attention network ban find bilinear attention distributions utilize give vision language information seamlessly ban consider bilinear interactions among two group input channel low rank bilinear pool extract joint representations pair channel furthermore propose variant multimodal residual network exploit eight attention map ban efficiently quantitatively qualitatively evaluate model visual question answer vqa flickr entities datasets show ban significantly outperform previous methods achieve new state arts datasets',\n",
              " 'reason play essential role visual question answer vqa multi step dynamic reason often necessary answer complex question example question place next bus right picture talk compound object bus right generate relation',\n",
              " 'present novel newton type method distribute optimization particularly well suit stochastic optimization learn problems quadratic objectives method enjoy linear rate convergence provably emph improve data size require essentially constant number iterations reasonable assumptions provide theoretical empirical evidence advantage method compare approach one shoot parameter average admm',\n",
              " 'bilinear model show achieve impressive performance wide range visual task semantic segmentation fine grain recognition face recognition however bilinear feature high dimensional typically order hundreds thousands million make impractical subsequent analysis propose two compact bilinear representations discriminative power full bilinear representation thousand dimension compact representations allow back propagation classification errors enable end end optimization visual recognition system compact bilinear representations derive novel kernelized analysis bilinear pool provide insights discriminative power bilinear pool platform research compact pool methods experimentation illustrate utility propose representations image classification shoot learn across several datasets',\n",
              " 'adapt ideas underlie success deep learn continuous action domain present actor critic model free algorithm base deterministic policy gradient operate continuous action space use learn algorithm network architecture hyper parameters algorithm robustly solve simulate physics task include classic problems cartpole swing dexterous manipulation legged locomotion car drive algorithm able find policies whose performance competitive find plan algorithm full access dynamics domain derivatives demonstrate many task algorithm learn policies end end directly raw pixel input',\n",
              " 'hash one popular powerful approximate nearest neighbor search techniques large scale image retrieval traditional hash methods first represent image shelf visual feature produce hash cod separate stage however shelf visual feature may optimally compatible hash code learn procedure may result sub optimal hash cod recently deep hash methods propose simultaneously learn image feature hash cod use deep neural network show superior performance traditional hash methods deep hash methods give supervise information form pairwise label triplet label current state art deep hash method dpsh cite feature base pairwise label perform image feature learn hash code learn simultaneously maximize likelihood pairwise similarities inspire dpsh cite feature propose triplet label base deep hash method aim maximize likelihood give triplet label experimental result show method outperform baselines cifar nus wide datasets include state art method dpsh cite feature previous triplet label base deep hash methods',\n",
              " 'accurate exposure key capture high quality photos computational photography especially mobile phone limit size camera modules inspire luminosity mask usually apply professional photographers paper develop novel algorithm learn local exposures deep reinforcement adversarial learn specific segment image sub image reflect variations dynamic range exposures accord raw low level feature base sub image local exposure sub image automatically learn virtue policy network sequentially reward learn globally design strike balance overall exposures aesthetic evaluation function approximate discriminator generative adversarial network reinforcement learn adversarial learn train collaboratively asynchronous deterministic policy gradient generative loss approximation simply algorithmic architecture also prove feasibility leverage discriminator value function employ local exposure retouch raw input image respectively thus deliver multiple retouch image different exposures fuse exposure blend extensive experiment verify algorithms superior state art methods term quantitative accuracy visual illustration',\n",
              " 'paper compare different type recurrent units recurrent neural network rnns especially focus sophisticate units implement gate mechanism long short term memory lstm unit recently propose gate recurrent unit gru evaluate recurrent units task polyphonic music model speech signal model experiment reveal advance recurrent units indeed better traditional recurrent units tanh units also find gru comparable lstm',\n",
              " 'societies often rely human experts take wide variety decisions affect members jail release decisions take judge stop frisk decisions take police officer accept reject decisions take academics context decision take expert typically choose uniformly random pool experts however decisions may imperfect due limit experience implicit bias faulty probabilistic reason improve accuracy fairness overall decision make process optimize assignment experts decisions',\n",
              " 'draw attention important yet largely overlook aspect evaluate fairness automate decision make systems namely risk welfare considerations propose family measure correspond long establish formulations cardinal social welfare economics justify rawlsian conception fairness behind veil ignorance convex formulation welfare base measure fairness allow integrate constraint convex loss minimization pipeline empirical analysis reveal interest trade off proposal prediction accuracy group discrimination dwork notion individual fairness furthermore perhaps importantly work provide heuristic justification empirical evidence suggest lower bind measure often lead bound inequality algorithmic outcomes hence present first computationally feasible mechanism bound individual level inequality',\n",
              " 'state art object detection network depend region proposal algorithms hypothesize object locations advance like sppnet fast cnn reduce run time detection network expose region proposal computation bottleneck work introduce region proposal network rpn share full image convolutional feature detection network thus enable nearly cost free region proposals rpn fully convolutional network simultaneously predict object bound objectness score position rpn train end end generate high quality region proposals use fast cnn detection merge rpn fast cnn single network share convolutional feature use recently popular terminology neural network attention mechanisms rpn component tell unify network look deep vgg model detection system frame rate fps include step gpu achieve state art object detection accuracy pascal voc coco datasets proposals per image ilsvrc coco competitions faster cnn rpn foundations place win entries several track code make publicly available',\n",
              " 'typical way network data record measure interactions involve specify set core nod produce graph contain core together potentially larger set fringe nod link core interactions nod fringe however present result graph data example phone service provider may record call least one participants customer include call customer non customer pair non customers knowledge nod belong core crucial interpret dataset metadata unavailable many case either lose due difficulties data provenance network consist find data obtain settings counter surveillance lead algorithmic problem recover core set since core vertex cover essentially plant vertex cover problem arbitrary underlie graph develop framework analyze plant vertex cover problem base theory fix parameter tractability together algorithms recover core algorithms fast simple implement perform several baselines base core periphery structure various real world datasets',\n",
              " 'technical challenge deep learn recognize target class without see data zero shoot learn leverage semantic representations attribute class prototypes bridge source target class exist standard zero shoot learn methods may prone overfitting see data source class blind semantic representations target class paper study generalize zero shoot learn assume accessible target class unseen data train prediction unseen data make search source target class propose novel deep calibration network dcn approach towards generalize zero shoot learn paradigm enable simultaneous calibration deep network confidence source class uncertainty target class approach map visual feature image semantic representations class prototypes common embed space compatibility see data source target class maximize show superior accuracy approach state art benchmark datasets generalize zero shoot learn include awa cub sun apy',\n",
              " 'large scale network model use neurons static nonlinearities produce analog output despite fact information process brain predominantly carry dynamic neurons produce discrete pulse call spike research spike base computation impede lack efficient supervise learn algorithm spike neural network present gradient descent method optimize spike network model introduce differentiable formulation spike dynamics derive exact gradient calculation demonstration train recurrent spike network two dynamic task one require optimize fast millisecond spike base interactions efficient encode information delay memory task extend duration second result show gradient descent approach indeed optimize network dynamics time scale individual spike well behavioral time scale conclusion method yield general purpose supervise learn algorithm spike neural network facilitate investigations spike base computations',\n",
              " 'stochastic gradient descent sgd popular algorithm achieve state art performance variety machine learn task several researchers recently propose scheme parallelize sgd require performance destroy memory lock synchronization work aim show use novel theoretical analysis algorithms implementation sgd implement without lock present update scheme call hogwild allow processors access share memory possibility overwrite work show associate optimization problem sparse mean gradient update modify small part decision variable hogwild achieve nearly optimal rate convergence demonstrate experimentally hogwild outperform alternative scheme use lock order magnitude',\n",
              " 'hypergraph partition important problem machine learn computer vision network analytics widely use method hypergraph partition rely minimize normalize sum cost partition hyperedges across cluster algorithmic solutions base approach assume different partition hyperedge incur cost however assumption fail leverage fact different subsets vertices within hyperedge may different structural importance hence propose new hypergraph cluster technique term inhomogeneous hypergraph partition assign different cost different hyperedge cut prove inhomogeneous partition produce quadratic approximation optimal solution inhomogeneous cost satisfy submodularity constraints moreover demonstrate inhomogenous partition offer significant performance improvements applications structure learn rank subspace segmentation motif cluster',\n",
              " 'work introduce interactive structure learn framework unify many different interactive learn task present generalization query committee active learn algorithm set study consistency rate convergence theoretically empirically without noise',\n",
              " 'convolutional neural network cnns recently achieve great success single image super resolution sisr however methods tend produce smooth output miss textural detail solve problems propose super resolution cliquenet srcliquenet reconstruct high resolution image better textural detail wavelet domain propose srcliquenet firstly extract set feature map low resolution image clique block group send set feature map clique sample module reconstruct image clique sample module consist four sub net predict high resolution wavelet coefficients four sub band since consider edge feature properties four sub band four sub net connect others learn coefficients four sub band jointly finally apply inverse discrete wavelet transform idwt output four sub net end clique sample module increase resolution reconstruct image extensive quantitative qualitative experiment benchmark datasets show method achieve superior performance state art methods',\n",
              " 'consider problem learn optimal reserve price repeat auction non myopic bidders may bid strategically order gain future round even single round auction truthful previous algorithms empirical price provide non trivial regret round set general introduce algorithms obtain small regret non myopic bidders either market large bidder appear constant fraction round bidders impatient discount future utility factor mildly bound away one approach carefully control information reveal bidder build techniques differentially private online learn well recent line work jointly differentially private algorithms',\n",
              " 'propose simple yet effective approach spatiotemporal feature learn use deep dimensional convolutional network convnets train large scale supervise video dataset find three fold convnets suitable spatiotemporal feature learn compare convnets homogeneous architecture small convolution kernels layer among best perform architectures convnets learn feature namely convolutional simple linear classifier outperform state art methods different benchmarks comparable current best methods benchmarks addition feature compact achieve accuracy ucf dataset dimension also efficient compute due fast inference convnets finally conceptually simple easy train use',\n",
              " 'goal predict future video frame give sequence input frame despite large amount video data remain challenge task high dimensionality video frame address challenge propose decompositional disentangle predictive auto encoder ddpae framework combine structure probabilistic model deep network automatically decompose high dimensional video aim predict components disentangle component low dimensional temporal dynamics easier predict crucially appropriately specify generative model video frame ddpae able learn latent decomposition disentanglement without explicit supervision move mnist dataset show ddpae able recover underlie components individual digits disentanglement appearance location would intuitively demonstrate ddpae apply bounce ball dataset involve complex interactions multiple object predict video frame directly pixels recover physical state without explicit supervision',\n",
              " 'paper introduce versatile filter construct efficient convolutional neural network consider demand efficient deep learn techniques run cost effective hardware number methods develop learn compact neural network work aim slim filter different ways investigate small sparse binarized filter contrast treat filter additive perspective series secondary filter derive primary filter secondary filter inherit primary filter without occupy storage unfold computation could significantly enhance capability filter integrate information extract different receptive field besides spatial versatile filter additionally investigate versatile filter channel perspective new techniques general upgrade filter exist cnns experimental result benchmark datasets neural network demonstrate cnns construct versatile filter able achieve comparable accuracy original filter require less memory flop',\n",
              " 'unsupervised learn generative adversarial network gans prove hugely successful regular gans hypothesize discriminator classifier sigmoid cross entropy loss function however find loss function may lead vanish gradients problem learn process overcome problem propose paper least square generative adversarial network lsgans adopt least square loss function discriminator show minimize objective function lsgan yield minimize pearson chi divergence two benefit lsgans regular gans first lsgans able generate higher quality image regular gans second lsgans perform stable learn process evaluate lsgans five scene datasets experimental result show image generate lsgans better quality ones generate regular gans also conduct two comparison experiment lsgans regular gans illustrate stability lsgans',\n",
              " 'suppose design matrix linear regression problem give response point hide unless explicitly request goal sample small number responses produce weight vector whose sum square loss point epsilon time minimum small jointly sample diverse subsets point crucial one method call volume sample unique desirable property weight vector produce unbiased estimate optimum therefore natural ask method offer optimal unbiased estimate term number responses need achieve epsilon loss approximation',\n",
              " 'several large scale deployments differential privacy use collect statistical information users however deployments periodically recollect data recompute statistics use algorithms design single use result systems provide meaningful privacy guarantee long time scale moreover exist techniques mitigate effect apply local model differential privacy systems use',\n",
              " 'recurrent network spike neurons rsnns underlie astound compute learn capabilities brain compute learn capabilities rsnn model remain poor least comparison anns address two possible reason one rsnns brain randomly connect design accord simple rule start learn tabula rasa network rather rsnns brain optimize task evolution development prior experience detail optimization process largely unknown functional contribution approximate powerful optimization methods backpropagation time bptt',\n",
              " 'propose novel flexible anchor mechanism name metaanchor object detection frameworks unlike many previous detectors model anchor via predefined manner metaanchor anchor function could dynamically generate arbitrary customize prior box take advantage weight prediction metaanchor able work anchor base object detection systems retinanet compare predefined anchor scheme empirically find metaanchor robust anchor settings bound box distributions addition also show potential transfer task experiment coco detection task show metaanchor consistently outperform counterparts various scenarios',\n",
              " 'paper propose conceptually simple general framework call metagan shoot learn problems state art shoot classification model integrate metagan principled straightforward way introduce adversarial generator condition task augment vanilla shoot classification model ability discriminate real fake data argue gin base approach help shoot classifiers learn sharper decision boundary could generalize better show metagan framework extend supervise shoot learn model naturally cope unsupervised data different previous work semi supervise shoot learn algorithms deal semi supervision sample level task level give theoretical justifications strength metagan validate effectiveness metagan challenge shoot image classification benchmarks',\n",
              " 'ensembles randomize decision tree usually refer random forest widely use classification regression task machine learn statistics random forest achieve competitive predictive performance computationally efficient train test make excellent candidates real world prediction task popular random forest variants breiman random forest extremely randomize tree operate batch train data online methods greater demand exist online random forest however require train data batch counterpart achieve comparable predictive performance work use mondrian process roy teh construct ensembles random decision tree call mondrian forest mondrian forest grow incremental online fashion remarkably distribution online mondrian forest batch mondrian forest mondrian forest achieve competitive predictive performance comparable exist online random forest periodically train batch random forest order magnitude faster thus represent better computation accuracy tradeoff',\n",
              " 'contextual bandit literature traditionally focus algorithms address exploration exploitation tradeoff particular greedy algorithms exploit current estimate without exploration may sub optimal general however exploration free greedy algorithms desirable practical settings exploration may costly unethical clinical trials surprisingly find simple greedy algorithm rate optimal achieve asymptotically optimal regret sufficient randomness observe contexts covariates prove always case two arm bandit general class context distributions satisfy condition term covariate diversity furthermore even absent condition show greedy algorithm rate optimal positive probability thus standard bandit algorithms may unnecessarily explore motivate result introduce greedy first new algorithm use observe contexts reward determine whether follow greedy algorithm explore prove algorithm rate optimal without additional assumptions context distribution number arm extensive simulations demonstrate greedy first successfully reduce exploration outperform exist exploration base contextual bandit algorithms thompson sample upper confidence bind ucb',\n",
              " 'consider problem online learn linear contextual bandits set also strong individual fairness constraints govern unknown similarity metric constraints demand select similar action individuals approximately equal probability dhprz may odds optimize reward thus model settings profit social policy tension assume learn unknown mahalanobis similarity metric weak feedback identify fairness violations quantify extent intend represent interventions regulator know unfairness see nevertheless enunciate quantitative fairness metric individuals main result algorithm adversarial context set number fairness violations depend logarithmically obtain optimal sqrt regret bind best fair policy',\n",
              " 'accurately answer question give image require combine observations general knowledge effortless humans reason general knowledge remain algorithmic challenge advance research direction novel reason correct answer jointly consider entities show challenge fvqa dataset lead improvement accuracy around compare state art',\n",
              " 'modern visual question answer vqa model show rely heavily superficial correlations question answer word learn train overwhelmingly report type room kitchen sport play tennis irrespective image alarmingly shortcoming often well reflect evaluation strong priors exist test distributions however vqa system fail grind question image content would likely perform poorly real world settings',\n",
              " 'people belong multiple communities word belong multiple topics book cover multiple genres overlap cluster commonplace many exist overlap cluster methods model person word book non negative weight combination exemplars belong solely one community small noise geometrically person point cone whose corner exemplars basic form encompass widely use mix membership stochastic blockmodel network degree correct variants well topic model lda show simple one class svm yield provably consistent parameter inference model scale large datasets experimental result several simulate real datasets show algorithm call svm cone accurate scalable',\n",
              " 'generative adversarial network gans technique learn generative model complex data distributions sample despite remarkable advance generate realistic image major shortcoming gans fact tend produce sample little diversity even train diverse datasets phenomenon know mode collapse focus much recent work study principled approach handle mode collapse call pack main idea modify discriminator make decisions base multiple sample class either real artificially generate draw analysis tool binary hypothesis test particular seminal result blackwell prove fundamental connection pack mode collapse show pack naturally penalize generators mode collapse thereby favor generator distributions less mode collapse train process numerical experiment benchmark datasets suggest pack provide significant improvements',\n",
              " 'learn solve complex sequence task leverage transfer avoid catastrophic forget remain key obstacle achieve human level intelligence progressive network approach represent step forward direction immune forget leverage prior knowledge via lateral connections previously learn feature evaluate architecture extensively wide variety reinforcement learn task atari maze game show outperform common baselines base pretraining finetuning use novel sensitivity measure demonstrate transfer occur low level sensory high level control layer learn policy',\n",
              " 'propose new family policy gradient methods reinforcement learn alternate sample data interaction environment optimize surrogate objective function use stochastic gradient ascent whereas standard policy gradient methods perform one gradient update per data sample propose novel objective function enable multiple epochs minibatch update new methods call proximal policy optimization ppo benefit trust region policy optimization trpo much simpler implement general better sample complexity empirically experiment test ppo collection benchmark task include simulate robotic locomotion atari game play show ppo outperform online policy gradient methods overall strike favorable balance sample complexity simplicity wall time',\n",
              " 'generative recurrent neural network quickly train unsupervised manner model popular reinforcement learn environments compress spatio temporal representations world model extract feature feed compact simple policies train evolution achieve state art result various environments also train agent entirely inside environment generate internal world model transfer policy back actual environment interactive version paper available https worldmodels github',\n",
              " 'study problem policy policy evaluation oppe contrast prior work consider estimate individual policy value average policy value accurately draw inspiration recent work causal reason propose new finite sample generalization error bind value estimate mdp model use upper bind objective develop learn algorithm mdp model balance representation show approach yield substantially lower mse common synthetic benchmarks hiv treatment simulation domain',\n",
              " 'propose structure adaptive variant state art stochastic variance reduce gradient algorithm katyusha regularize empirical risk minimization propose method able exploit intrinsic low dimensional structure solution sparsity low rank enforce non smooth regularization achieve even faster convergence rate provable algorithmic improvement do restart katyusha algorithm accord restrict strong convexity constants demonstrate effectiveness approach via numerical experiment',\n",
              " 'introduce new approach decomposable submodular function minimization dsfm exploit incidence relations incidence relations describe variables effectively influence component function properly utilize allow improve convergence rat dsfm solvers main result include precise parametrization dsfm problem base incidence relations development new scalable alternative projections parallel coordinate descent methods accompany rigorous analysis convergence rat',\n",
              " 'note consider normalize gradient descent ngd natural modification classical gradient descent optimization problems serious shortcoming non convex problems may take arbitrarily long escape neighborhood saddle point issue make convergence arbitrarily slow particularly high dimensional non convex problems relative number saddle point often large paper focus continuous time descent show contrary standard ngd escape saddle point quickly particular show ngd almost never converge saddle point time require ngd escape ball radius saddle point sqrt kappa kappa condition number hessian application result global convergence time bind establish ngd mild assumptions',\n",
              " 'paper propose stochastic recursive gradient algorithm sarah well practical variant sarah novel approach finite sum minimization problems different vanilla sgd modern stochastic methods svrg sag saga sarah admit simple recursive framework update stochastic gradient estimate compare sag saga sarah require storage past gradients linear convergence rate sarah prove strong convexity assumption also prove linear convergence rate strongly convex case inner loop sarah property svrg possess numerical experiment demonstrate efficiency algorithm',\n",
              " 'semantic scene completion predict volumetric occupancy object category scene help intelligent agents understand interact surround work propose disentangle framework sequentially carry semantic segmentation reprojection semantic scene completion three stage framework three advantage explicit semantic segmentation significantly boost performance flexible fusion ways sensor data bring good extensibility progress subtask promote holistic performance experimental result show regardless input single depth rgb framework generate high quality semantic scene completion outperform state art approach synthetic real datasets',\n",
              " 'propose novel randomize first order optimization method sega sketch gradient method progressively throughout iterations build variance reduce estimate gradient random linear measurements sketch gradient provide iteration oracle iteration sega update current estimate gradient sketch project operation use information provide latest sketch subsequently use compute unbiased estimate true gradient random relaxation procedure unbiased estimate use perform gradient step unlike standard subspace descent methods coordinate descent sega use optimization problems non separable proximal term provide general convergence analysis prove linear convergence strongly convex objectives special case coordinate sketch sega enhance various techniques importance sample minibatching acceleration rate small constant factor identical best know rate coordinate descent',\n",
              " 'introduce approach convert mono audio record video camera spatial audio representation distribution sound full view sphere spatial audio important component immersive video view spatial audio microphones still rare current video production system consist end end trainable neural network separate individual sound source localize view sphere condition multi modal analysis audio video frame introduce several datasets include one film one collect wild youtube consist videos upload spatial audio train grind truth spatial audio serve self supervision mix mono track form input network use approach show possible infer spatial localization sound base synchronize video mono audio track',\n",
              " 'duplicate removal critical step accomplish reasonable amount predictions prevalent proposal base object detection frameworks albeit simple effective previous algorithms utilize greedy process without make sufficient use properties input data work design new two stage framework effectively select appropriate proposal candidate object first stage suppress easy negative object proposals second stage select true positives reduce proposal set two stag share network structure encoder decoder form recurrent neural network rnn global attention context gate encoder scan proposal candidates sequential manner capture global context information feed decoder extract optimal proposals extensive experiment propose method outperform alternatives large margin',\n",
              " 'present shapenet richly annotate large scale repository shape represent cad model object shapenet contain model multitude semantic categories organize wordnet taxonomy collection datasets provide many semantic annotations model consistent rigid alignments part bilateral symmetry plan physical size keywords well plan annotations annotations make available public web base interface enable data visualization object attribute promote data drive geometric analysis provide large scale quantitative benchmark research computer graphics vision time technical report shapenet index model model classify categories wordnet synsets report describe shapenet effort whole provide detail currently available datasets summarize future plan',\n",
              " 'softmax output activation function model categorical probability distributions many applications deep learn however recent study reveal softmax bottleneck representational capacity neural network language model softmax bottleneck paper propose output activation function break softmax bottleneck without additional parameters analyze softmax bottleneck perspective output set log softmax identify cause softmax bottleneck basis analysis propose sigsoftmax compose multiplication exponential function sigmoid function sigsoftmax break softmax bottleneck experiment language model demonstrate sigsoftmax mixture sigsoftmax outperform softmax mixture softmax respectively',\n",
              " 'despite remarkable advance image synthesis research exist work often fail manipulate image context large geometric transformations synthesize person image condition arbitrary pose one representative examples generation quality largely rely capability identify model arbitrary transformations different body part current generative model often build local convolutions overlook key challenge heavy occlusions different view dramatic appearance change distinct geometric change happen part cause arbitrary pose manipulations paper aim resolve challenge induce geometric variability spatial displacements via new soft gate warp generative adversarial network warp gin compose two stag first synthesize target part segmentation map give target pose depict region level spatial layouts guide image synthesis higher level structure constraints warp gin equip soft gate warp block learn feature level map render textures original image generate segmentation map warp gin capable control different transformation degrees give distinct target pose moreover propose warp block light weight flexible enough inject network human perceptual study quantitative evaluations demonstrate superiority warp gin significantly outperform exist methods two large datasets',\n",
              " 'tremendous recent progress equilibrium find algorithms zero sum imperfect information extensive form game puzzle gap theory practice first order methods significantly better theoretical convergence rat counterfactual regret minimization cfr variant despite cfr variants favor practice experiment first order methods conduct small medium size game methods complicate implement set cfr variants enhance extensively decade perform well practice paper show particular first order method state art variant excessive gap technique instantiate dilate entropy distance function efficiently solve large real world problems competitively cfr variants show large endgames encounter libratus poker recently beat top human poker specialist professionals limit texas hold show experimental result variant excessive gap technique well prior version introduce numerically friendly implementation smooth best response computation associate first order methods extensive form game solve present knowledge first gpu implementation first order method extensive form game present comparisons several excessive gap technique cfr variants',\n",
              " 'significant interest able predict crimes happen example aid efficient task police protective measure aim model temporal spatial dependencies often exhibit violent crimes order make predictions temporal variation crimes typically follow pattern familiar time series analysis spatial pattern irregular vary smoothly across area instead find spatially disjoint regions exhibit correlate crime pattern indeterminate inter region correlation structure along low count discrete nature count serious crimes motivate propose forecast tool particular propose model crime count region use integer value first order autoregressive process take bayesian nonparametric approach flexibly discover cluster region specific time series describe account covariates within framework approach adjust seasonality demonstrate approach analysis weekly report violent crimes washington forecast outperform standard methods additionally provide useful tool prediction intervals',\n",
              " 'convolutional neural network cnns inherently subject invariable filter aggregate local input topological structure cause cnns allow manage data euclidean grid like structure image ones non euclidean graph structure traffic network broaden reach cnns develop structure aware convolution eliminate invariance yield unify mechanism deal euclidean non euclidean structure data technically filter structure aware convolution generalize univariate function capable aggregate local input diverse topological structure since infinite parameters require determine univariate function parameterize filter number learnable parameters context function approximation theory replace classical convolution cnns structure aware convolution structure aware convolutional neural network sacnns readily establish extensive experiment eleven datasets strongly evidence sacnns outperform current model various machine learn task include image classification cluster text categorization skeleton base action recognition molecular activity detection taxi flow prediction',\n",
              " 'wide adoption dnns give birth unrelenting compute requirements force datacenter operators adopt domain specific accelerators train accelerators typically employ densely pack full precision float point arithmetic maximize performance per area ongoing research efforts seek increase performance density replace float point fix point arithmetic however significant roadblock attempt fix point narrow dynamic range insufficient dnn train convergence identify block float point bfp promise alternative representation since exhibit wide dynamic range enable majority dnn operations perform fix point logic unfortunately bfp alone introduce several limitations preclude direct applicability work introduce hbfp hybrid bfp approach perform dot products bfp operations float point hbfp deliver best worlds high accuracy float point superior hardware density fix point wide variety model show hbfp match float point accuracy enable hardware implementations deliver higher throughput',\n",
              " 'describe iterative procedure optimize policies guarantee monotonic improvement make several approximations theoretically justify procedure develop practical algorithm call trust region policy optimization trpo algorithm similar natural policy gradient methods effective optimize large nonlinear policies neural network experiment demonstrate robust performance wide variety task learn simulate robotic swim hop walk gaits play atari game use image screen input despite approximations deviate theory trpo tend give monotonic improvement little tune hyperparameters',\n",
              " 'real world applications education effective teacher adaptively choose next example teach base learner current state however exist work algorithmic machine teach focus batch set adaptivity play role paper study case teach consistent version space learners interactive set time step teacher provide example learner perform update teacher observe learner new state highlight adaptivity speed teach process consider exist model version space learners worst case model learner pick next hypothesis randomly version space preference base model learner pick hypothesis accord global preference inspire human teach propose new model learner pick hypotheses accord local preference define current hypothesis show model exhibit several desirable properties adaptivity play key role learner transition hypotheses smooth interpretable develop adaptive teach algorithms demonstrate result via simulation user study',\n",
              " 'recent years supervise learn convolutional network cnns see huge adoption computer vision applications comparatively unsupervised learn cnns receive less attention work hope help bridge gap success cnns supervise learn unsupervised learn introduce class cnns call deep convolutional generative adversarial network dcgans certain architectural constraints demonstrate strong candidate unsupervised learn train various image datasets show convince evidence deep convolutional adversarial pair learn hierarchy representations object part scenes generator discriminator additionally use learn feature novel task demonstrate applicability general image representations',\n",
              " 'recurrent neural network powerful tool understand model computation representation populations neurons continuous variable rate model network analyze apply extensively purpose however neurons fire action potentials discrete nature spike important feature neural circuit dynamics despite significant advance train recurrently connect spike neural network remain challenge present procedure train recurrently connect spike network generate dynamical pattern autonomously produce complex temporal output base integrate network input model physiological data procedure make use continuous variable network identify target train input spike model neurons surprisingly able construct spike network duplicate task perform continuous variable network relatively minor expansion number neurons approach provide novel view significance appropriate use fire rate model useful approach build model spike network use address important question representation computation neural systems',\n",
              " 'decision tree random forest well establish model offer good predictive performance also provide rich feature importance information practitioners often employ variable importance methods rely impurity base information methods remain poorly characterize theoretical perspective provide novel insights performance methods derive finite sample performance guarantee high dimensional set various model assumptions demonstrate effectiveness impurity base methods via extensive set simulations',\n",
              " 'deep reinforcement learn successfully solve many challenge control task real world applicability limit inability ensure safety learn policies propose approach verifiable reinforcement learn train decision tree policies represent complex policies since nonparametric yet efficiently verify use exist techniques since highly structure challenge decision tree policies difficult train propose viper algorithm combine ideas model compression imitation learn learn decision tree policies guide dnn policy call oracle function show substantially outperform two baselines use viper learn provably robust decision tree policy variant atari pong symbolic state space learn decision tree policy toy game base pong provably never lose iii learn provably stable decision tree policy cart pole case decision tree policy achieve performance equal original dnn policy',\n",
              " 'humans routinely retrace path novel environment forward backwards despite uncertainty motion paper present approach give demonstration path first network generate path equip second network observe world decide act order retrace path noisy actuation change environment two network optimize end end train time evaluate method two realistic simulators perform path follow forward backwards experiment show approach outperform classical approach solve task well number baselines',\n",
              " 'paper introduce wasserstein variational inference new form approximate bayesian inference base optimal transport theory wasserstein variational inference use new family divergences include divergences wasserstein distance special case gradients wasserstein variational loss obtain backpropagating sinkhorn iterations technique result stable likelihood free train method use implicit distributions probabilistic program use wasserstein variational inference framework introduce several new form autoencoders test robustness performance exist variational autoencoding techniques',\n",
              " 'infer intent observe behavior study extensively within frameworks bayesian inverse plan inverse reinforcement learn methods infer goal reward function best explain action observe agent typically human demonstrator another agent use infer intent predict imitate assist human user however central assumption inverse reinforcement learn demonstrator close optimal model suboptimal behavior exist typically assume suboptimal action result type random noise know cognitive bias like temporal inconsistency paper take alternative approach model suboptimal behavior result internal model misspecification reason user action might deviate near optimal action user incorrect set beliefs rule dynamics govern action affect environment insight demonstrate action may suboptimal real world may actually near optimal respect user internal model dynamics estimate internal beliefs observe behavior arrive new method infer intent demonstrate simulation user study participants approach enable accurately model human intent use variety applications include offer assistance share autonomy framework infer human preferences',\n",
              " 'give rigorous analysis statistical behavior gradients randomly initialize fully connect network relu activations result show empirical variance square entries input output jacobian exponential simple architecture dependent constant beta give sum reciprocals hide layer widths beta large gradients compute initialization vary wildly approach complement mean field theory analysis random network point view rigorously compute finite width corrections statistics gradients edge chaos',\n",
              " 'humans make repeat choices among options imperfectly know reward outcomes important problem psychology neuroscience often study use multi arm bandits also frequently study machine learn present data human stationary bandit experiment vary average abundance variability reward availability mean variance reward rate distributions surprisingly find subject significantly underestimate prior mean reward rat base self report end game reward expectation non choose arm previously human learn bandit task find well capture bayesian ideal learn model dynamic belief model dbm albeit incorrect generative assumption temporal structure humans assume reward rat change time even though actually fix find pessimism bias bandit task well capture prior mean dbm fit human choices poorly capture prior mean fix belief model fbm alternative bayesian model correctly assume reward rat constants pessimism bias also incompletely capture simple reinforcement learn model commonly use neuroscience psychology term fit initial value seem sub optimal thus mysterious humans underestimate prior reward expectation simulations show underestimate prior mean help maximize long term gain observer assume volatility reward rat stable utilize softmax decision policy instead optimal one obtainable dynamic program raise intrigue possibility brain underestimate reward rat compensate incorrect non stationarity assumption generative model simplify decision policy']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqZgpUdRP5EG"
      },
      "source": [
        "#mydata_embeddings = model.encode(mydata,convert_to_tensor=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5YIRvM-ZaUI"
      },
      "source": [
        "mydata_embeddings = []\n",
        "for i in range(len(mydata)):\n",
        "  for j in range(len(data_list2)):\n",
        "    if mydata[i] == data_list2[j]:\n",
        "      #print(data_list2[j])\n",
        "      mydata_embeddings.append(embeddings_test[j])\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6_z-9h1tt8M",
        "outputId": "092a2932-07d8-4d1e-9766-ff690c09223f"
      },
      "source": [
        "'''\n",
        "This is the code to produce clusters automatically with cosine similarity\n",
        "This is kind of a big code and works really good. I have to work more in order to minimize the Time Complexity\n",
        "But this code represents just 1 forward iteration\n",
        "And so the end clusters are not that good\n",
        "'''\n",
        "\n",
        "list = []\n",
        "topics_fin = []\n",
        "\n",
        "length = len(mydata)\n",
        "int = 0\n",
        "for i in range(len(mydata_embeddings)):\n",
        "  \n",
        "  if len(mydata) > 0:\n",
        "    sublist = []\n",
        "    topics = []\n",
        "    # mydata_embeddings = model.encode(mydata,convert_to_tensor=False)\n",
        "\n",
        "  \n",
        "  \n",
        "    # remove = []\n",
        "    # print('len')\n",
        "    # print(len(mydata))\n",
        "    # print('**')\n",
        "    try:\n",
        "      sublist.append(mydata[i])\n",
        "      topics.append(topic_new_clusters[i])\n",
        "    except:\n",
        "      # print('*********')\n",
        "      print(i)\n",
        "    # mydata.pop(i)\n",
        "    remove = []\n",
        "    if int !=  length:\n",
        "      for j in range(len(mydata_embeddings[1:])):\n",
        "        similarity = cosine_similarity(mydata_embeddings[i].reshape(1,-1),mydata_embeddings[j+1].reshape(1,-1))\n",
        "      \n",
        "        if similarity > 0.5 and similarity < 0.99:\n",
        "          #print(similarity)\n",
        "          sublist.append(mydata[j+1])\n",
        "          topics.append(topic_new_clusters[j+1])\n",
        "          remove.append(j+1)\n",
        "    int+= len(sublist)\n",
        "    list.append(sublist)\n",
        "    topics_fin.append(topics)\n",
        "    remove = Reverse(remove)\n",
        "\n",
        "    for i in remove:\n",
        "      # print(i)\n",
        "      mydata.pop(i)\n",
        "      topic_new_clusters.pop(i)\n",
        "    mydata_embeddings = model.encode(mydata,convert_to_tensor=False)\n",
        "    print(len(mydata))\n",
        "    print(len(topic_new_clusters))\n",
        "    # print('len2')\n",
        "    # print(len(mydata_embeddings))\n",
        "    # print('**')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# sublist = []\n",
        "# mydata_embeddings = model.encode(mydata,convert_to_tensor=False)\n",
        "# for i in range(len(mydata_embeddings)):\n",
        " \n",
        "\n",
        "#   remove = []\n",
        "#   sublist.append(mydata[i])\n",
        "#   remove.append(i)\n",
        "#   for j in range(len(mydata_embeddings[1:])):\n",
        "#     similarity = cosine_similarity(mydata_embeddings[i].reshape(1,-1),mydata_embeddings[j+1].reshape(1,-1))\n",
        "  \n",
        "#     if similarity > 0.5:\n",
        "#       print(similarity)\n",
        "#       sublist.append(mydata[j+1])\n",
        "#       remove.append(j+1)\n",
        "\n",
        "#   list.append(sublist)\n",
        "#   remove = Reverse(remove)\n",
        "\n",
        "#   for i in remove:\n",
        "#     print(i)\n",
        "#     mydata.pop(i)\n",
        "\n",
        "#   break\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "# remove = Reverse(remove)\n",
        "\n",
        "# for i in remove:\n",
        "#   print(i)\n",
        "#   mydata.pop(i)\n",
        "\n",
        "    \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "77\n",
            "77\n",
            "62\n",
            "62\n",
            "60\n",
            "60\n",
            "58\n",
            "58\n",
            "54\n",
            "54\n",
            "38\n",
            "38\n",
            "38\n",
            "38\n",
            "36\n",
            "36\n",
            "36\n",
            "36\n",
            "36\n",
            "36\n",
            "34\n",
            "34\n",
            "34\n",
            "34\n",
            "34\n",
            "34\n",
            "34\n",
            "34\n",
            "34\n",
            "34\n",
            "34\n",
            "34\n",
            "33\n",
            "33\n",
            "31\n",
            "31\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "30\n",
            "31\n",
            "30\n",
            "30\n",
            "32\n",
            "30\n",
            "30\n",
            "33\n",
            "30\n",
            "30\n",
            "34\n",
            "30\n",
            "30\n",
            "35\n",
            "30\n",
            "30\n",
            "36\n",
            "30\n",
            "30\n",
            "37\n",
            "30\n",
            "30\n",
            "38\n",
            "30\n",
            "30\n",
            "39\n",
            "30\n",
            "30\n",
            "40\n",
            "30\n",
            "30\n",
            "41\n",
            "30\n",
            "30\n",
            "42\n",
            "30\n",
            "30\n",
            "43\n",
            "30\n",
            "30\n",
            "44\n",
            "30\n",
            "30\n",
            "45\n",
            "30\n",
            "30\n",
            "46\n",
            "30\n",
            "30\n",
            "47\n",
            "30\n",
            "30\n",
            "48\n",
            "30\n",
            "30\n",
            "49\n",
            "30\n",
            "30\n",
            "50\n",
            "30\n",
            "30\n",
            "51\n",
            "30\n",
            "30\n",
            "52\n",
            "30\n",
            "30\n",
            "53\n",
            "30\n",
            "30\n",
            "54\n",
            "30\n",
            "30\n",
            "55\n",
            "30\n",
            "30\n",
            "56\n",
            "30\n",
            "30\n",
            "57\n",
            "30\n",
            "30\n",
            "58\n",
            "30\n",
            "30\n",
            "59\n",
            "30\n",
            "30\n",
            "60\n",
            "30\n",
            "30\n",
            "61\n",
            "30\n",
            "30\n",
            "62\n",
            "30\n",
            "30\n",
            "63\n",
            "30\n",
            "30\n",
            "64\n",
            "30\n",
            "30\n",
            "65\n",
            "30\n",
            "30\n",
            "66\n",
            "30\n",
            "30\n",
            "67\n",
            "30\n",
            "30\n",
            "68\n",
            "30\n",
            "30\n",
            "69\n",
            "30\n",
            "30\n",
            "70\n",
            "30\n",
            "30\n",
            "71\n",
            "30\n",
            "30\n",
            "72\n",
            "30\n",
            "30\n",
            "73\n",
            "30\n",
            "30\n",
            "74\n",
            "30\n",
            "30\n",
            "75\n",
            "30\n",
            "30\n",
            "76\n",
            "30\n",
            "30\n",
            "77\n",
            "30\n",
            "30\n",
            "78\n",
            "30\n",
            "30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOdtoBI9jMpp",
        "outputId": "9524df3b-d27f-48c5-dd40-69d9091a7ae2"
      },
      "source": [
        "'''\n",
        "Now this code uses the above list and performs 1 more reverse iteration so which means the end clusters produced from the above list are now much better\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "list2 = []\n",
        "sim = []\n",
        "empty_list = []\n",
        "for i in Reverse(list):\n",
        "  if len(i) > 0:\n",
        "    list2.append(i)\n",
        "list2_a = []\n",
        "for i in list:\n",
        "  if len(i) > 0:\n",
        "    list2_a.append(i)\n",
        "count = 0\n",
        "list_made = []\n",
        "for i in range(len(list2)):\n",
        "  for j in range(len(list2[i])):\n",
        "    print(list2[i][j])\n",
        "    j2 = model.encode(list2[i][j], convert_to_tensor=False)\n",
        "    sim = []\n",
        "    embedd = []\n",
        "    #instead of giving sentences as k, give embeddings as k to save a lot of time\n",
        "    for k in range(len(list2_a)):\n",
        "      #print(list2_a[k])\n",
        "      embedd = []\n",
        "      for m in range(len(list2_a[k])):\n",
        "        for l in range(len(data_list2)):\n",
        "          if list2_a[k][m] == data_list2[l]:\n",
        "            embedd.append(embeddings_test[l])\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "      summ = []\n",
        "      for z in embedd:\n",
        "      #k2 = model.encode(list2_a[k], convert_to_tensor=False)\n",
        "        similarity = cosine_similarity(j2.reshape(1,-1),z.reshape(1,-1))\n",
        "        summ.append(similarity)\n",
        "      sim.append(np.sum(summ)/len(list2_a[k]))\n",
        "    \n",
        "    maxi = max(sim)\n",
        "    indexe = sim.index(maxi)\n",
        "    sim.pop(indexe)\n",
        "    maxi = max(sim)\n",
        "    print(maxi)\n",
        "    indexe = sim.index(maxi)\n",
        "    if maxi > 0.47 and maxi < 0.98:\n",
        "    \n",
        "      #print(maxi)\n",
        "      empty_list.append(list2[i][j])\n",
        "      print(indexe)\n",
        "      for ii in list2_a[indexe]:\n",
        "        empty_list.append(ii)\n",
        "      \n",
        "      list2_a[indexe] = empty_list\n",
        "      count+=1\n",
        "      list2_a[-(i+1)][j] = 'nan'\n",
        "      #print('.......')\n",
        "      #print(empty_list)\n",
        "      #print(list2_a[indexe])\n",
        "      empty_list = []\n",
        "      sim = []\n",
        "  \n",
        "\n",
        "\n",
        "    \n",
        "  \n",
        "      \n",
        "\n",
        "      # for l in k:\n",
        "      #   print(l)\n",
        "      #   l2 = model.encode(l, convert_to_tensor=False)\n",
        "      #   similarity = cosine_similarity(j2.reshape(1,-1),l2.reshape(1,-1))\n",
        "      #   print(similarity)\n",
        "    \n",
        "        \n",
        "      \n",
        "  \n",
        "        \n",
        "      \n",
        " \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "infer intent observe behavior study extensively within frameworks bayesian inverse plan inverse reinforcement learn methods infer goal reward function best explain action observe agent typically human demonstrator another agent use infer intent predict imitate assist human user however central assumption inverse reinforcement learn demonstrator close optimal model suboptimal behavior exist typically assume suboptimal action result type random noise know cognitive bias like temporal inconsistency paper take alternative approach model suboptimal behavior result internal model misspecification reason user action might deviate near optimal action user incorrect set beliefs rule dynamics govern action affect environment insight demonstrate action may suboptimal real world may actually near optimal respect user internal model dynamics estimate internal beliefs observe behavior arrive new method infer intent demonstrate simulation user study participants approach enable accurately model human intent use variety applications include offer assistance share autonomy framework infer human preferences\n",
            "0.4949619770050049\n",
            "4\n",
            "paper introduce wasserstein variational inference new form approximate bayesian inference base optimal transport theory wasserstein variational inference use new family divergences include divergences wasserstein distance special case gradients wasserstein variational loss obtain backpropagating sinkhorn iterations technique result stable likelihood free train method use implicit distributions probabilistic program use wasserstein variational inference framework introduce several new form autoencoders test robustness performance exist variational autoencoding techniques\n",
            "0.47101283073425293\n",
            "2\n",
            "decision tree random forest well establish model offer good predictive performance also provide rich feature importance information practitioners often employ variable importance methods rely impurity base information methods remain poorly characterize theoretical perspective provide novel insights performance methods derive finite sample performance guarantee high dimensional set various model assumptions demonstrate effectiveness impurity base methods via extensive set simulations\n",
            "0.49589115381240845\n",
            "19\n",
            "real world applications education effective teacher adaptively choose next example teach base learner current state however exist work algorithmic machine teach focus batch set adaptivity play role paper study case teach consistent version space learners interactive set time step teacher provide example learner perform update teacher observe learner new state highlight adaptivity speed teach process consider exist model version space learners worst case model learner pick next hypothesis randomly version space preference base model learner pick hypothesis accord global preference inspire human teach propose new model learner pick hypotheses accord local preference define current hypothesis show model exhibit several desirable properties adaptivity play key role learner transition hypotheses smooth interpretable develop adaptive teach algorithms demonstrate result via simulation user study\n",
            "0.4572715163230896\n",
            "softmax output activation function model categorical probability distributions many applications deep learn however recent study reveal softmax bottleneck representational capacity neural network language model softmax bottleneck paper propose output activation function break softmax bottleneck without additional parameters analyze softmax bottleneck perspective output set log softmax identify cause softmax bottleneck basis analysis propose sigsoftmax compose multiplication exponential function sigmoid function sigsoftmax break softmax bottleneck experiment language model demonstrate sigsoftmax mixture sigsoftmax outperform softmax mixture softmax respectively\n",
            "0.4386623700459798\n",
            "introduce approach convert mono audio record video camera spatial audio representation distribution sound full view sphere spatial audio important component immersive video view spatial audio microphones still rare current video production system consist end end trainable neural network separate individual sound source localize view sphere condition multi modal analysis audio video frame introduce several datasets include one film one collect wild youtube consist videos upload spatial audio train grind truth spatial audio serve self supervision mix mono track form input network use approach show possible infer spatial localization sound base synchronize video mono audio track\n",
            "0.4530712068080902\n",
            "people belong multiple communities word belong multiple topics book cover multiple genres overlap cluster commonplace many exist overlap cluster methods model person word book non negative weight combination exemplars belong solely one community small noise geometrically person point cone whose corner exemplars basic form encompass widely use mix membership stochastic blockmodel network degree correct variants well topic model lda show simple one class svm yield provably consistent parameter inference model scale large datasets experimental result several simulate real datasets show algorithm call svm cone accurate scalable\n",
            "0.4549766182899475\n",
            "modern visual question answer vqa model show rely heavily superficial correlations question answer word learn train overwhelmingly report type room kitchen sport play tennis irrespective image alarmingly shortcoming often well reflect evaluation strong priors exist test distributions however vqa system fail grind question image content would likely perform poorly real world settings\n",
            "0.4927148222923279\n",
            "21\n",
            "accurately answer question give image require combine observations general knowledge effortless humans reason general knowledge remain algorithmic challenge advance research direction novel reason correct answer jointly consider entities show challenge fvqa dataset lead improvement accuracy around compare state art\n",
            "0.494934618473053\n",
            "11\n",
            "propose novel flexible anchor mechanism name metaanchor object detection frameworks unlike many previous detectors model anchor via predefined manner metaanchor anchor function could dynamically generate arbitrary customize prior box take advantage weight prediction metaanchor able work anchor base object detection systems retinanet compare predefined anchor scheme empirically find metaanchor robust anchor settings bound box distributions addition also show potential transfer task experiment coco detection task show metaanchor consistently outperform counterparts various scenarios\n",
            "0.45275264978408813\n",
            "several large scale deployments differential privacy use collect statistical information users however deployments periodically recollect data recompute statistics use algorithms design single use result systems provide meaningful privacy guarantee long time scale moreover exist techniques mitigate effect apply local model differential privacy systems use\n",
            "0.4165685772895813\n",
            "consider problem learn optimal reserve price repeat auction non myopic bidders may bid strategically order gain future round even single round auction truthful previous algorithms empirical price provide non trivial regret round set general introduce algorithms obtain small regret non myopic bidders either market large bidder appear constant fraction round bidders impatient discount future utility factor mildly bound away one approach carefully control information reveal bidder build techniques differentially private online learn well recent line work jointly differentially private algorithms\n",
            "0.4618893265724182\n",
            "duplicate removal critical step accomplish reasonable amount predictions prevalent proposal base object detection frameworks albeit simple effective previous algorithms utilize greedy process without make sufficient use properties input data work design new two stage framework effectively select appropriate proposal candidate object first stage suppress easy negative object proposals second stage select true positives reduce proposal set two stag share network structure encoder decoder form recurrent neural network rnn global attention context gate encoder scan proposal candidates sequential manner capture global context information feed decoder extract optimal proposals extensive experiment propose method outperform alternatives large margin\n",
            "0.44585688908894855\n",
            "large scale network model use neurons static nonlinearities produce analog output despite fact information process brain predominantly carry dynamic neurons produce discrete pulse call spike research spike base computation impede lack efficient supervise learn algorithm spike neural network present gradient descent method optimize spike network model introduce differentiable formulation spike dynamics derive exact gradient calculation demonstration train recurrent spike network two dynamic task one require optimize fast millisecond spike base interactions efficient encode information delay memory task extend duration second result show gradient descent approach indeed optimize network dynamics time scale individual spike well behavioral time scale conclusion method yield general purpose supervise learn algorithm spike neural network facilitate investigations spike base computations\n",
            "0.4964916408061981\n",
            "1\n",
            "suppose design matrix linear regression problem give response point hide unless explicitly request goal sample small number responses produce weight vector whose sum square loss point epsilon time minimum small jointly sample diverse subsets point crucial one method call volume sample unique desirable property weight vector produce unbiased estimate optimum therefore natural ask method offer optimal unbiased estimate term number responses need achieve epsilon loss approximation\n",
            "0.4749676585197449\n",
            "18\n",
            "recurrent network spike neurons rsnns underlie astound compute learn capabilities brain compute learn capabilities rsnn model remain poor least comparison anns address two possible reason one rsnns brain randomly connect design accord simple rule start learn tabula rasa network rather rsnns brain optimize task evolution development prior experience detail optimization process largely unknown functional contribution approximate powerful optimization methods backpropagation time bptt\n",
            "0.4474276900291443\n",
            "technical challenge deep learn recognize target class without see data zero shoot learn leverage semantic representations attribute class prototypes bridge source target class exist standard zero shoot learn methods may prone overfitting see data source class blind semantic representations target class paper study generalize zero shoot learn assume accessible target class unseen data train prediction unseen data make search source target class propose novel deep calibration network dcn approach towards generalize zero shoot learn paradigm enable simultaneous calibration deep network confidence source class uncertainty target class approach map visual feature image semantic representations class prototypes common embed space compatibility see data source target class maximize show superior accuracy approach state art benchmark datasets generalize zero shoot learn include awa cub sun apy\n",
            "0.4640449285507202\n",
            "paper propose conceptually simple general framework call metagan shoot learn problems state art shoot classification model integrate metagan principled straightforward way introduce adversarial generator condition task augment vanilla shoot classification model ability discriminate real fake data argue gin base approach help shoot classifiers learn sharper decision boundary could generalize better show metagan framework extend supervise shoot learn model naturally cope unsupervised data different previous work semi supervise shoot learn algorithms deal semi supervision sample level task level give theoretical justifications strength metagan validate effectiveness metagan challenge shoot image classification benchmarks\n",
            "0.4497167468070984\n",
            "draw attention important yet largely overlook aspect evaluate fairness automate decision make systems namely risk welfare considerations propose family measure correspond long establish formulations cardinal social welfare economics justify rawlsian conception fairness behind veil ignorance convex formulation welfare base measure fairness allow integrate constraint convex loss minimization pipeline empirical analysis reveal interest trade off proposal prediction accuracy group discrimination dwork notion individual fairness furthermore perhaps importantly work provide heuristic justification empirical evidence suggest lower bind measure often lead bound inequality algorithmic outcomes hence present first computationally feasible mechanism bound individual level inequality\n",
            "0.4478679895401001\n",
            "societies often rely human experts take wide variety decisions affect members jail release decisions take judge stop frisk decisions take police officer accept reject decisions take academics context decision take expert typically choose uniformly random pool experts however decisions may imperfect due limit experience implicit bias faulty probabilistic reason improve accuracy fairness overall decision make process optimize assignment experts decisions\n",
            "0.3965900739034017\n",
            "paper compare different type recurrent units recurrent neural network rnns especially focus sophisticate units implement gate mechanism long short term memory lstm unit recently propose gate recurrent unit gru evaluate recurrent units task polyphonic music model speech signal model experiment reveal advance recurrent units indeed better traditional recurrent units tanh units also find gru comparable lstm\n",
            "0.4579443136850993\n",
            "hash one popular powerful approximate nearest neighbor search techniques large scale image retrieval traditional hash methods first represent image shelf visual feature produce hash cod separate stage however shelf visual feature may optimally compatible hash code learn procedure may result sub optimal hash cod recently deep hash methods propose simultaneously learn image feature hash cod use deep neural network show superior performance traditional hash methods deep hash methods give supervise information form pairwise label triplet label current state art deep hash method dpsh cite feature base pairwise label perform image feature learn hash code learn simultaneously maximize likelihood pairwise similarities inspire dpsh cite feature propose triplet label base deep hash method aim maximize likelihood give triplet label experimental result show method outperform baselines cifar nus wide datasets include state art method dpsh cite feature previous triplet label base deep hash methods\n",
            "0.43557870388031006\n",
            "reason play essential role visual question answer vqa multi step dynamic reason often necessary answer complex question example question place next bus right picture talk compound object bus right generate relation\n",
            "0.3810562193393707\n",
            "attention network multimodal learn provide efficient way utilize give visual information selectively however computational cost learn attention distributions every pair multimodal input channel prohibitively expensive solve problem attention build two separate attention distributions modality neglect interaction multimodal input paper propose bilinear attention network ban find bilinear attention distributions utilize give vision language information seamlessly ban consider bilinear interactions among two group input channel low rank bilinear pool extract joint representations pair channel furthermore propose variant multimodal residual network exploit eight attention map ban efficiently quantitatively qualitatively evaluate model visual question answer vqa flickr entities datasets show ban significantly outperform previous methods achieve new state arts datasets\n",
            "0.482147216796875\n",
            "3\n",
            "bilinear model show achieve impressive performance wide range visual task semantic segmentation fine grain recognition face recognition however bilinear feature high dimensional typically order hundreds thousands million make impractical subsequent analysis propose two compact bilinear representations discriminative power full bilinear representation thousand dimension compact representations allow back propagation classification errors enable end end optimization visual recognition system compact bilinear representations derive novel kernelized analysis bilinear pool provide insights discriminative power bilinear pool platform research compact pool methods experimentation illustrate utility propose representations image classification shoot learn across several datasets\n",
            "0.4683794677257538\n",
            "unsupervised learn generative adversarial network gans prove hugely successful regular gans hypothesize discriminator classifier sigmoid cross entropy loss function however find loss function may lead vanish gradients problem learn process overcome problem propose paper least square generative adversarial network lsgans adopt least square loss function discriminator show minimize objective function lsgan yield minimize pearson chi divergence two benefit lsgans regular gans first lsgans able generate higher quality image regular gans second lsgans perform stable learn process evaluate lsgans five scene datasets experimental result show image generate lsgans better quality ones generate regular gans also conduct two comparison experiment lsgans regular gans illustrate stability lsgans\n",
            "0.48334357142448425\n",
            "3\n",
            "goal orient dialog give attention due numerous applications artificial intelligence goal orient dialogue task occur questioner ask action orient question answerer respond intent let questioner know correct action take ask adequate question deep learn reinforcement learn recently apply however approach struggle find competent recurrent neural questioner owe complexity learn series sentence motivate theory mind propose answerer questioner mind aqm novel information theoretic algorithm goal orient dialog aqm questioner ask infer base approximate probabilistic model answerer questioner figure answerer intention via select plausible question explicitly calculate information gain candidate intentions possible answer question test framework two goal orient visual dialog task mnist count dialog guesswhat experiment aqm outperform comparative algorithms large margin\n",
            "0.44036799669265747\n",
            "paper try organize machine teach coherent set ideas idea present vary along dimension collection dimension form problem space machine teach exist teach problems characterize space hope organization allow gain deeper understand individual teach problems discover connections among identify gap field\n",
            "0.43174853920936584\n",
            "learn decision tree data difficult optimization problem widespread algorithm practice date base greedy growth tree structure recursively split nod possibly prune back final tree parameters decision function internal node approximately estimate minimize impurity measure give algorithm give input tree structure parameter value nod produce new tree smaller structure new parameter value provably lower leave unchanged misclassification error apply axis align oblique tree experiment show consistently outperform various algorithms highly scalable large datasets tree algorithm handle sparsity penalty learn sparse oblique tree structure subset original tree nonzero parameters combine best axis align oblique tree flexibility model correlate data low generalization error fast inference interpretable nod involve feature decision\n",
            "0.4872454106807709\n",
            "13\n",
            "ensembles randomize decision tree usually refer random forest widely use classification regression task machine learn statistics random forest achieve competitive predictive performance computationally efficient train test make excellent candidates real world prediction task popular random forest variants breiman random forest extremely randomize tree operate batch train data online methods greater demand exist online random forest however require train data batch counterpart achieve comparable predictive performance work use mondrian process roy teh construct ensembles random decision tree call mondrian forest mondrian forest grow incremental online fashion remarkably distribution online mondrian forest batch mondrian forest mondrian forest achieve competitive predictive performance comparable exist online random forest periodically train batch random forest order magnitude faster thus represent better computation accuracy tradeoff\n",
            "0.4713968833287557\n",
            "7\n",
            "deep reinforcement learn successfully solve many challenge control task real world applicability limit inability ensure safety learn policies propose approach verifiable reinforcement learn train decision tree policies represent complex policies since nonparametric yet efficiently verify use exist techniques since highly structure challenge decision tree policies difficult train propose viper algorithm combine ideas model compression imitation learn learn decision tree policies guide dnn policy call oracle function show substantially outperform two baselines use viper learn provably robust decision tree policy variant atari pong symbolic state space learn decision tree policy toy game base pong provably never lose iii learn provably stable decision tree policy cart pole case decision tree policy achieve performance equal original dnn policy\n",
            "0.4909658432006836\n",
            "4\n",
            "machine learn model vulnerable adversarial examples small change image cause computer vision model make mistake identify school bus ostrich however still open question whether humans prone similar mistake address question leverage recent techniques transfer adversarial examples computer vision model know parameters architecture model unknown parameters architecture match initial process human visual system find adversarial examples strongly transfer across computer vision model influence classifications make time limit human observers\n",
            "0.457271546125412\n",
            "present first accelerate randomize algorithm solve linear systems euclidean space one essential problem type matrix inversion problem particular algorithm specialize invert positive definite matrices way iterate approximate solutions generate algorithm positive definite matrices open way many applications field optimization machine learn application general theory develop first accelerate deterministic stochastic quasi newton update update lead provably aggressive approximations inverse hessian lead speed up classical non accelerate rule numerical experiment experiment empirical risk minimization show rule accelerate train machine learn model\n",
            "0.5173273881276449\n",
            "0\n",
            "stochastic gradient descent popular large scale optimization slow convergence asymptotically due inherent variance remedy problem introduce explicit variance reduction method stochastic gradient descent call stochastic variance reduce gradient svrg smooth strongly convex function prove method enjoy fast convergence rate stochastic dual coordinate ascent sdca stochastic average gradient sag however analysis significantly simpler intuitive moreover unlike sdca sag method require storage gradients thus easily applicable complex problems structure prediction problems neural network learn\n",
            "0.5091516831341911\n",
            "1\n",
            "algorithmically construct multi output gaussian process priors satisfy linear differential equations approach attempt parametrize solutions equations use bner base successful push forward gaussian process along paramerization desire prior consider several examples physics geomathmatics control among full inhomogeneous system maxwell equations bring together stochastic learn computeralgebra novel way combine noisy observations precise algebraic computations\n",
            "0.44731871287027997\n",
            "policy gradient methods widely use control reinforcement learn particularly continuous action set host theoretically sound algorithms propose policy set due existence policy gradient theorem provide simplify form gradient policy learn however behaviour policy necessarily attempt learn follow optimal policy give task existence theorem elusive work solve open problem provide first policy policy gradient theorem key derivation use emphatic weight develop new actor critic algorithm call actor critic emphatic weight ace approximate simplify gradients provide theorem demonstrate simple counterexample previous policy policy gradient methods particularly offpac dpg converge wrong solution whereas ace find optimal solution\n",
            "0.47088310453626847\n",
            "1\n",
            "present novel newton type method distribute optimization particularly well suit stochastic optimization learn problems quadratic objectives method enjoy linear rate convergence provably emph improve data size require essentially constant number iterations reasonable assumptions provide theoretical empirical evidence advantage method compare approach one shoot parameter average admm\n",
            "0.5189998149871826\n",
            "2\n",
            "state art object detection network depend region proposal algorithms hypothesize object locations advance like sppnet fast cnn reduce run time detection network expose region proposal computation bottleneck work introduce region proposal network rpn share full image convolutional feature detection network thus enable nearly cost free region proposals rpn fully convolutional network simultaneously predict object bound objectness score position rpn train end end generate high quality region proposals use fast cnn detection merge rpn fast cnn single network share convolutional feature use recently popular terminology neural network attention mechanisms rpn component tell unify network look deep vgg model detection system frame rate fps include step gpu achieve state art object detection accuracy pascal voc coco datasets proposals per image ilsvrc coco competitions faster cnn rpn foundations place win entries several track code make publicly available\n",
            "0.5017024592349404\n",
            "1\n",
            "propose simple yet effective approach spatiotemporal feature learn use deep dimensional convolutional network convnets train large scale supervise video dataset find three fold convnets suitable spatiotemporal feature learn compare convnets homogeneous architecture small convolution kernels layer among best perform architectures convnets learn feature namely convolutional simple linear classifier outperform state art methods different benchmarks comparable current best methods benchmarks addition feature compact achieve accuracy ucf dataset dimension also efficient compute due fast inference convnets finally conceptually simple easy train use\n",
            "0.49579545855522156\n",
            "25\n",
            "learn solve complex sequence task leverage transfer avoid catastrophic forget remain key obstacle achieve human level intelligence progressive network approach represent step forward direction immune forget leverage prior knowledge via lateral connections previously learn feature evaluate architecture extensively wide variety reinforcement learn task atari maze game show outperform common baselines base pretraining finetuning use novel sensitivity measure demonstrate transfer occur low level sensory high level control layer learn policy\n",
            "0.48421525955200195\n",
            "1\n",
            "propose new family policy gradient methods reinforcement learn alternate sample data interaction environment optimize surrogate objective function use stochastic gradient ascent whereas standard policy gradient methods perform one gradient update per data sample propose novel objective function enable multiple epochs minibatch update new methods call proximal policy optimization ppo benefit trust region policy optimization trpo much simpler implement general better sample complexity empirically experiment test ppo collection benchmark task include simulate robotic locomotion atari game play show ppo outperform online policy gradient methods overall strike favorable balance sample complexity simplicity wall time\n",
            "0.4872491019112723\n",
            "3\n",
            "generative recurrent neural network quickly train unsupervised manner model popular reinforcement learn environments compress spatio temporal representations world model extract feature feed compact simple policies train evolution achieve state art result various environments also train agent entirely inside environment generate internal world model transfer policy back actual environment interactive version paper available https worldmodels github\n",
            "0.4769965708255768\n",
            "24\n",
            "note consider normalize gradient descent ngd natural modification classical gradient descent optimization problems serious shortcoming non convex problems may take arbitrarily long escape neighborhood saddle point issue make convergence arbitrarily slow particularly high dimensional non convex problems relative number saddle point often large paper focus continuous time descent show contrary standard ngd escape saddle point quickly particular show ngd almost never converge saddle point time require ngd escape ball radius saddle point sqrt kappa kappa condition number hessian application result global convergence time bind establish ngd mild assumptions\n",
            "0.46631315776279997\n",
            "paper propose stochastic recursive gradient algorithm sarah well practical variant sarah novel approach finite sum minimization problems different vanilla sgd modern stochastic methods svrg sag saga sarah admit simple recursive framework update stochastic gradient estimate compare sag saga sarah require storage past gradients linear convergence rate sarah prove strong convexity assumption also prove linear convergence rate strongly convex case inner loop sarah property svrg possess numerical experiment demonstrate efficiency algorithm\n",
            "0.4947308131626674\n",
            "1\n",
            "present shapenet richly annotate large scale repository shape represent cad model object shapenet contain model multitude semantic categories organize wordnet taxonomy collection datasets provide many semantic annotations model consistent rigid alignments part bilateral symmetry plan physical size keywords well plan annotations annotations make available public web base interface enable data visualization object attribute promote data drive geometric analysis provide large scale quantitative benchmark research computer graphics vision time technical report shapenet index model model classify categories wordnet synsets report describe shapenet effort whole provide detail currently available datasets summarize future plan\n",
            "0.44852763414382935\n",
            "significant interest able predict crimes happen example aid efficient task police protective measure aim model temporal spatial dependencies often exhibit violent crimes order make predictions temporal variation crimes typically follow pattern familiar time series analysis spatial pattern irregular vary smoothly across area instead find spatially disjoint regions exhibit correlate crime pattern indeterminate inter region correlation structure along low count discrete nature count serious crimes motivate propose forecast tool particular propose model crime count region use integer value first order autoregressive process take bayesian nonparametric approach flexibly discover cluster region specific time series describe account covariates within framework approach adjust seasonality demonstrate approach analysis weekly report violent crimes washington forecast outperform standard methods additionally provide useful tool prediction intervals\n",
            "0.5037250518798828\n",
            "19\n",
            "wide adoption dnns give birth unrelenting compute requirements force datacenter operators adopt domain specific accelerators train accelerators typically employ densely pack full precision float point arithmetic maximize performance per area ongoing research efforts seek increase performance density replace float point fix point arithmetic however significant roadblock attempt fix point narrow dynamic range insufficient dnn train convergence identify block float point bfp promise alternative representation since exhibit wide dynamic range enable majority dnn operations perform fix point logic unfortunately bfp alone introduce several limitations preclude direct applicability work introduce hbfp hybrid bfp approach perform dot products bfp operations float point hbfp deliver best worlds high accuracy float point superior hardware density fix point wide variety model show hbfp match float point accuracy enable hardware implementations deliver higher throughput\n",
            "0.4663095474243164\n",
            "describe iterative procedure optimize policies guarantee monotonic improvement make several approximations theoretically justify procedure develop practical algorithm call trust region policy optimization trpo algorithm similar natural policy gradient methods effective optimize large nonlinear policies neural network experiment demonstrate robust performance wide variety task learn simulate robotic swim hop walk gaits play atari game use image screen input despite approximations deviate theory trpo tend give monotonic improvement little tune hyperparameters\n",
            "0.49617306391398114\n",
            "3\n",
            "give rigorous analysis statistical behavior gradients randomly initialize fully connect network relu activations result show empirical variance square entries input output jacobian exponential simple architecture dependent constant beta give sum reciprocals hide layer widths beta large gradients compute initialization vary wildly approach complement mean field theory analysis random network point view rigorously compute finite width corrections statistics gradients edge chaos\n",
            "0.4902052879333496\n",
            "0\n",
            "bandit learn characterize tension long term exploration short term exploitation however recently note settings choices learn algorithm correspond important decisions individual people criminal recidivism prediction lend sequential drug trials exploration correspond explicitly sacrifice well one individual potential future benefit others settings one might like run greedy algorithm always make optimal decision individuals hand result catastrophic failure learn paper consider linear contextual bandit problem revisit performance greedy algorithm\n",
            "0.4592709541320801\n",
            "contextual bandit literature traditionally focus algorithms address exploration exploitation tradeoff particular greedy algorithms exploit current estimate without exploration may sub optimal general however exploration free greedy algorithms desirable practical settings exploration may costly unethical clinical trials surprisingly find simple greedy algorithm rate optimal achieve asymptotically optimal regret sufficient randomness observe contexts covariates prove always case two arm bandit general class context distributions satisfy condition term covariate diversity furthermore even absent condition show greedy algorithm rate optimal positive probability thus standard bandit algorithms may unnecessarily explore motivate result introduce greedy first new algorithm use observe contexts reward determine whether follow greedy algorithm explore prove algorithm rate optimal without additional assumptions context distribution number arm extensive simulations demonstrate greedy first successfully reduce exploration outperform exist exploration base contextual bandit algorithms thompson sample upper confidence bind ucb\n",
            "0.5263036489486694\n",
            "14\n",
            "consider problem online learn linear contextual bandits set also strong individual fairness constraints govern unknown similarity metric constraints demand select similar action individuals approximately equal probability dhprz may odds optimize reward thus model settings profit social policy tension assume learn unknown mahalanobis similarity metric weak feedback identify fairness violations quantify extent intend represent interventions regulator know unfairness see nevertheless enunciate quantitative fairness metric individuals main result algorithm adversarial context set number fairness violations depend logarithmically obtain optimal sqrt regret bind best fair policy\n",
            "0.5092781271253314\n",
            "4\n",
            "humans routinely retrace path novel environment forward backwards despite uncertainty motion paper present approach give demonstration path first network generate path equip second network observe world decide act order retrace path noisy actuation change environment two network optimize end end train time evaluate method two realistic simulators perform path follow forward backwards experiment show approach outperform classical approach solve task well number baselines\n",
            "0.47520530223846436\n",
            "24\n",
            "humans make repeat choices among options imperfectly know reward outcomes important problem psychology neuroscience often study use multi arm bandits also frequently study machine learn present data human stationary bandit experiment vary average abundance variability reward availability mean variance reward rate distributions surprisingly find subject significantly underestimate prior mean reward rat base self report end game reward expectation non choose arm previously human learn bandit task find well capture bayesian ideal learn model dynamic belief model dbm albeit incorrect generative assumption temporal structure humans assume reward rat change time even though actually fix find pessimism bias bandit task well capture prior mean dbm fit human choices poorly capture prior mean fix belief model fbm alternative bayesian model correctly assume reward rat constants pessimism bias also incompletely capture simple reinforcement learn model commonly use neuroscience psychology term fit initial value seem sub optimal thus mysterious humans underestimate prior reward expectation simulations show underestimate prior mean help maximize long term gain observer assume volatility reward rat stable utilize softmax decision policy instead optimal one obtainable dynamic program raise intrigue possibility brain underestimate reward rat compensate incorrect non stationarity assumption generative model simplify decision policy\n",
            "0.4430046081542969\n",
            "recent work suggest enhance bloom filter use pre filter base apply machine learn determine function model data set bloom filter mean represent model learn bloom filter follow outcomes clarify guarantee associate structure show estimate size learn function must obtain order obtain improve performance provide simple method sandwich optimize learn bloom filter propose design analysis approach learn bloomier filter base model approach\n",
            "0.42423248291015625\n",
            "stochastic gradient descent sgd popular algorithm achieve state art performance variety machine learn task several researchers recently propose scheme parallelize sgd require performance destroy memory lock synchronization work aim show use novel theoretical analysis algorithms implementation sgd implement without lock present update scheme call hogwild allow processors access share memory possibility overwrite work show associate optimization problem sparse mean gradient update modify small part decision variable hogwild achieve nearly optimal rate convergence demonstrate experimentally hogwild outperform alternative scheme use lock order magnitude\n",
            "0.5227523717013273\n",
            "1\n",
            "paper introduce versatile filter construct efficient convolutional neural network consider demand efficient deep learn techniques run cost effective hardware number methods develop learn compact neural network work aim slim filter different ways investigate small sparse binarized filter contrast treat filter additive perspective series secondary filter derive primary filter secondary filter inherit primary filter without occupy storage unfold computation could significantly enhance capability filter integrate information extract different receptive field besides spatial versatile filter additionally investigate versatile filter channel perspective new techniques general upgrade filter exist cnns experimental result benchmark datasets neural network demonstrate cnns construct versatile filter able achieve comparable accuracy original filter require less memory flop\n",
            "0.5189304351806641\n",
            "24\n",
            "introduce game theoretic approach study recommendation systems strategic content providers systems fair stable show traditional approach fail satisfy requirements propose shapley mediator show shapley mediator satisfy fairness stability requirements run linear time economically efficient mechanism satisfy properties\n",
            "0.45518428087234497\n",
            "propose structure adaptive variant state art stochastic variance reduce gradient algorithm katyusha regularize empirical risk minimization propose method able exploit intrinsic low dimensional structure solution sparsity low rank enforce non smooth regularization achieve even faster convergence rate provable algorithmic improvement do restart katyusha algorithm accord restrict strong convexity constants demonstrate effectiveness approach via numerical experiment\n",
            "0.5201859951019288\n",
            "0\n",
            "introduce new approach decomposable submodular function minimization dsfm exploit incidence relations incidence relations describe variables effectively influence component function properly utilize allow improve convergence rat dsfm solvers main result include precise parametrization dsfm problem base incidence relations development new scalable alternative projections parallel coordinate descent methods accompany rigorous analysis convergence rat\n",
            "0.4943935473759969\n",
            "0\n",
            "aim obtain interpretable expressive disentangle scene representation contain comprehensive structural textural information object previous scene representations learn neural network often uninterpretable limit single object lack knowledge work propose scene render network sdn address issue integrate disentangle representations semantics geometry appearance deep generative model scene encoder perform inverse graphics translate scene structure object wise representation decoder two components differentiable shape renderer neural texture generator disentanglement semantics geometry appearance support aware scene manipulation rotate move object freely keep consistent shape texture change object appearance without affect shape experiment demonstrate edit scheme base sdn superior counterpart\n",
            "0.49269968271255493\n",
            "11\n",
            "ideas enjoy large impact deep learn convolution problem involve pixels spatial representations common intuition hold convolutional neural network may appropriate paper show strike counterexample intuition via seemingly trivial coordinate transform problem simply require learn map coordinate cartesian space coordinate one hot pixel space although convolutional network would seem appropriate task show fail spectacularly demonstrate carefully analyze failure first toy problem point simple fix become obvious call solution coordconv work give convolution access input coordinate use extra coordinate channel without sacrifice computational parametric efficiency ordinary convolution coordconv allow network learn either complete translation invariance vary degrees translation dependence require end task coordconv solve coordinate transform problem perfect generalization time faster time fewer parameters convolution stark contrast raise question extent inability convolution persist insidiously inside task subtly hamper performance within complete answer question require investigation show preliminary evidence swap convolution coordconv improve model diverse set task use coordconv gin produce less mode collapse transform high level spatial latents pixels become easier learn faster cnn detection model train mnist detection show better iou use coordconv reinforcement learn domain agents play atari game benefit significantly use coordconv layer\n",
            "0.5180183478764125\n",
            "0\n",
            "adapt ideas underlie success deep learn continuous action domain present actor critic model free algorithm base deterministic policy gradient operate continuous action space use learn algorithm network architecture hyper parameters algorithm robustly solve simulate physics task include classic problems cartpole swing dexterous manipulation legged locomotion car drive algorithm able find policies whose performance competitive find plan algorithm full access dynamics domain derivatives demonstrate many task algorithm learn policies end end directly raw pixel input\n",
            "0.48061010111933167\n",
            "0\n",
            "accurate exposure key capture high quality photos computational photography especially mobile phone limit size camera modules inspire luminosity mask usually apply professional photographers paper develop novel algorithm learn local exposures deep reinforcement adversarial learn specific segment image sub image reflect variations dynamic range exposures accord raw low level feature base sub image local exposure sub image automatically learn virtue policy network sequentially reward learn globally design strike balance overall exposures aesthetic evaluation function approximate discriminator generative adversarial network reinforcement learn adversarial learn train collaboratively asynchronous deterministic policy gradient generative loss approximation simply algorithmic architecture also prove feasibility leverage discriminator value function employ local exposure retouch raw input image respectively thus deliver multiple retouch image different exposures fuse exposure blend extensive experiment verify algorithms superior state art methods term quantitative accuracy visual illustration\n",
            "0.4842236042022705\n",
            "23\n",
            "typical way network data record measure interactions involve specify set core nod produce graph contain core together potentially larger set fringe nod link core interactions nod fringe however present result graph data example phone service provider may record call least one participants customer include call customer non customer pair non customers knowledge nod belong core crucial interpret dataset metadata unavailable many case either lose due difficulties data provenance network consist find data obtain settings counter surveillance lead algorithmic problem recover core set since core vertex cover essentially plant vertex cover problem arbitrary underlie graph develop framework analyze plant vertex cover problem base theory fix parameter tractability together algorithms recover core algorithms fast simple implement perform several baselines base core periphery structure various real world datasets\n",
            "0.45604387919108075\n",
            "hypergraph partition important problem machine learn computer vision network analytics widely use method hypergraph partition rely minimize normalize sum cost partition hyperedges across cluster algorithmic solutions base approach assume different partition hyperedge incur cost however assumption fail leverage fact different subsets vertices within hyperedge may different structural importance hence propose new hypergraph cluster technique term inhomogeneous hypergraph partition assign different cost different hyperedge cut prove inhomogeneous partition produce quadratic approximation optimal solution inhomogeneous cost satisfy submodularity constraints moreover demonstrate inhomogenous partition offer significant performance improvements applications structure learn rank subspace segmentation motif cluster\n",
            "0.4835915035671658\n",
            "0\n",
            "goal predict future video frame give sequence input frame despite large amount video data remain challenge task high dimensionality video frame address challenge propose decompositional disentangle predictive auto encoder ddpae framework combine structure probabilistic model deep network automatically decompose high dimensional video aim predict components disentangle component low dimensional temporal dynamics easier predict crucially appropriately specify generative model video frame ddpae able learn latent decomposition disentanglement without explicit supervision move mnist dataset show ddpae able recover underlie components individual digits disentanglement appearance location would intuitively demonstrate ddpae apply bounce ball dataset involve complex interactions multiple object predict video frame directly pixels recover physical state without explicit supervision\n",
            "0.45885035395622253\n",
            "generative adversarial network gans technique learn generative model complex data distributions sample despite remarkable advance generate realistic image major shortcoming gans fact tend produce sample little diversity even train diverse datasets phenomenon know mode collapse focus much recent work study principled approach handle mode collapse call pack main idea modify discriminator make decisions base multiple sample class either real artificially generate draw analysis tool binary hypothesis test particular seminal result blackwell prove fundamental connection pack mode collapse show pack naturally penalize generators mode collapse thereby favor generator distributions less mode collapse train process numerical experiment benchmark datasets suggest pack provide significant improvements\n",
            "0.48870134353637695\n",
            "22\n",
            "study problem policy policy evaluation oppe contrast prior work consider estimate individual policy value average policy value accurately draw inspiration recent work causal reason propose new finite sample generalization error bind value estimate mdp model use upper bind objective develop learn algorithm mdp model balance representation show approach yield substantially lower mse common synthetic benchmarks hiv treatment simulation domain\n",
            "0.4829282283782959\n",
            "0\n",
            "semantic scene completion predict volumetric occupancy object category scene help intelligent agents understand interact surround work propose disentangle framework sequentially carry semantic segmentation reprojection semantic scene completion three stage framework three advantage explicit semantic segmentation significantly boost performance flexible fusion ways sensor data bring good extensibility progress subtask promote holistic performance experimental result show regardless input single depth rgb framework generate high quality semantic scene completion outperform state art approach synthetic real datasets\n",
            "0.46194855372111004\n",
            "propose novel randomize first order optimization method sega sketch gradient method progressively throughout iterations build variance reduce estimate gradient random linear measurements sketch gradient provide iteration oracle iteration sega update current estimate gradient sketch project operation use information provide latest sketch subsequently use compute unbiased estimate true gradient random relaxation procedure unbiased estimate use perform gradient step unlike standard subspace descent methods coordinate descent sega use optimization problems non separable proximal term provide general convergence analysis prove linear convergence strongly convex objectives special case coordinate sketch sega enhance various techniques importance sample minibatching acceleration rate small constant factor identical best know rate coordinate descent\n",
            "0.46971086661020917\n",
            "despite remarkable advance image synthesis research exist work often fail manipulate image context large geometric transformations synthesize person image condition arbitrary pose one representative examples generation quality largely rely capability identify model arbitrary transformations different body part current generative model often build local convolutions overlook key challenge heavy occlusions different view dramatic appearance change distinct geometric change happen part cause arbitrary pose manipulations paper aim resolve challenge induce geometric variability spatial displacements via new soft gate warp generative adversarial network warp gin compose two stag first synthesize target part segmentation map give target pose depict region level spatial layouts guide image synthesis higher level structure constraints warp gin equip soft gate warp block learn feature level map render textures original image generate segmentation map warp gin capable control different transformation degrees give distinct target pose moreover propose warp block light weight flexible enough inject network human perceptual study quantitative evaluations demonstrate superiority warp gin significantly outperform exist methods two large datasets\n",
            "0.5015170574188232\n",
            "22\n",
            "tremendous recent progress equilibrium find algorithms zero sum imperfect information extensive form game puzzle gap theory practice first order methods significantly better theoretical convergence rat counterfactual regret minimization cfr variant despite cfr variants favor practice experiment first order methods conduct small medium size game methods complicate implement set cfr variants enhance extensively decade perform well practice paper show particular first order method state art variant excessive gap technique instantiate dilate entropy distance function efficiently solve large real world problems competitively cfr variants show large endgames encounter libratus poker recently beat top human poker specialist professionals limit texas hold show experimental result variant excessive gap technique well prior version introduce numerically friendly implementation smooth best response computation associate first order methods extensive form game solve present knowledge first gpu implementation first order method extensive form game present comparisons several excessive gap technique cfr variants\n",
            "0.5072090029716492\n",
            "14\n",
            "convolutional neural network cnns inherently subject invariable filter aggregate local input topological structure cause cnns allow manage data euclidean grid like structure image ones non euclidean graph structure traffic network broaden reach cnns develop structure aware convolution eliminate invariance yield unify mechanism deal euclidean non euclidean structure data technically filter structure aware convolution generalize univariate function capable aggregate local input diverse topological structure since infinite parameters require determine univariate function parameterize filter number learnable parameters context function approximation theory replace classical convolution cnns structure aware convolution structure aware convolutional neural network sacnns readily establish extensive experiment eleven datasets strongly evidence sacnns outperform current model various machine learn task include image classification cluster text categorization skeleton base action recognition molecular activity detection taxi flow prediction\n",
            "0.4887583472511985\n",
            "0\n",
            "recent years supervise learn convolutional network cnns see huge adoption computer vision applications comparatively unsupervised learn cnns receive less attention work hope help bridge gap success cnns supervise learn unsupervised learn introduce class cnns call deep convolutional generative adversarial network dcgans certain architectural constraints demonstrate strong candidate unsupervised learn train various image datasets show convince evidence deep convolutional adversarial pair learn hierarchy representations object part scenes generator discriminator additionally use learn feature novel task demonstrate applicability general image representations\n",
            "0.49811840057373047\n",
            "23\n",
            "recurrent neural network powerful tool understand model computation representation populations neurons continuous variable rate model network analyze apply extensively purpose however neurons fire action potentials discrete nature spike important feature neural circuit dynamics despite significant advance train recurrently connect spike neural network remain challenge present procedure train recurrently connect spike network generate dynamical pattern autonomously produce complex temporal output base integrate network input model physiological data procedure make use continuous variable network identify target train input spike model neurons surprisingly able construct spike network duplicate task perform continuous variable network relatively minor expansion number neurons approach provide novel view significance appropriate use fire rate model useful approach build model spike network use address important question representation computation neural systems\n",
            "0.4522149960199992\n",
            "paper address graph match problem follow recent work cite zaslavskiy path vestner analyze generalize idea concave relaxations introduce concepts emph conditionally concave emph probably conditionally concave energies polytopes show encapsulate many instance graph match problem include match euclidean graph graph surface prove local minima probably conditionally concave energies general match polytopes doubly stochastic high probability extreme point match polytope permutations\n",
            "0.4828670024871826\n",
            "7\n",
            "work introduce interactive structure learn framework unify many different interactive learn task present generalization query committee active learn algorithm set study consistency rate convergence theoretically empirically without noise\n",
            "0.4545886814594269\n",
            "convolutional neural network cnns recently achieve great success single image super resolution sisr however methods tend produce smooth output miss textural detail solve problems propose super resolution cliquenet srcliquenet reconstruct high resolution image better textural detail wavelet domain propose srcliquenet firstly extract set feature map low resolution image clique block group send set feature map clique sample module reconstruct image clique sample module consist four sub net predict high resolution wavelet coefficients four sub band since consider edge feature properties four sub band four sub net connect others learn coefficients four sub band jointly finally apply inverse discrete wavelet transform idwt output four sub net end clique sample module increase resolution reconstruct image extensive quantitative qualitative experiment benchmark datasets show method achieve superior performance state art methods\n",
            "0.49504634737968445\n",
            "24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0zo3uYEnevL",
        "outputId": "f53707f5-1b03-4b97-b527-40d9d3700fee"
      },
      "source": [
        "list2 = []\n",
        "sim = []\n",
        "empty_list = []\n",
        "for i in Reverse(list):\n",
        "  if len(i) > 0:\n",
        "    list2.append(i)\n",
        "list2_a = []\n",
        "for i in list:\n",
        "  if len(i) > 0:\n",
        "    list2_a.append(i)\n",
        "for k in range(len(list2_a)):\n",
        "  k2 = model.encode(list2_a[k], convert_to_tensor=False)\n",
        "  print(k2[0])\n",
        "  break\n",
        "  \n",
        "      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 3.48688394e-01  1.14210509e-01  3.30486149e-01  2.54061431e-01\n",
            " -5.59634924e-01  3.54014218e-01  1.03870705e-01  1.42412037e-01\n",
            "  3.18173200e-01  1.22078836e-01 -2.67956018e-01 -3.29017282e-01\n",
            "  8.70834216e-02  5.19963622e-01  2.11012855e-01 -4.09339786e-01\n",
            "  1.01074167e-01 -9.11884010e-02  1.31264076e-01 -2.11125836e-02\n",
            " -1.05277494e-01  6.46452531e-02 -2.22320825e-01 -3.70285124e-01\n",
            "  1.92893520e-01  1.20432056e-01 -2.45764136e-01 -1.30696103e-01\n",
            " -2.31865406e-01 -3.11057836e-01  5.49843460e-02 -2.41370186e-01\n",
            " -5.63828982e-02  1.19837478e-01  2.35281765e-01 -2.51238555e-01\n",
            "  1.32085755e-01 -3.65415066e-02 -1.52697459e-01  6.23982698e-02\n",
            "  5.70226252e-01  5.24002850e-01 -6.64203707e-03  8.22785136e-04\n",
            " -1.81432277e-01 -3.10951382e-01  9.29651409e-03  3.67190510e-01\n",
            "  1.93866283e-01  7.52349347e-02  3.16127390e-01  5.67721911e-02\n",
            " -2.61641234e-01  1.00542486e-01 -2.35177055e-01 -2.10447147e-01\n",
            " -1.05468489e-01  4.61082160e-01 -3.57637435e-01 -2.68475246e-02\n",
            " -1.19831190e-01 -4.93524373e-02  2.90894032e-01  4.00831968e-01\n",
            "  4.22693491e-01 -1.58457667e-01  1.59388512e-01  7.08546579e-01\n",
            " -1.74623191e-01 -1.80732384e-01 -9.60375220e-02  9.62308496e-02\n",
            "  3.58319789e-01  1.70702208e-02  2.99840271e-01  5.69026649e-01\n",
            "  8.11785012e-02  4.78732958e-03 -1.80479720e-01  7.87397087e-01\n",
            "  1.34417042e-01  5.94551861e-03  1.44620299e-01  1.43859252e-01\n",
            " -6.74086213e-02  4.86144334e-01 -1.13492787e-01  1.14335231e-02\n",
            "  1.56091988e-01  2.30443791e-01  2.89601702e-02  5.15808128e-02\n",
            " -2.96751350e-01 -1.82915345e-01  6.46276698e-02 -4.56826501e-02\n",
            " -1.97009876e-01 -4.36007440e-01 -5.34730516e-02 -2.55756319e-01\n",
            " -2.64846653e-01 -1.41768217e-01 -4.05851334e-01  2.94751227e-01\n",
            " -8.80271569e-02 -4.94174547e-02 -3.26661617e-01 -1.49981946e-01\n",
            "  2.08268553e-01 -1.21034257e-01  2.27931514e-02  4.46010560e-01\n",
            " -1.06204391e-01  4.26943228e-02  1.32960483e-01 -2.03009639e-02\n",
            "  4.67367582e-02 -3.74728769e-01  2.25429982e-02  1.62603542e-01\n",
            " -1.27031237e-01  2.27629989e-01 -7.33554363e-02 -7.62371793e-02\n",
            " -7.29009926e-01  4.84272420e-01 -2.19504073e-01  5.92105463e-03\n",
            " -2.45688379e-01  2.64665157e-01  6.52083084e-02 -1.44893736e-01\n",
            " -2.30101147e-03 -5.05513966e-01 -8.51201341e-02  1.46995440e-01\n",
            " -1.43910572e-01  3.22545762e-03 -1.27951711e-01 -1.01866499e-01\n",
            "  7.88057745e-02 -2.25072041e-01 -6.57578856e-02 -5.36324568e-02\n",
            "  2.22724155e-01  1.86474830e-01  2.00911649e-02 -2.28381947e-01\n",
            "  1.22782573e-01  1.97938517e-01 -4.13068309e-02  6.85671642e-02\n",
            " -7.03127310e-03 -2.80640662e-01  9.94796753e-02  3.33165646e-01\n",
            "  3.10997754e-01 -4.20026422e-01 -2.29963437e-02 -1.09724708e-01\n",
            "  2.07707193e-02  4.94391695e-02  1.97564978e-02 -8.08830336e-02\n",
            " -1.39907286e-01  2.24819541e-01  2.66766667e-01  7.50196259e-03\n",
            " -1.77697197e-01 -2.11058497e-01 -7.64921904e-02 -1.87547311e-01\n",
            " -2.49717772e-01  9.01593193e-02 -2.52052695e-01 -3.38029325e-01\n",
            " -2.24298790e-01 -1.17098674e-01  1.59402892e-01  1.04214726e-02\n",
            " -1.40326545e-01  4.54453640e-02 -1.75607443e-01 -1.86227076e-02\n",
            "  2.94119060e-01  1.66192830e-01 -1.23936556e-01  5.30068517e-01\n",
            " -2.07624286e-01 -2.80451756e-02 -1.30898401e-01  3.38322893e-02\n",
            " -1.67386681e-01  1.88767344e-01  2.64351994e-01 -3.54251981e-01\n",
            " -9.85893086e-02  9.58148092e-02  2.68498451e-01 -1.87097043e-01\n",
            " -1.18734688e-01  5.61085343e-01  2.35451087e-01  1.19640686e-01\n",
            " -3.67036536e-02 -3.46731283e-02  6.76995397e-01 -2.00898387e-02\n",
            "  2.16487899e-01  3.13558072e-01  2.22570539e-01  5.72935820e-01\n",
            "  3.82803500e-01  3.18700105e-01 -4.31551874e-01 -5.49944537e-03\n",
            "  1.99994937e-01  7.62200296e-01 -3.12982380e-01  2.86501646e-01\n",
            " -3.49863581e-02  1.59845233e-01  1.86761811e-01  9.93632749e-02\n",
            "  1.72343314e-01 -1.07944846e-01  1.87964603e-01  1.37135282e-01\n",
            " -2.61174619e-01 -2.12367758e-01 -6.09240048e-02  7.71362856e-02\n",
            "  6.24794737e-02  5.05525768e-02  3.67322445e-01  4.65195663e-02\n",
            " -1.43976197e-01  2.18414411e-01 -1.54119432e-01  1.67365879e-01\n",
            "  3.71308327e-01 -9.50555280e-02 -1.03852316e-03  1.81371570e-01\n",
            "  1.11896455e-01  2.12971672e-01  2.46771589e-01 -6.78927183e-01\n",
            "  2.63980657e-01 -6.23879075e-01  1.50926068e-01  1.37404352e-01\n",
            " -1.43178135e-01 -2.77074296e-02  4.67595935e-01 -4.53205407e-01\n",
            " -2.27917343e-01  8.60007703e-02  3.52867872e-01 -1.25527456e-01\n",
            " -4.89954986e-02  2.80411661e-01  8.88169452e-04  2.24511489e-01\n",
            " -2.74166428e-02  5.47876298e-01  8.42558369e-02 -2.52045602e-01\n",
            "  2.61872560e-01 -1.96283609e-01  1.59403071e-01  2.61479169e-01\n",
            "  4.39358503e-01  9.80518162e-02 -7.06122816e-02  2.79820591e-01\n",
            " -1.81973800e-01 -2.63384253e-01 -5.13620973e-01  6.94265887e-02\n",
            " -1.10497646e-01 -9.01878718e-03 -1.36995763e-01 -1.51131928e-01\n",
            "  1.79761797e-01 -5.70086949e-02 -1.47445902e-01 -1.43322900e-01\n",
            " -8.00203383e-02 -1.54118374e-01  1.17080577e-01 -1.85959071e-01\n",
            " -2.01499850e-01  1.24585874e-01 -3.64094484e-03  1.53848588e-01\n",
            "  1.13193668e-01 -1.30735770e-01  9.39571559e-02 -1.20177373e-01\n",
            " -5.67669153e-01  8.67590070e-01  6.78455293e-01  2.23900557e-01\n",
            "  1.42205104e-01  8.49241316e-02  1.71701983e-01  2.34986126e-01\n",
            " -3.31946880e-01  1.25424445e-01 -3.53522539e-01  1.46035746e-01\n",
            "  2.95308530e-02 -9.64076370e-02 -4.58406359e-02  1.80233881e-01\n",
            " -6.59596846e-02 -1.17221221e-01  2.37380713e-03 -1.96145073e-01\n",
            " -2.48507828e-01  3.48268688e-01 -2.25623343e-02  1.85174838e-01\n",
            "  8.52068588e-02  6.70521930e-02  1.53948486e-01 -2.84261405e-01\n",
            " -3.15685004e-01  5.27390689e-02  3.81615236e-02  4.08347696e-01\n",
            "  1.13265906e-02  3.64250928e-01 -2.17152223e-01  1.37185842e-01\n",
            "  1.53289422e-01  1.06471579e-03  1.25462800e-01  1.46368961e-03\n",
            " -6.51196986e-02 -8.99662077e-02 -4.16406244e-01 -4.57220292e-03\n",
            " -1.81149051e-01 -1.26432985e-01  3.68095994e-01 -2.31691240e-03\n",
            "  5.06316796e-02 -3.54800791e-01  1.69983923e-01 -4.89740781e-02\n",
            "  8.32865909e-02 -2.40378469e-01 -2.75919378e-01 -3.69860023e-01\n",
            " -1.23959161e-01  4.26878892e-02 -1.26789398e-02  2.57926099e-02\n",
            "  6.02843352e-02  1.75763033e-02 -7.90235624e-02 -2.92239338e-01\n",
            "  2.95500964e-01  2.62255281e-01 -4.37525548e-02  2.85014123e-01\n",
            " -3.83697838e-01  1.19897284e-01 -9.00354832e-02 -1.17014714e-01\n",
            "  2.87523419e-02  3.71944606e-01  4.76114213e-01  1.64630115e-01\n",
            "  2.03219473e-01  9.28097218e-02 -1.27942771e-01  2.38829806e-01\n",
            " -7.60317817e-02  2.58094579e-01 -1.06024772e-01 -2.19522677e-02\n",
            "  1.07425205e-01 -2.14736819e-01 -4.92286310e-03  1.77085370e-01\n",
            "  1.67002380e-01  3.96770798e-02  2.42281303e-01 -2.96366036e-01\n",
            "  7.50732347e-02 -1.12624787e-01  2.01117441e-01  1.19659625e-01\n",
            "  7.58463889e-02 -2.21422851e-01  3.49368960e-01  1.11836970e-01\n",
            " -9.37838852e-03 -1.19990140e-01  4.86290641e-02 -3.81916314e-02\n",
            "  3.03567499e-02 -6.06952086e-02 -2.54036218e-01 -1.15343012e-01\n",
            "  1.63771123e-01 -2.17552483e-01  7.93377124e-03  2.97484189e-01\n",
            "  7.65354410e-02 -2.67683733e-02 -4.46052253e-01  2.95545787e-01\n",
            "  1.74761295e-01 -2.98718601e-01  1.07181601e-01 -2.47234896e-01\n",
            "  1.43851459e-01 -1.64107233e-01 -2.14923814e-01 -1.25701830e-01\n",
            " -5.00129938e-01 -7.73498788e-03  1.28072903e-01  2.33284265e-01\n",
            " -1.79398566e-01  1.07494384e-01  1.32212371e-01 -5.09679727e-02\n",
            "  1.59034263e-02 -2.14492436e-02  2.12678447e-01  3.95568162e-02\n",
            "  6.10940233e-02 -2.45075017e-01 -8.28838050e-02 -2.21549552e-02\n",
            " -4.04588312e-01  1.16051286e-01 -1.38319880e-01  2.74585304e-03\n",
            "  7.55752251e-02  2.07782388e-01  1.54515073e-01 -2.75865626e-02\n",
            "  6.32622242e-01 -4.53887403e-01  1.82176739e-01 -9.25024152e-02\n",
            " -7.63174176e-01  2.96771586e-01 -2.36868888e-01  4.54393655e-01\n",
            "  7.92844445e-02  2.48186924e-02 -7.12382495e-02 -3.09612364e-01\n",
            "  1.76294168e-04  1.84069917e-01 -1.32342681e-01 -1.10044651e-01\n",
            " -1.95704237e-01  2.21674979e-01  2.34408259e-01  3.27634245e-01\n",
            " -3.54794919e-01  4.96008724e-01 -1.49321854e-01 -1.63287878e-01\n",
            "  5.83130196e-02  7.55572990e-02 -2.03367114e-01 -4.74933833e-02\n",
            " -2.10274160e-01 -2.07117289e-01 -2.58302897e-01 -2.45017439e-01\n",
            "  1.10059254e-01  2.29021274e-02  1.55520225e-02 -3.64336163e-01\n",
            " -2.43426953e-03  1.40882745e-01 -1.08071916e-01 -2.59312820e-02\n",
            " -2.82395869e-01 -1.43830359e-01 -9.22446977e-03  4.33907807e-01\n",
            " -3.01638395e-01  3.21222208e-02  1.34861708e+00 -6.35713041e-02\n",
            " -1.09648608e-01  3.59798312e-01 -1.01244882e-01 -3.21379229e-02\n",
            " -2.33007446e-01  3.29977274e-02 -1.75279856e-01  1.71756014e-01\n",
            " -2.66015142e-01  1.16300941e-01  7.44084939e-02  2.51254439e-01\n",
            "  3.19966525e-01 -5.80866151e-02  2.53793776e-01 -3.65599126e-01\n",
            "  3.16669613e-01  2.28683069e-01  2.55657941e-01  3.61550480e-01\n",
            "  3.23774099e-01  8.01142305e-02  8.38185661e-03  6.38550937e-01\n",
            " -1.76768936e-02  2.34991729e-01  1.46333694e-01  6.65521845e-02\n",
            "  2.04729438e-02 -2.25428447e-01 -5.14842093e-01 -1.59427095e-02\n",
            "  1.31457970e-01 -3.39744240e-01 -1.37816772e-01 -3.84767294e-01\n",
            "  7.62654319e-02 -9.19400603e-02 -2.97340006e-02 -2.20745444e-01\n",
            "  2.55170643e-01  1.52631015e-01 -2.59789824e-01 -7.52862692e-02\n",
            "  7.98058391e-01  1.58114493e-01  3.31936739e-02 -3.77371103e-01\n",
            " -2.28156522e-01  4.74337153e-02  3.54992867e-01 -1.12107046e-01\n",
            "  1.61954686e-01 -2.21665710e-01  1.24233641e-01  5.80786727e-02\n",
            " -2.30983961e-02  4.18347239e-01  3.94602753e-02  1.29283650e-03\n",
            " -3.40175699e-03 -6.33896813e-02 -2.58703291e-01  3.30926448e-01\n",
            "  9.82852355e-02 -6.15464270e-01  8.63039419e-02 -4.18688394e-02\n",
            "  3.49218816e-01  5.73409535e-02 -7.63699859e-02  1.69393539e-01\n",
            "  1.37415767e-01  1.41643494e-01 -2.72941947e-01  2.31452599e-01\n",
            " -2.17907414e-01  6.77695990e-01 -2.19650045e-01  3.45497169e-02\n",
            "  1.36585504e-01 -1.51577353e-01 -2.74632037e-01  1.01451546e-01\n",
            "  2.70701319e-01 -1.57835469e-01  3.72596905e-02 -2.20349818e-01\n",
            "  4.16498110e-02  2.89067268e-01 -3.68461102e-01 -7.82314390e-02\n",
            " -5.16091764e-01  4.46397394e-01 -4.26661730e-01  7.82034695e-02\n",
            " -3.33399564e-01  1.59473404e-01  7.04321414e-02 -2.46694773e-01\n",
            "  2.54005313e-01 -4.70372975e-01 -1.02833949e-01  1.10761099e-01\n",
            " -6.24144524e-02  8.95108357e-02  2.54944805e-02  5.96732609e-02\n",
            " -3.11008573e-01 -9.84058902e-02  1.98601276e-01 -8.23146626e-02\n",
            " -2.67935991e-01  2.99569815e-01 -1.33822143e-01 -2.12245375e-01\n",
            " -1.22009747e-01  1.07729740e-01  3.94642539e-02 -1.29027963e-01\n",
            "  9.82768387e-02 -1.02006070e-01 -5.54297715e-02  2.14904845e-01\n",
            " -1.61470585e-02 -1.40588935e-02  1.73085228e-01  4.70724344e-01\n",
            "  1.10122763e-01 -1.56781748e-01 -1.57243133e-01 -1.02966884e-02\n",
            "  1.16892405e-01  1.27629012e-01 -1.35847166e-01 -1.37733325e-01\n",
            "  1.19284969e-02 -1.94506213e-01 -1.00096054e-01  4.97378819e-02\n",
            "  4.16249558e-02  3.02650809e-01 -2.41984546e-01  7.71465153e-02\n",
            "  2.20671475e-01  3.44391279e-02 -9.57596153e-02  2.65737116e-01\n",
            " -1.25101507e-01 -1.05853334e-01  9.90145057e-02 -3.80689591e-01\n",
            " -7.11801043e-03 -3.80195946e-01 -8.90522227e-02 -2.91234821e-01\n",
            " -1.56907097e-01 -5.22968918e-02 -1.06691867e-01  2.25389898e-02\n",
            " -6.60724223e-01  3.20383936e-01  1.14411019e-01 -2.22786561e-01\n",
            "  2.95159400e-01  2.72691339e-01 -1.37970239e-01 -3.85492772e-01\n",
            "  5.79200029e-01  2.01685891e-01  2.96482928e-02  2.66896605e-01\n",
            "  2.33590789e-02  1.95099935e-02  1.70418329e-03 -3.44129279e-02\n",
            "  6.43602133e-01  2.45275609e-02 -3.38272363e-01 -3.06629598e-01\n",
            "  9.46122706e-01  4.66567993e-01 -2.34055847e-01 -6.54111356e-02\n",
            " -1.58039719e-01 -1.88021258e-01  7.17893243e-01  8.59077089e-03\n",
            " -6.13639541e-02  4.02261406e-01 -2.35424653e-01 -1.22486502e-01\n",
            "  1.14191808e-02 -4.02802736e-01 -1.73203379e-01  2.30496705e-01\n",
            " -2.94283569e-01 -1.28730416e-01  1.02230683e-01 -8.59398618e-02\n",
            "  8.59019160e-03 -5.16355112e-02 -2.23240271e-01  3.81788194e-01\n",
            " -9.25586820e-02  3.72230113e-01  2.04543825e-02 -1.91614591e-02\n",
            " -1.55057997e-01  2.83435620e-02  2.92650163e-01 -2.26930782e-01\n",
            " -8.32340196e-02  5.12270182e-02  3.23549241e-01 -4.45660874e-02\n",
            " -4.63693477e-02  1.13395691e-01  1.55980989e-01  1.19335391e-02\n",
            " -6.57192171e-01  2.03663915e-01  5.57737648e-01  9.13728997e-02\n",
            "  5.48686922e-01 -2.98629642e-01  1.89471409e-01 -1.15112476e-02\n",
            " -5.57015061e-01 -7.91777968e-02 -5.59441984e-01 -3.20289403e-01\n",
            " -2.64926814e-02  4.39798757e-02  1.19346930e-02  9.54440683e-02\n",
            "  4.67100829e-01  2.19063938e-01 -2.02382028e-01 -5.98676682e-01\n",
            "  4.61265333e-02 -1.64001867e-01 -1.35376066e-01  7.18358085e-02\n",
            " -2.78063357e-01 -5.09221375e-01 -4.21622545e-01 -8.78877714e-02\n",
            " -3.49897444e-01  1.10333152e-01 -7.52317235e-02 -3.22169960e-02\n",
            "  9.63519365e-02 -3.07184663e-02 -4.18766946e-01  1.42567262e-01\n",
            " -2.19035238e-01  5.73556274e-02  3.80503796e-02  2.12556526e-01\n",
            "  1.84968054e-01  1.81798302e-02 -2.26596773e-01 -7.42641911e-02\n",
            " -1.08532682e-01  5.46775758e-02 -2.05737978e-01  2.46406645e-01\n",
            " -3.64944905e-01 -2.43236441e-02  3.02145898e-01 -2.25133896e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYV4KNqSs2CS",
        "outputId": "76c3a5cd-e2b1-40cf-eb40-2f9482cd998c"
      },
      "source": [
        "len(k2[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYooTG3HnN5T",
        "outputId": "9b2c2e52-7070-4148-e0cc-6c629e7ac25d"
      },
      "source": [
        "#print(list2_a[k])\n",
        "\n",
        "for k in range(len(list2_a)):\n",
        "  embedd = []\n",
        "  for m in range(len(list2_a[k])):\n",
        "    for l in range(len(data_list2)):\n",
        "      if list2_a[k][m] == data_list2[l]:\n",
        "        embedd.append(embeddings_test[l])\n",
        "  print(embedd[0:])\n",
        "\n",
        "  break\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([ 3.48688573e-01,  1.14210613e-01,  3.30486178e-01,  2.54061282e-01,\n",
            "       -5.59635103e-01,  3.54013711e-01,  1.03870757e-01,  1.42412066e-01,\n",
            "        3.18173200e-01,  1.22079022e-01, -2.67955810e-01, -3.29017073e-01,\n",
            "        8.70833695e-02,  5.19963562e-01,  2.11012900e-01, -4.09339905e-01,\n",
            "        1.01073906e-01, -9.11883265e-02,  1.31264031e-01, -2.11125333e-02,\n",
            "       -1.05277471e-01,  6.46450073e-02, -2.22320810e-01, -3.70284915e-01,\n",
            "        1.92893609e-01,  1.20432071e-01, -2.45764345e-01, -1.30696133e-01,\n",
            "       -2.31865272e-01, -3.11057925e-01,  5.49844950e-02, -2.41370395e-01,\n",
            "       -5.63827232e-02,  1.19837396e-01,  2.35282019e-01, -2.51238585e-01,\n",
            "        1.32085666e-01, -3.65415663e-02, -1.52697414e-01,  6.23982884e-02,\n",
            "        5.70225954e-01,  5.24003029e-01, -6.64209668e-03,  8.22800852e-04,\n",
            "       -1.81432307e-01, -3.10951442e-01,  9.29630548e-03,  3.67190570e-01,\n",
            "        1.93866313e-01,  7.52348453e-02,  3.16127419e-01,  5.67723438e-02,\n",
            "       -2.61641324e-01,  1.00542374e-01, -2.35177204e-01, -2.10446790e-01,\n",
            "       -1.05468579e-01,  4.61082160e-01, -3.57637256e-01, -2.68475153e-02,\n",
            "       -1.19831391e-01, -4.93524931e-02,  2.90894121e-01,  4.00832176e-01,\n",
            "        4.22693491e-01, -1.58457711e-01,  1.59388632e-01,  7.08546758e-01,\n",
            "       -1.74623162e-01, -1.80732310e-01, -9.60373208e-02,  9.62309167e-02,\n",
            "        3.58319700e-01,  1.70703139e-02,  2.99840242e-01,  5.69026887e-01,\n",
            "        8.11783895e-02,  4.78718057e-03, -1.80479571e-01,  7.87397206e-01,\n",
            "        1.34417042e-01,  5.94564853e-03,  1.44620359e-01,  1.43859193e-01,\n",
            "       -6.74086958e-02,  4.86144334e-01, -1.13492891e-01,  1.14337634e-02,\n",
            "        1.56092077e-01,  2.30443850e-01,  2.89600547e-02,  5.15809357e-02,\n",
            "       -2.96751380e-01, -1.82915181e-01,  6.46275654e-02, -4.56826761e-02,\n",
            "       -1.97010025e-01, -4.36007529e-01, -5.34730330e-02, -2.55756378e-01,\n",
            "       -2.64846563e-01, -1.41768396e-01, -4.05851185e-01,  2.94751525e-01,\n",
            "       -8.80272910e-02, -4.94173169e-02, -3.26661408e-01, -1.49981946e-01,\n",
            "        2.08268598e-01, -1.21034205e-01,  2.27932092e-02,  4.46010560e-01,\n",
            "       -1.06204346e-01,  4.26943526e-02,  1.32960409e-01, -2.03011371e-02,\n",
            "        4.67368700e-02, -3.74728739e-01,  2.25428361e-02,  1.62603736e-01,\n",
            "       -1.27031103e-01,  2.27630004e-01, -7.33553097e-02, -7.62368515e-02,\n",
            "       -7.29009748e-01,  4.84272420e-01, -2.19503969e-01,  5.92090515e-03,\n",
            "       -2.45688349e-01,  2.64665008e-01,  6.52084276e-02, -1.44893631e-01,\n",
            "       -2.30096583e-03, -5.05513728e-01, -8.51199627e-02,  1.46995261e-01,\n",
            "       -1.43910572e-01,  3.22548836e-03, -1.27951637e-01, -1.01866499e-01,\n",
            "        7.88055435e-02, -2.25072026e-01, -6.57578260e-02, -5.36323711e-02,\n",
            "        2.22724184e-01,  1.86474890e-01,  2.00911909e-02, -2.28382006e-01,\n",
            "        1.22782782e-01,  1.97938800e-01, -4.13069353e-02,  6.85671270e-02,\n",
            "       -7.03111477e-03, -2.80640602e-01,  9.94797200e-02,  3.33165586e-01,\n",
            "        3.10997754e-01, -4.20026302e-01, -2.29964461e-02, -1.09724618e-01,\n",
            "        2.07708087e-02,  4.94393557e-02,  1.97564960e-02, -8.08832124e-02,\n",
            "       -1.39907315e-01,  2.24819630e-01,  2.66766548e-01,  7.50178052e-03,\n",
            "       -1.77697301e-01, -2.11058557e-01, -7.64921680e-02, -1.87547371e-01,\n",
            "       -2.49717802e-01,  9.01594236e-02, -2.52052486e-01, -3.38029444e-01,\n",
            "       -2.24298775e-01, -1.17098801e-01,  1.59402952e-01,  1.04216114e-02,\n",
            "       -1.40326619e-01,  4.54453528e-02, -1.75607502e-01, -1.86227001e-02,\n",
            "        2.94119239e-01,  1.66192651e-01, -1.23936541e-01,  5.30068636e-01,\n",
            "       -2.07624331e-01, -2.80451011e-02, -1.30898282e-01,  3.38322856e-02,\n",
            "       -1.67386681e-01,  1.88767359e-01,  2.64351934e-01, -3.54251951e-01,\n",
            "       -9.85891670e-02,  9.58150476e-02,  2.68498689e-01, -1.87097073e-01,\n",
            "       -1.18734799e-01,  5.61085582e-01,  2.35451117e-01,  1.19640686e-01,\n",
            "       -3.67035605e-02, -3.46730575e-02,  6.76995158e-01, -2.00897940e-02,\n",
            "        2.16487989e-01,  3.13558191e-01,  2.22570509e-01,  5.72935939e-01,\n",
            "        3.82803410e-01,  3.18700105e-01, -4.31551784e-01, -5.49913617e-03,\n",
            "        1.99994892e-01,  7.62200296e-01, -3.12982321e-01,  2.86501557e-01,\n",
            "       -3.49865891e-02,  1.59845293e-01,  1.86761960e-01,  9.93631482e-02,\n",
            "        1.72343373e-01, -1.07945047e-01,  1.87964633e-01,  1.37135178e-01,\n",
            "       -2.61174709e-01, -2.12368011e-01, -6.09240159e-02,  7.71360174e-02,\n",
            "        6.24795891e-02,  5.05526215e-02,  3.67322564e-01,  4.65195216e-02,\n",
            "       -1.43976018e-01,  2.18414500e-01, -1.54119477e-01,  1.67365834e-01,\n",
            "        3.71308506e-01, -9.50555056e-02, -1.03845110e-03,  1.81371629e-01,\n",
            "        1.11896344e-01,  2.12971509e-01,  2.46771619e-01, -6.78927243e-01,\n",
            "        2.63980985e-01, -6.23879015e-01,  1.50926068e-01,  1.37404606e-01,\n",
            "       -1.43178105e-01, -2.77075525e-02,  4.67596084e-01, -4.53205526e-01,\n",
            "       -2.27917284e-01,  8.60010535e-02,  3.52867961e-01, -1.25527471e-01,\n",
            "       -4.89952378e-02,  2.80411571e-01,  8.88193026e-04,  2.24511489e-01,\n",
            "       -2.74167266e-02,  5.47876060e-01,  8.42559338e-02, -2.52045453e-01,\n",
            "        2.61872441e-01, -1.96283787e-01,  1.59403086e-01,  2.61479288e-01,\n",
            "        4.39358592e-01,  9.80519652e-02, -7.06123635e-02,  2.79820502e-01,\n",
            "       -1.81973740e-01, -2.63384521e-01, -5.13621092e-01,  6.94265068e-02,\n",
            "       -1.10497683e-01, -9.01872665e-03, -1.36995584e-01, -1.51131824e-01,\n",
            "        1.79761738e-01, -5.70084825e-02, -1.47446081e-01, -1.43323168e-01,\n",
            "       -8.00202936e-02, -1.54118255e-01,  1.17080413e-01, -1.85959086e-01,\n",
            "       -2.01499641e-01,  1.24586031e-01, -3.64112831e-03,  1.53848529e-01,\n",
            "        1.13193624e-01, -1.30735740e-01,  9.39572752e-02, -1.20177545e-01,\n",
            "       -5.67669451e-01,  8.67590368e-01,  6.78455293e-01,  2.23900676e-01,\n",
            "        1.42205074e-01,  8.49241391e-02,  1.71702221e-01,  2.34986141e-01,\n",
            "       -3.31946611e-01,  1.25424266e-01, -3.53522629e-01,  1.46035641e-01,\n",
            "        2.95307618e-02, -9.64076146e-02, -4.58404981e-02,  1.80233732e-01,\n",
            "       -6.59596920e-02, -1.17221214e-01,  2.37376383e-03, -1.96145043e-01,\n",
            "       -2.48507887e-01,  3.48268896e-01, -2.25624014e-02,  1.85174793e-01,\n",
            "        8.52068514e-02,  6.70523122e-02,  1.53948218e-01, -2.84261584e-01,\n",
            "       -3.15684915e-01,  5.27392961e-02,  3.81612591e-02,  4.08347696e-01,\n",
            "        1.13266362e-02,  3.64251137e-01, -2.17152223e-01,  1.37186006e-01,\n",
            "        1.53289422e-01,  1.06466340e-03,  1.25462711e-01,  1.46392547e-03,\n",
            "       -6.51196986e-02, -8.99663642e-02, -4.16406244e-01, -4.57223505e-03,\n",
            "       -1.81149185e-01, -1.26432955e-01,  3.68096143e-01, -2.31701857e-03,\n",
            "        5.06315976e-02, -3.54800761e-01,  1.69983923e-01, -4.89740744e-02,\n",
            "        8.32864717e-02, -2.40378290e-01, -2.75919497e-01, -3.69860023e-01,\n",
            "       -1.23959154e-01,  4.26878333e-02, -1.26790609e-02,  2.57926825e-02,\n",
            "        6.02844097e-02,  1.75763015e-02, -7.90236443e-02, -2.92239487e-01,\n",
            "        2.95500964e-01,  2.62255609e-01, -4.37525697e-02,  2.85014153e-01,\n",
            "       -3.83697927e-01,  1.19897321e-01, -9.00354683e-02, -1.17014773e-01,\n",
            "        2.87522506e-02,  3.71944547e-01,  4.76114213e-01,  1.64629921e-01,\n",
            "        2.03219444e-01,  9.28096995e-02, -1.27942979e-01,  2.38829732e-01,\n",
            "       -7.60315880e-02,  2.58094579e-01, -1.06024742e-01, -2.19521932e-02,\n",
            "        1.07425146e-01, -2.14736626e-01, -4.92285658e-03,  1.77085623e-01,\n",
            "        1.67002246e-01,  3.96770313e-02,  2.42281362e-01, -2.96366125e-01,\n",
            "        7.50731453e-02, -1.12624921e-01,  2.01117337e-01,  1.19659521e-01,\n",
            "        7.58464485e-02, -2.21422642e-01,  3.49368900e-01,  1.11837022e-01,\n",
            "       -9.37819760e-03, -1.19990237e-01,  4.86290529e-02, -3.81917432e-02,\n",
            "        3.03565487e-02, -6.06952868e-02, -2.54036218e-01, -1.15343168e-01,\n",
            "        1.63770929e-01, -2.17552230e-01,  7.93394260e-03,  2.97484100e-01,\n",
            "        7.65354484e-02, -2.67682076e-02, -4.46052164e-01,  2.95545548e-01,\n",
            "        1.74761042e-01, -2.98719078e-01,  1.07181683e-01, -2.47235015e-01,\n",
            "        1.43851548e-01, -1.64107054e-01, -2.14923814e-01, -1.25701740e-01,\n",
            "       -5.00129998e-01, -7.73502467e-03,  1.28072798e-01,  2.33284265e-01,\n",
            "       -1.79398715e-01,  1.07494310e-01,  1.32212296e-01, -5.09678572e-02,\n",
            "        1.59032922e-02, -2.14492623e-02,  2.12678432e-01,  3.95567603e-02,\n",
            "        6.10941909e-02, -2.45074928e-01, -8.28837976e-02, -2.21550316e-02,\n",
            "       -4.04588431e-01,  1.16051264e-01, -1.38319910e-01,  2.74556945e-03,\n",
            "        7.55750909e-02,  2.07782477e-01,  1.54514715e-01, -2.75865123e-02,\n",
            "        6.32622242e-01, -4.53887522e-01,  1.82176650e-01, -9.25025716e-02,\n",
            "       -7.63174057e-01,  2.96771705e-01, -2.36869022e-01,  4.54393655e-01,\n",
            "        7.92843774e-02,  2.48186607e-02, -7.12383166e-02, -3.09612483e-01,\n",
            "        1.76298752e-04,  1.84070110e-01, -1.32342741e-01, -1.10044703e-01,\n",
            "       -1.95704073e-01,  2.21675187e-01,  2.34408095e-01,  3.27634394e-01,\n",
            "       -3.54794592e-01,  4.96008724e-01, -1.49321780e-01, -1.63287818e-01,\n",
            "        5.83131798e-02,  7.55572617e-02, -2.03367189e-01, -4.74936254e-02,\n",
            "       -2.10274220e-01, -2.07117438e-01, -2.58303016e-01, -2.45017484e-01,\n",
            "        1.10059246e-01,  2.29019858e-02,  1.55520029e-02, -3.64336163e-01,\n",
            "       -2.43446324e-03,  1.40882760e-01, -1.08072154e-01, -2.59310920e-02,\n",
            "       -2.82395810e-01, -1.43830165e-01, -9.22425091e-03,  4.33907866e-01,\n",
            "       -3.01638305e-01,  3.21222879e-02,  1.34861732e+00, -6.35714456e-02,\n",
            "       -1.09648578e-01,  3.59798312e-01, -1.01244546e-01, -3.21376473e-02,\n",
            "       -2.33007446e-01,  3.29976194e-02, -1.75279871e-01,  1.71755627e-01,\n",
            "       -2.66015202e-01,  1.16301067e-01,  7.44085088e-02,  2.51254439e-01,\n",
            "        3.19966704e-01, -5.80869503e-02,  2.53793746e-01, -3.65599036e-01,\n",
            "        3.16669703e-01,  2.28683025e-01,  2.55658031e-01,  3.61550391e-01,\n",
            "        3.23774159e-01,  8.01142529e-02,  8.38176720e-03,  6.38551056e-01,\n",
            "       -1.76770557e-02,  2.34991938e-01,  1.46333754e-01,  6.65521845e-02,\n",
            "        2.04730313e-02, -2.25428522e-01, -5.14842033e-01, -1.59425344e-02,\n",
            "        1.31457821e-01, -3.39744121e-01, -1.37816980e-01, -3.84767294e-01,\n",
            "        7.62654468e-02, -9.19402391e-02, -2.97339875e-02, -2.20745355e-01,\n",
            "        2.55170792e-01,  1.52630985e-01, -2.59789675e-01, -7.52862245e-02,\n",
            "        7.98058569e-01,  1.58114731e-01,  3.31935175e-02, -3.77371073e-01,\n",
            "       -2.28156522e-01,  4.74339053e-02,  3.54993254e-01, -1.12107038e-01,\n",
            "        1.61954671e-01, -2.21665934e-01,  1.24233715e-01,  5.80786131e-02,\n",
            "       -2.30983421e-02,  4.18347120e-01,  3.94600593e-02,  1.29289413e-03,\n",
            "       -3.40173952e-03, -6.33897707e-02, -2.58703351e-01,  3.30926448e-01,\n",
            "        9.82853696e-02, -6.15464389e-01,  8.63037333e-02, -4.18687910e-02,\n",
            "        3.49219054e-01,  5.73408157e-02, -7.63700455e-02,  1.69393569e-01,\n",
            "        1.37415901e-01,  1.41643435e-01, -2.72942096e-01,  2.31452465e-01,\n",
            "       -2.17907429e-01,  6.77696168e-01, -2.19650120e-01,  3.45497690e-02,\n",
            "        1.36585429e-01, -1.51577756e-01, -2.74632305e-01,  1.01451620e-01,\n",
            "        2.70701468e-01, -1.57835379e-01,  3.72595340e-02, -2.20349863e-01,\n",
            "        4.16500755e-02,  2.89067149e-01, -3.68460983e-01, -7.82315955e-02,\n",
            "       -5.16091466e-01,  4.46397334e-01, -4.26661700e-01,  7.82033503e-02,\n",
            "       -3.33399504e-01,  1.59473300e-01,  7.04321861e-02, -2.46694878e-01,\n",
            "        2.54005164e-01, -4.70372796e-01, -1.02834180e-01,  1.10761076e-01,\n",
            "       -6.24144562e-02,  8.95109251e-02,  2.54944842e-02,  5.96733652e-02,\n",
            "       -3.11008632e-01, -9.84059200e-02,  1.98601335e-01, -8.23146701e-02,\n",
            "       -2.67936170e-01,  2.99569905e-01, -1.33822128e-01, -2.12245271e-01,\n",
            "       -1.22009844e-01,  1.07729673e-01,  3.94641198e-02, -1.29028097e-01,\n",
            "        9.82767493e-02, -1.02006018e-01, -5.54296412e-02,  2.14904785e-01,\n",
            "       -1.61470976e-02, -1.40588777e-02,  1.73085064e-01,  4.70724493e-01,\n",
            "        1.10122815e-01, -1.56781748e-01, -1.57243237e-01, -1.02966642e-02,\n",
            "        1.16892375e-01,  1.27629206e-01, -1.35847241e-01, -1.37733236e-01,\n",
            "        1.19285006e-02, -1.94506243e-01, -1.00095756e-01,  4.97377887e-02,\n",
            "        4.16249931e-02,  3.02650690e-01, -2.41984546e-01,  7.71465674e-02,\n",
            "        2.20671415e-01,  3.44388634e-02, -9.57598612e-02,  2.65736997e-01,\n",
            "       -1.25101462e-01, -1.05853356e-01,  9.90145952e-02, -3.80689263e-01,\n",
            "       -7.11808773e-03, -3.80195826e-01, -8.90521407e-02, -2.91234612e-01,\n",
            "       -1.56906992e-01, -5.22969700e-02, -1.06691822e-01,  2.25392953e-02,\n",
            "       -6.60724163e-01,  3.20384026e-01,  1.14410847e-01, -2.22786635e-01,\n",
            "        2.95159489e-01,  2.72691429e-01, -1.37970194e-01, -3.85492712e-01,\n",
            "        5.79200089e-01,  2.01685980e-01,  2.96482891e-02,  2.66896486e-01,\n",
            "        2.33590622e-02,  1.95099376e-02,  1.70443219e-03, -3.44130397e-02,\n",
            "        6.43601894e-01,  2.45274063e-02, -3.38272542e-01, -3.06629539e-01,\n",
            "        9.46122587e-01,  4.66568232e-01, -2.34055787e-01, -6.54111132e-02,\n",
            "       -1.58039808e-01, -1.88021392e-01,  7.17893183e-01,  8.59060511e-03,\n",
            "       -6.13638423e-02,  4.02261347e-01, -2.35424697e-01, -1.22486509e-01,\n",
            "        1.14191379e-02, -4.02802855e-01, -1.73203409e-01,  2.30496734e-01,\n",
            "       -2.94283569e-01, -1.28730640e-01,  1.02230608e-01, -8.59397501e-02,\n",
            "        8.59022047e-03, -5.16356044e-02, -2.23240197e-01,  3.81788075e-01,\n",
            "       -9.25584659e-02,  3.72230321e-01,  2.04545222e-02, -1.91614553e-02,\n",
            "       -1.55058011e-01,  2.83435471e-02,  2.92650193e-01, -2.26930842e-01,\n",
            "       -8.32340941e-02,  5.12269773e-02,  3.23549300e-01, -4.45661545e-02,\n",
            "       -4.63694148e-02,  1.13395482e-01,  1.55981079e-01,  1.19337011e-02,\n",
            "       -6.57192290e-01,  2.03664050e-01,  5.57737708e-01,  9.13728848e-02,\n",
            "        5.48686564e-01, -2.98629791e-01,  1.89471483e-01, -1.15111014e-02,\n",
            "       -5.57015121e-01, -7.91778490e-02, -5.59442103e-01, -3.20289493e-01,\n",
            "       -2.64927167e-02,  4.39800024e-02,  1.19347470e-02,  9.54440311e-02,\n",
            "        4.67100918e-01,  2.19063640e-01, -2.02382162e-01, -5.98676980e-01,\n",
            "        4.61266227e-02, -1.64001822e-01, -1.35375753e-01,  7.18358085e-02,\n",
            "       -2.78063416e-01, -5.09221315e-01, -4.21622723e-01, -8.78878385e-02,\n",
            "       -3.49897414e-01,  1.10333167e-01, -7.52316639e-02, -3.22169065e-02,\n",
            "        9.63520706e-02, -3.07186283e-02, -4.18766975e-01,  1.42567262e-01,\n",
            "       -2.19035625e-01,  5.73557094e-02,  3.80502827e-02,  2.12556526e-01,\n",
            "        1.84968084e-01,  1.81797054e-02, -2.26596683e-01, -7.42641762e-02,\n",
            "       -1.08532682e-01,  5.46775013e-02, -2.05738023e-01,  2.46406749e-01,\n",
            "       -3.64944875e-01, -2.43235901e-02,  3.02145958e-01, -2.25134000e-01],\n",
            "      dtype=float32), array([ 2.57173598e-01, -1.44270351e-02,  1.64692178e-01, -8.54398832e-02,\n",
            "       -5.44542260e-02,  1.09104201e-01,  2.90856898e-01,  6.05236255e-02,\n",
            "        8.60511065e-02, -8.89503956e-02, -3.88474375e-01,  1.03231460e-01,\n",
            "        1.24525100e-01,  2.83864617e-01, -7.05178082e-02, -5.98625958e-01,\n",
            "        2.79682398e-01,  3.03964317e-01,  2.06405059e-01, -8.29078332e-02,\n",
            "       -9.04026851e-02, -2.21540689e-01, -6.79238319e-01, -1.73255876e-01,\n",
            "        1.30267397e-01, -4.85911742e-02, -1.44538343e-01,  1.97717667e-01,\n",
            "       -3.80695701e-01,  5.73636703e-02, -5.30971698e-02, -1.29310906e-01,\n",
            "        5.24048768e-02,  1.19366467e-01,  4.60523844e-01,  1.36452317e-01,\n",
            "        2.29174748e-01,  5.66318147e-02, -3.50648165e-01,  8.37347433e-02,\n",
            "        1.17328294e-01,  2.06564233e-01, -6.29786775e-02,  1.51314914e-01,\n",
            "       -2.99871713e-01, -1.25284284e-01, -2.92514443e-01,  2.57261813e-01,\n",
            "        1.05533719e-01,  1.68761998e-01,  3.58201295e-01,  1.01335838e-01,\n",
            "       -1.95376179e-03,  5.11900112e-02, -1.39226317e-01, -3.89734050e-03,\n",
            "       -3.09420060e-02, -3.74524623e-01, -3.21758352e-02, -2.89839745e-01,\n",
            "        2.64281332e-01, -1.51579743e-02,  2.31562391e-01,  4.89667535e-01,\n",
            "       -5.71400411e-02, -1.09771870e-01,  1.16391234e-01,  1.32598355e-01,\n",
            "       -1.69327468e-01,  3.03315133e-01, -1.57714501e-01, -1.65769964e-01,\n",
            "        3.95382941e-01, -3.19239676e-01,  2.85044551e-01,  6.80388868e-01,\n",
            "       -1.64561227e-01, -1.94273144e-01, -3.29971790e-01,  6.12217486e-01,\n",
            "        1.85778379e-01, -1.14643395e-01,  2.11242363e-02,  9.10067838e-03,\n",
            "        4.85635012e-01,  1.30626634e-01, -3.83559205e-02, -1.38352066e-01,\n",
            "        7.68764988e-02, -1.39219254e-01,  2.36327257e-02, -3.82358804e-02,\n",
            "       -3.61815602e-01, -3.67899798e-02,  1.87624440e-01, -2.58322418e-01,\n",
            "        2.00116619e-01, -7.79066503e-01,  8.76978692e-03, -1.71663567e-01,\n",
            "       -3.95878047e-01,  9.26910415e-02, -4.93029118e-01,  1.75834745e-01,\n",
            "       -1.20706007e-01,  4.42834407e-01,  2.55351245e-01, -2.64668584e-01,\n",
            "        2.75165260e-01, -1.81800067e-01,  1.78831052e-02, -9.53294933e-02,\n",
            "        1.09054558e-01, -1.26502300e-02,  1.07499532e-01, -1.30654141e-01,\n",
            "        5.89182861e-02, -8.86966959e-02,  4.48888510e-01,  1.94978833e-01,\n",
            "       -2.24549636e-01, -6.60058632e-02,  7.35304924e-03, -2.20368966e-01,\n",
            "       -8.56577456e-01, -5.14207967e-02,  1.66951999e-01,  1.62697017e-01,\n",
            "       -7.63342306e-02,  3.85700129e-02,  4.18526493e-03, -2.23291427e-01,\n",
            "       -1.00973837e-01, -5.57204902e-01, -9.81621072e-02, -2.05268711e-01,\n",
            "       -2.02045798e-01,  1.90047175e-01, -4.39944938e-02,  4.13054824e-02,\n",
            "       -2.87971586e-01, -1.01432852e-01, -3.46783966e-01, -1.24906063e-01,\n",
            "       -1.70775145e-01,  3.21497828e-01,  2.95408994e-01, -1.92390993e-01,\n",
            "        9.80078280e-02,  6.91018328e-02,  1.11369696e-02,  1.60131112e-01,\n",
            "        1.38198391e-01, -1.70778316e-02,  8.29737168e-03,  2.91979194e-01,\n",
            "        3.00363809e-01,  1.23249210e-01,  1.38735279e-01,  1.58867821e-01,\n",
            "       -9.94324461e-02,  4.76271585e-02,  6.32209238e-03,  7.53278285e-02,\n",
            "       -7.51021355e-02,  1.83974188e-02,  5.03164172e-01,  1.14665478e-01,\n",
            "       -3.37618649e-01,  2.03387216e-01, -1.29927725e-01, -4.26641732e-01,\n",
            "       -1.97308123e-01,  1.05178401e-01, -2.17319671e-02, -2.05650017e-01,\n",
            "       -2.39724874e-01,  8.79800320e-02,  1.26159772e-01, -1.36293501e-01,\n",
            "       -4.20352863e-03, -4.79434058e-02, -1.57194212e-01,  1.07916869e-01,\n",
            "        3.43399018e-01,  6.17410541e-02,  1.71245903e-01,  5.78974068e-01,\n",
            "       -1.30589604e-01,  8.54463205e-02, -2.46458456e-01, -1.32050082e-01,\n",
            "        2.52947718e-01,  2.18462422e-01,  2.39947677e-01, -9.16239396e-02,\n",
            "        8.12147930e-02,  1.05499908e-01,  3.97592783e-01, -2.35228047e-01,\n",
            "       -1.65582865e-01,  2.57966429e-01, -6.44655675e-02,  2.98891187e-01,\n",
            "       -7.71356001e-02, -9.94719118e-02,  1.72080353e-01,  3.40058319e-02,\n",
            "        8.73525515e-02,  2.06020519e-01,  1.86453685e-01, -3.97281915e-01,\n",
            "        7.07098916e-02,  5.16146898e-01,  5.65192588e-02, -1.80160910e-01,\n",
            "       -9.36831087e-02,  4.47689235e-01,  1.09330714e-01,  4.62545305e-01,\n",
            "       -1.14133954e-01,  1.56461313e-01,  1.34150133e-01,  4.44904059e-01,\n",
            "       -1.57771736e-01, -1.49428189e-01, -2.59526879e-01, -7.04836100e-02,\n",
            "       -3.30499202e-01, -3.95702980e-02, -6.91380277e-02,  7.34941587e-02,\n",
            "       -2.16920987e-01,  2.09752798e-01,  2.03038782e-01,  8.13990831e-02,\n",
            "        4.09488892e-03,  2.08246969e-02,  1.73009895e-02,  2.90552676e-01,\n",
            "        1.14481732e-01,  2.01450452e-01,  1.23840898e-01, -6.70491233e-02,\n",
            "        1.44104555e-01,  1.82479933e-01,  2.57636577e-01, -5.88737190e-01,\n",
            "        1.32421777e-01, -4.18453723e-01,  5.40483557e-02, -3.51492882e-01,\n",
            "        3.71808320e-01, -6.51877373e-02,  5.02203107e-01, -4.87446815e-01,\n",
            "       -1.63031444e-01, -2.41604149e-02,  6.09117709e-02, -4.48639780e-01,\n",
            "       -6.15220666e-02,  3.93959545e-02, -2.95993656e-01,  9.85957161e-02,\n",
            "       -1.48050413e-01, -4.99094911e-02,  3.05217057e-01, -1.24651954e-01,\n",
            "        2.54505634e-01, -1.26945570e-01,  2.22776175e-01,  2.41643012e-01,\n",
            "        3.95223320e-01, -8.32469016e-02,  6.59135878e-02,  1.20308228e-01,\n",
            "       -1.08256221e-01, -8.76110718e-02, -3.55539203e-01,  3.82042021e-01,\n",
            "       -3.71727407e-01, -2.18502760e-01, -1.16751969e-01,  1.13951247e-02,\n",
            "        8.39896053e-02,  1.27141839e-02,  9.16177332e-02, -4.74239439e-01,\n",
            "       -9.52848978e-03,  7.49055296e-02, -1.28556684e-01, -2.06668139e-01,\n",
            "       -2.81249911e-01,  7.27268904e-02,  3.06159221e-02,  3.15033197e-01,\n",
            "        3.45777094e-01, -2.27006853e-01,  2.19045892e-01, -8.79967585e-02,\n",
            "       -2.53260016e-01,  3.68278176e-01,  2.31850281e-01,  1.86382905e-01,\n",
            "        1.98889598e-01,  1.59511656e-01,  8.78379270e-02, -5.77361099e-02,\n",
            "       -1.83052495e-01, -1.42026767e-01, -1.53114706e-01,  1.97999962e-02,\n",
            "        2.58476287e-01, -3.50972891e-01, -2.80165561e-02,  2.03932047e-01,\n",
            "        1.55436814e-01,  2.72435129e-01,  9.05968249e-02,  1.72488034e-01,\n",
            "        1.06427558e-02, -1.10942014e-01, -2.35038355e-01, -8.34353920e-03,\n",
            "        9.68743563e-02, -1.03685938e-01,  5.21993399e-01, -6.84126437e-01,\n",
            "       -2.62777597e-01, -9.63682532e-02, -1.52049407e-01, -1.11516707e-01,\n",
            "        6.18538335e-02,  3.05429585e-02, -9.73858088e-02, -1.81960985e-01,\n",
            "        6.28785491e-02,  5.68949655e-02, -1.37813315e-01, -5.07278629e-02,\n",
            "        2.59822935e-01, -1.55261159e-01, -1.19162038e-01, -6.13731258e-02,\n",
            "       -2.93107599e-01, -1.16509974e-01, -1.51613086e-01,  1.29650414e-01,\n",
            "       -1.69298157e-01, -6.07180297e-02, -6.38040975e-02,  2.97200829e-02,\n",
            "        1.57452181e-01, -1.25321493e-01, -4.56141204e-01, -9.76366773e-02,\n",
            "       -2.13626266e-01,  2.64166743e-01,  2.59389877e-02,  4.02003735e-01,\n",
            "       -2.71843284e-01,  1.67582244e-01, -2.89828122e-01,  1.31531224e-01,\n",
            "        2.92929411e-01,  2.46711418e-01, -2.11176813e-01,  3.57778817e-01,\n",
            "       -2.03181878e-01, -1.17914729e-01, -1.99332926e-03, -2.93861896e-01,\n",
            "        1.60355911e-01,  9.03316513e-02,  3.44422728e-01, -9.41620320e-02,\n",
            "        4.24300641e-01,  1.95954973e-03,  2.85779417e-01,  3.19430798e-01,\n",
            "       -2.28861600e-01, -3.12757641e-02, -1.85615905e-02, -1.94348022e-01,\n",
            "       -4.34147678e-02, -4.70630169e-01, -2.00281721e-02,  2.44831547e-01,\n",
            "       -1.25742480e-01, -1.80377923e-02, -1.46190953e-02, -2.43777916e-01,\n",
            "        2.03171924e-01, -1.94158871e-02, -1.37468725e-01,  1.74515963e-01,\n",
            "       -9.71428398e-03, -2.56184757e-01, -7.16012567e-02,  3.20550114e-01,\n",
            "       -7.87411928e-02, -1.73510879e-01,  5.26327342e-02, -2.53547728e-01,\n",
            "       -2.91244090e-02,  2.59222090e-01,  1.28655121e-01, -3.84121463e-02,\n",
            "        6.17348254e-01, -4.80529755e-01, -2.38667160e-01, -2.34205976e-01,\n",
            "        9.87700745e-02,  1.61448956e-01,  2.86880016e-01,  1.54611707e-01,\n",
            "       -2.09455401e-01,  8.26811567e-02, -2.39911526e-01,  1.73157364e-01,\n",
            "       -1.09628797e-01, -8.54671076e-02,  3.98796238e-02, -4.27550711e-02,\n",
            "       -4.92061555e-01,  1.02648221e-01, -1.97151363e-01,  4.17411804e-01,\n",
            "       -1.81291327e-01,  2.63294041e-01, -8.94365683e-02, -1.45396635e-01,\n",
            "        5.57681084e-01,  2.44775950e-03,  1.27037525e-01,  2.50235200e-01,\n",
            "        1.24236308e-01, -3.37509185e-01, -6.70047104e-02,  2.77049653e-02,\n",
            "       -4.02380824e-02,  1.49370983e-01,  1.27944350e-01, -7.13811293e-02,\n",
            "        4.78137918e-02,  1.12563930e-02, -1.05246156e-01,  5.39797656e-02,\n",
            "        2.34775513e-01, -1.96111962e-01,  2.16279328e-01,  5.34252189e-02,\n",
            "       -3.96288306e-01,  3.45339477e-01, -5.34962714e-01, -5.13830259e-02,\n",
            "        1.96539953e-01,  1.55047819e-01,  1.34443678e-02,  4.54977490e-02,\n",
            "       -2.34959230e-01,  1.82545424e-01, -2.11439282e-01,  1.13729373e-01,\n",
            "       -6.31864443e-02,  2.36254647e-01,  3.23858947e-01,  2.47714534e-01,\n",
            "       -1.37081444e-01,  1.09378628e-01, -9.21890065e-02, -2.77864218e-01,\n",
            "        1.53043158e-02,  2.31961221e-01, -2.44266048e-01,  1.20311514e-01,\n",
            "       -3.83439720e-01, -2.40124166e-01, -2.48693779e-01, -1.75067008e-01,\n",
            "       -3.59246284e-02,  1.06902190e-01,  1.07041821e-01, -2.15280894e-02,\n",
            "        9.81440544e-02, -4.10682708e-02, -2.03404445e-02, -4.61774208e-02,\n",
            "       -1.74957693e-01, -2.99835019e-02,  2.92988449e-01,  8.92125666e-02,\n",
            "       -2.59063900e-01,  5.07637441e-01,  1.02603519e+00,  1.66889504e-01,\n",
            "       -3.14462304e-01,  2.06059352e-01,  5.21215141e-01,  1.61318198e-01,\n",
            "       -1.36379391e-01, -1.24343768e-01, -2.60598928e-01,  3.53831768e-01,\n",
            "       -4.76424158e-01,  7.60831460e-02,  4.09282371e-02, -9.19189453e-02,\n",
            "        8.60339850e-02,  1.80079520e-01,  5.49981818e-02,  3.57722193e-01,\n",
            "        2.53901988e-01, -5.54126725e-02,  9.11549404e-02,  2.90885657e-01,\n",
            "       -1.18378825e-01,  1.63187966e-01,  5.20770550e-01,  1.31167784e-01,\n",
            "       -7.07207918e-02,  6.05603158e-02, -3.93152684e-02,  2.91668754e-02,\n",
            "        2.18566000e-01, -6.53399006e-02, -3.77148092e-01, -1.54911637e-01,\n",
            "       -8.75763670e-02, -4.42266136e-01, -9.23520327e-02, -4.32485878e-01,\n",
            "       -2.58426785e-01, -1.13147423e-01,  5.87948896e-02, -1.88994948e-02,\n",
            "        2.29344964e-01,  4.52769548e-03,  3.05023223e-01,  5.30600071e-01,\n",
            "       -1.77140281e-01,  1.32459864e-01,  3.81340295e-01, -3.19960937e-02,\n",
            "       -7.98611939e-02, -1.46686018e-01,  1.47466287e-01, -3.42903249e-02,\n",
            "        1.51450083e-01, -2.00945616e-01,  1.17927141e-01,  7.30712637e-02,\n",
            "       -9.07027274e-02, -1.96453165e-02,  2.63940036e-01,  4.97163571e-02,\n",
            "       -1.16250338e-02, -1.87985167e-01,  8.06115493e-02,  3.58302236e-01,\n",
            "        1.24135248e-01, -1.36207432e-01,  1.04544382e-03,  6.15768284e-02,\n",
            "       -1.33058026e-01,  5.83218895e-02, -1.54353054e-02,  8.65339637e-02,\n",
            "       -1.48355350e-01,  1.15697190e-01, -4.59806435e-02,  3.55062097e-01,\n",
            "       -1.37775779e-01,  6.92119896e-02, -3.43577117e-01, -1.39186785e-01,\n",
            "        7.60324448e-02,  1.19825572e-01, -5.13793290e-01,  2.57227093e-01,\n",
            "        4.89155315e-02, -1.17980756e-01,  1.09064676e-01,  1.73838176e-02,\n",
            "        4.00066748e-03,  5.84470667e-02, -4.33850408e-01, -3.42951752e-02,\n",
            "       -2.14455172e-01,  2.53878057e-01, -1.89044401e-01,  4.28615138e-02,\n",
            "       -9.97387245e-02,  2.49388382e-01,  1.07573561e-01, -1.39792785e-01,\n",
            "       -1.03473067e-01, -2.51954585e-01,  1.76792786e-01, -1.50786743e-01,\n",
            "        1.41727865e-01, -1.13821216e-01, -2.10039243e-01,  7.86250755e-02,\n",
            "       -6.30944550e-01, -2.88581792e-02, -3.75476062e-01,  1.77741706e-01,\n",
            "       -2.59572625e-01,  1.52942300e-01,  3.55390157e-03,  3.59453494e-03,\n",
            "       -1.93479151e-01,  4.28440392e-01, -2.27816820e-01, -5.16066290e-02,\n",
            "       -1.38815001e-01, -8.50623399e-02, -1.64718673e-01,  9.27661806e-02,\n",
            "        8.23743567e-02, -1.88578367e-01,  1.01448245e-01,  5.05587637e-01,\n",
            "        3.06532592e-01, -5.55344624e-04,  1.62204832e-01,  4.44043130e-01,\n",
            "       -1.78418443e-01, -1.22899197e-01,  9.50548649e-02,  2.76188785e-03,\n",
            "       -2.51665860e-01, -1.59431085e-01, -1.84927225e-01, -2.78461333e-02,\n",
            "        5.09927571e-01,  2.94052273e-01, -1.73916772e-01, -3.23517099e-02,\n",
            "        1.43384323e-01, -1.68088093e-01,  1.18389040e-01,  8.28437135e-02,\n",
            "       -5.80320656e-02, -3.56801562e-02, -2.74486747e-02, -3.75841498e-01,\n",
            "        6.48597896e-04, -3.89852077e-01,  4.24841009e-02, -2.96957105e-01,\n",
            "       -7.08676279e-01,  6.00699000e-02, -1.53146103e-01, -6.71172738e-02,\n",
            "       -7.99050272e-01,  1.69117093e-01,  1.74194679e-01,  5.62879890e-02,\n",
            "        1.63805962e-01,  3.02478552e-01,  3.34393531e-02, -3.12143750e-02,\n",
            "        1.29300579e-01,  1.29802180e-02, -4.83099632e-02,  3.11056077e-01,\n",
            "        1.42349675e-01,  3.20128083e-01,  2.41260305e-02,  2.43414328e-01,\n",
            "        6.12399936e-01,  2.15776160e-01, -3.80197257e-01,  1.47379100e-01,\n",
            "        2.08564371e-01,  6.60986543e-01,  2.24103201e-02,  6.60399254e-03,\n",
            "       -4.90662344e-02,  3.64687115e-01,  4.26501125e-01,  2.83675253e-01,\n",
            "        2.60003179e-01,  1.67852983e-01,  1.82634413e-01,  7.72281438e-02,\n",
            "       -2.58963317e-01,  1.01569034e-02,  1.34771064e-01,  3.46147984e-01,\n",
            "        7.43561983e-02, -1.17822982e-01, -3.07541396e-02, -6.34037256e-02,\n",
            "       -1.42102480e-01,  5.56880161e-02, -1.41869113e-01,  1.41205922e-01,\n",
            "       -2.11856678e-01,  1.73847497e-01,  1.41825348e-01, -2.90753216e-01,\n",
            "       -6.24994151e-02,  2.72626340e-01,  6.85344487e-02, -2.00388700e-01,\n",
            "        7.62401223e-02, -5.59274014e-03, -5.93913235e-02,  4.83377501e-02,\n",
            "       -1.56641826e-02, -6.06692471e-02,  1.11498937e-01, -1.18334241e-01,\n",
            "       -2.51725763e-01,  1.51092604e-01,  2.96288610e-01,  1.33338809e-01,\n",
            "        6.90744221e-01, -2.80998973e-03,  1.77187860e-01, -3.46421264e-02,\n",
            "       -3.07273656e-01, -9.77279767e-02, -1.69587255e-01, -3.33006769e-01,\n",
            "       -1.06383815e-01, -1.37150064e-01,  2.43794881e-02, -1.38637805e-02,\n",
            "        1.24704696e-01,  5.10343909e-01, -2.49516398e-01,  4.22052145e-01,\n",
            "       -9.41766351e-02, -2.35010400e-01, -3.02714825e-01,  8.42635185e-02,\n",
            "       -6.23337589e-02,  3.54943909e-02,  3.08084488e-02, -2.53202736e-01,\n",
            "       -1.16392389e-01,  2.40900666e-02,  3.25620592e-01, -3.93096730e-02,\n",
            "       -9.80640650e-02, -2.66787499e-01,  1.47006825e-01,  3.66512179e-01,\n",
            "       -4.48731542e-01, -2.80621275e-02, -8.56493488e-02,  2.09136695e-01,\n",
            "        4.72896248e-01,  2.85654273e-02, -9.28747654e-03, -3.56347300e-02,\n",
            "        1.02629334e-01, -5.50288148e-02, -1.38054192e-01,  1.43759176e-01,\n",
            "       -1.27830669e-01,  2.96210825e-01, -2.91332789e-03, -2.08428457e-01],\n",
            "      dtype=float32), array([ 3.68658870e-01,  2.87736714e-01,  1.34914875e-01,  4.06746902e-02,\n",
            "       -3.49985629e-01,  7.93633759e-02,  2.34887972e-01,  4.51667190e-01,\n",
            "        2.02245533e-01, -2.60777295e-01, -4.37686861e-01, -1.54562056e-01,\n",
            "       -2.34514594e-01,  6.56817138e-01, -1.91474929e-02, -4.71996635e-01,\n",
            "        8.22060183e-02, -2.53826261e-01,  4.56779674e-02,  1.95287749e-01,\n",
            "       -1.26180381e-01, -3.27256471e-02, -4.40591902e-01, -4.41516601e-02,\n",
            "        1.48551479e-01,  7.68867880e-02, -7.66820759e-02, -1.92973375e-01,\n",
            "       -7.05920219e-01,  9.27539840e-02,  1.19440533e-01, -1.73013091e-01,\n",
            "       -2.46790022e-01,  1.68457747e-01,  1.88094854e-01, -1.38047814e-01,\n",
            "        2.28066221e-01, -7.07545578e-02, -6.31645739e-01,  5.14777936e-02,\n",
            "        4.33502764e-01,  3.85545015e-01, -1.88373357e-01,  1.32732809e-01,\n",
            "       -3.99182379e-01, -3.54292542e-01, -3.36840034e-01,  6.36602938e-01,\n",
            "        1.10884540e-01, -1.27703771e-01, -5.13096340e-04,  2.04840168e-01,\n",
            "       -1.85394928e-01,  1.16212666e-03,  5.71246929e-02, -2.02728305e-02,\n",
            "       -2.34303456e-02, -8.40481520e-02,  1.91131532e-01, -4.51730192e-02,\n",
            "        4.82003033e-01, -3.28649431e-01,  8.63934234e-02,  5.79326034e-01,\n",
            "        8.51536542e-02, -5.77587634e-04,  1.47077039e-01,  6.96807578e-02,\n",
            "        6.12276383e-02,  1.35512054e-01, -1.60620019e-01, -3.49500060e-01,\n",
            "        6.15053892e-01,  1.51511908e-01,  3.45337093e-01,  4.37490404e-01,\n",
            "        2.89446443e-01,  1.69764891e-01, -3.10657233e-01,  5.21116138e-01,\n",
            "        2.22051874e-01, -3.52195382e-01,  6.94959581e-01,  4.78307940e-02,\n",
            "        3.96207452e-01, -3.21359158e-01,  1.68110758e-01, -1.65899768e-01,\n",
            "        3.05908978e-01,  7.43434625e-03, -1.11095756e-02, -2.90840894e-01,\n",
            "       -2.53664583e-01, -5.94714843e-03, -2.07319111e-02, -2.04074785e-01,\n",
            "       -3.26201737e-01, -2.16888309e-01,  9.71354768e-02, -2.94062346e-01,\n",
            "       -2.00973898e-01, -8.59544426e-03, -2.48389333e-01,  2.99998939e-01,\n",
            "        7.94409364e-02,  2.98496127e-01, -4.99630183e-01, -6.28452241e-01,\n",
            "       -4.86178696e-03, -7.15427697e-02,  9.00803655e-02, -1.96914196e-01,\n",
            "       -3.53226736e-02,  6.05535172e-02, -1.36828139e-01, -9.25644115e-03,\n",
            "        2.49793380e-01, -2.40448892e-01, -7.92597383e-02, -9.89923254e-03,\n",
            "        1.14559054e-01,  1.88519478e-01, -5.04691660e-01,  4.75665629e-02,\n",
            "       -3.61420810e-01,  7.58625329e-01, -1.73996001e-01,  3.75266850e-01,\n",
            "        2.50855647e-02, -6.67058304e-02,  5.46985455e-02, -2.26910263e-01,\n",
            "       -9.29396451e-02, -3.66846114e-01,  1.57199875e-01, -7.63072222e-02,\n",
            "        4.30754542e-01,  1.13404967e-01, -2.24096581e-01, -2.33300269e-01,\n",
            "       -1.74276561e-01, -3.41217726e-01,  2.82126870e-02, -3.10737550e-01,\n",
            "       -1.85473323e-01,  3.50441076e-02,  2.17394799e-01, -5.89037687e-03,\n",
            "        4.23280634e-02, -1.80155203e-01,  6.21688329e-02, -1.43614516e-01,\n",
            "       -7.06207380e-02, -6.69791251e-02,  3.61213386e-01,  4.24503177e-01,\n",
            "        4.52378035e-01, -4.74423587e-01,  2.97684759e-01,  4.72069420e-02,\n",
            "        7.58609250e-02, -1.32407889e-01, -2.91268259e-01, -3.11804742e-01,\n",
            "        2.76948899e-01,  1.84987754e-01,  1.83275983e-01,  7.35151246e-02,\n",
            "       -2.00852767e-01,  2.17776783e-02,  5.80391139e-02,  2.06941348e-02,\n",
            "       -3.96477520e-01, -1.21906623e-01, -1.71607360e-01, -4.60510999e-02,\n",
            "        2.66966969e-02,  1.09238334e-01,  3.13487291e-01, -2.70912826e-01,\n",
            "       -8.48399252e-02, -2.30075002e-01,  6.64124191e-02, -3.49965125e-01,\n",
            "        1.03543907e-01,  7.84564853e-01, -2.74204165e-01,  6.61413074e-01,\n",
            "       -3.82465065e-01,  8.72009993e-02, -3.11712265e-01,  2.77495891e-01,\n",
            "       -1.69984624e-02,  2.67832160e-01,  3.18470418e-01, -5.54033220e-01,\n",
            "       -4.52760547e-01, -6.72226772e-02,  4.09066558e-01, -1.55460060e-01,\n",
            "       -5.24520874e-01,  3.27174276e-01, -1.77323550e-01, -1.70742045e-03,\n",
            "       -2.20273435e-01, -4.71326225e-02,  4.63231176e-01,  3.70515361e-02,\n",
            "        5.07177785e-03,  3.41194361e-01,  1.48242533e-01,  6.94817662e-01,\n",
            "        1.76206663e-01,  4.24030423e-01,  2.00644881e-01, -1.70393288e-01,\n",
            "        3.08412999e-01,  1.08601344e+00, -1.75844193e-01,  3.27536792e-01,\n",
            "        2.34692395e-02,  3.90237510e-01,  3.15865904e-01, -2.44514018e-01,\n",
            "        8.38983297e-01, -5.32034636e-01,  4.73588072e-02, -9.10711884e-02,\n",
            "       -2.32764825e-01, -4.75856334e-01, -8.24977607e-02, -2.09867924e-01,\n",
            "        5.70674241e-02,  6.71986043e-02,  2.35497996e-01,  7.61492848e-01,\n",
            "       -1.07270010e-01,  1.71559602e-01,  9.47448164e-02,  7.04584047e-02,\n",
            "        1.12334475e-01, -6.78176135e-02,  3.32493633e-02, -2.39603192e-01,\n",
            "       -1.80142760e-01,  3.45273137e-01,  2.07707554e-01, -5.80046177e-01,\n",
            "       -7.60194585e-02, -6.23056054e-01, -2.66454786e-01, -3.82233649e-01,\n",
            "        1.48718148e-01, -9.55029950e-02,  6.92725301e-01, -4.68698949e-01,\n",
            "       -1.34179205e-01,  2.20581830e-01, -1.30794808e-01,  2.61240929e-01,\n",
            "       -1.66545704e-01,  2.76761919e-01,  4.43487346e-01,  1.82131037e-01,\n",
            "        5.93892224e-02,  6.39348030e-01, -2.14329898e-01, -5.40509298e-02,\n",
            "        1.85577512e-01,  3.12035441e-01,  1.34363577e-01,  1.78655922e-01,\n",
            "        4.93690401e-01, -1.60573006e-01, -1.36311725e-01,  2.74939418e-01,\n",
            "        3.40721786e-01,  1.58304065e-01, -1.59258008e-01,  3.93875957e-01,\n",
            "       -3.17701459e-01, -3.10964525e-01, -2.76398450e-01, -2.42875874e-01,\n",
            "        4.17194963e-01,  2.27925301e-01, -1.39707834e-01, -4.72401738e-01,\n",
            "        1.41442299e-01,  1.23036355e-01,  1.48121476e-01,  2.97253281e-02,\n",
            "       -3.82320434e-01, -3.44741680e-02, -1.84763193e-01, -4.71798778e-02,\n",
            "       -1.69256032e-02, -2.47559965e-01,  1.08585127e-01, -1.21627927e-01,\n",
            "       -6.71841502e-01,  3.57410014e-01,  1.47990078e-01,  2.00050622e-01,\n",
            "       -1.49836555e-01,  1.15174256e-01,  5.76138854e-01,  3.43430787e-02,\n",
            "       -2.41793096e-01, -4.69488710e-01, -2.99621254e-01, -2.30183993e-02,\n",
            "        5.41466512e-02, -1.14182033e-01,  3.47606502e-02,  3.18355381e-01,\n",
            "        1.13967508e-01,  2.47078642e-01, -1.70396820e-01, -3.77158999e-01,\n",
            "        1.33206978e-01,  3.33481848e-01, -2.74759948e-01,  8.93774852e-02,\n",
            "        5.44189692e-01, -3.03679019e-01,  5.27936578e-01, -4.00425017e-01,\n",
            "        7.88979083e-02,  7.51813427e-02,  7.15398669e-01,  3.79564434e-01,\n",
            "        1.20258197e-01,  6.24415167e-02,  8.49090219e-02,  6.35571172e-03,\n",
            "        3.13689560e-01,  5.15580317e-03, -1.70284882e-01,  4.63799313e-02,\n",
            "       -4.64337841e-02, -4.95948792e-02, -1.95385441e-01, -2.86506146e-01,\n",
            "       -1.27601430e-01,  7.13744983e-02,  8.80566090e-02, -8.92163441e-02,\n",
            "        1.76406652e-01, -2.89378464e-01,  8.85528922e-02, -8.06234330e-02,\n",
            "        4.05633628e-01,  5.68032973e-02, -5.82851827e-01, -2.26248093e-02,\n",
            "       -2.51666456e-01, -3.10605109e-01, -3.43478024e-02,  4.36506420e-01,\n",
            "       -7.99132362e-02,  3.45346302e-01, -3.25397372e-01, -6.62606955e-02,\n",
            "        8.71762410e-02,  2.06517309e-01,  1.62269443e-01,  2.83219278e-01,\n",
            "       -1.16090953e-01, -2.90749241e-02, -3.99052650e-02, -4.24462974e-01,\n",
            "        1.07054338e-01, -1.58467039e-01,  9.02565271e-02,  4.23768312e-02,\n",
            "        3.68479013e-01, -7.19590336e-02,  1.38884559e-01,  1.72944248e-01,\n",
            "       -3.24506789e-01, -2.42288932e-01, -8.46536905e-02, -1.24938846e-01,\n",
            "       -3.35137807e-02, -3.69018942e-01,  8.86621233e-03,  1.26738578e-01,\n",
            "        5.94859004e-01, -1.93489075e-01, -4.40431722e-02,  1.66520014e-01,\n",
            "       -2.08121106e-01, -1.38920546e-01,  4.23034042e-01, -1.02393538e-01,\n",
            "       -4.12971526e-01, -1.85584635e-01, -2.79272497e-01,  4.56024148e-02,\n",
            "        2.98736036e-01, -3.34554106e-01, -4.63009216e-02,  2.66856164e-01,\n",
            "       -3.75296324e-01,  3.58349502e-01,  1.83089018e-01, -2.13272020e-01,\n",
            "        6.31897628e-01, -4.15687487e-02,  6.79680929e-02,  7.17952788e-01,\n",
            "       -7.56321773e-02, -3.85211501e-03, -1.52541235e-01,  6.71694279e-02,\n",
            "       -3.67863894e-01, -3.06911059e-02, -7.27656931e-02, -5.67198098e-01,\n",
            "        1.74406514e-01,  7.37451762e-02,  7.81027973e-02,  1.30359381e-01,\n",
            "       -1.35895938e-01,  2.68156976e-01, -2.06140019e-02,  3.76268148e-01,\n",
            "       -5.85414767e-01,  1.44537568e-01,  3.23177069e-01, -2.70481586e-01,\n",
            "        2.99207389e-01, -4.78620753e-02,  1.79893225e-02, -2.37206854e-02,\n",
            "       -5.25551066e-02,  1.56206302e-02, -3.22972208e-01, -2.08539397e-01,\n",
            "        1.82102084e-01,  3.84713024e-01, -1.13525085e-01, -5.44339418e-01,\n",
            "       -2.35933289e-02,  3.73610854e-01, -3.33802283e-01, -7.87000805e-02,\n",
            "        2.56244004e-01, -2.43567050e-01,  1.34475201e-01, -4.49264795e-01,\n",
            "       -3.82780850e-01,  1.93422303e-01, -2.23989964e-01, -1.52317137e-01,\n",
            "        1.08120725e-01,  3.30468059e-01,  1.13233095e-02, -1.32125556e-01,\n",
            "        5.08808754e-02,  1.36689439e-01, -1.45134956e-01, -2.45228022e-01,\n",
            "       -2.49885529e-01,  7.03978911e-02,  9.01856795e-02,  3.77793700e-01,\n",
            "        2.47409344e-01, -4.29737605e-02, -2.73785964e-02, -3.27570856e-01,\n",
            "       -6.39501493e-03,  3.37618172e-01,  7.52070248e-02,  6.04848526e-02,\n",
            "       -2.02763706e-01, -3.09005886e-01, -1.42632082e-01,  4.40134928e-02,\n",
            "        1.89309806e-01,  1.27468273e-01,  2.85668194e-01, -2.79187828e-01,\n",
            "       -3.13321054e-01,  2.10015163e-01, -1.70027375e-01, -5.23127690e-02,\n",
            "        2.50637159e-03,  1.93086609e-01, -3.77322465e-01,  1.41968150e-02,\n",
            "       -2.06047297e-02,  1.22899398e-01,  7.16751158e-01,  1.95497826e-01,\n",
            "       -4.03721064e-01,  6.93048358e-01,  4.94164109e-01,  3.92864309e-02,\n",
            "       -1.92512050e-01, -1.11036472e-01,  1.26412317e-01, -5.17395020e-01,\n",
            "       -2.14806199e-01, -9.06869620e-02,  1.18402213e-01,  5.63913174e-02,\n",
            "        1.86454445e-01,  1.59614652e-01, -2.50828974e-02, -1.36493176e-01,\n",
            "        1.92819729e-01,  1.24489270e-01,  2.90393740e-01,  3.17311019e-01,\n",
            "        3.39361966e-01, -7.85109252e-02, -3.93991955e-02,  4.65398461e-01,\n",
            "        2.18271926e-01,  2.44019747e-01, -5.05608208e-02,  3.20635252e-02,\n",
            "       -9.29897875e-02,  1.16614446e-01, -4.37992662e-01, -4.31343392e-02,\n",
            "        1.40528008e-01, -2.93691188e-01,  9.59034488e-02, -2.05161244e-01,\n",
            "        1.39130801e-01, -2.11627319e-01,  3.32728833e-01,  1.08773954e-01,\n",
            "        3.25899482e-01,  1.90337569e-01,  3.43614630e-03,  4.51244786e-02,\n",
            "       -1.88339919e-01, -1.42284065e-01,  2.80357122e-01, -1.27422512e-01,\n",
            "       -2.53628433e-01, -4.59102988e-01, -1.27240807e-01, -1.16696224e-01,\n",
            "        2.18811765e-01,  1.65818892e-02, -3.29608142e-01,  3.83855402e-01,\n",
            "       -9.48337018e-02,  3.81213129e-01, -3.61985490e-02, -1.97327033e-01,\n",
            "       -1.50011867e-01, -3.11313957e-01, -1.88849300e-01,  5.81674755e-01,\n",
            "        2.18048245e-01, -5.63323021e-01, -9.40135047e-02,  2.76221037e-01,\n",
            "        1.24938540e-01, -1.26935840e-02,  2.09691674e-01,  5.11592217e-02,\n",
            "        2.98632324e-01,  4.18348491e-01,  4.38666105e-01,  1.26049012e-01,\n",
            "       -1.65207237e-01,  3.89733940e-01,  1.79479763e-01,  1.78280130e-01,\n",
            "        2.37370312e-01, -4.23099756e-01, -1.52948827e-01,  3.38247716e-02,\n",
            "        4.14100111e-01, -3.14458966e-01, -3.14880520e-01, -2.46734187e-01,\n",
            "        2.29747683e-01,  3.63544598e-02, -3.72227788e-01, -7.76288956e-02,\n",
            "        1.38762563e-01, -1.12383962e-01,  5.84176704e-02,  1.70900654e-02,\n",
            "       -1.88188225e-01, -6.16658255e-02,  5.81795685e-02, -1.88389003e-01,\n",
            "        3.09170514e-01, -2.35828385e-01, -6.18072487e-02,  3.25202554e-01,\n",
            "        2.31807649e-01, -9.17187855e-02, -9.22403038e-02, -1.38448730e-01,\n",
            "       -1.67417139e-01, -9.71844569e-02,  4.57033888e-02,  3.64981070e-02,\n",
            "       -2.53821403e-01, -2.10693866e-01, -3.55646312e-01,  8.96791667e-02,\n",
            "       -8.85482207e-02,  2.14717761e-01, -8.01844671e-02,  4.88955855e-01,\n",
            "        1.81753173e-01, -3.50439176e-02, -2.52072543e-01,  3.24768871e-01,\n",
            "        2.76056111e-01, -1.49604470e-01, -8.58977288e-02,  2.46605456e-01,\n",
            "        1.99123576e-01,  7.09568560e-02,  7.98935592e-02,  1.06217735e-01,\n",
            "       -1.72610164e-01, -4.27518673e-02, -2.48535529e-01, -2.66696483e-01,\n",
            "        7.69931376e-02, -1.05289929e-01,  4.54268694e-01,  1.08086199e-01,\n",
            "        2.22662196e-01,  2.37459451e-01,  7.94707797e-04,  1.86697036e-01,\n",
            "        1.16221659e-01,  4.72971886e-01, -2.30478078e-01,  3.97424474e-02,\n",
            "        7.87257999e-02,  2.49804959e-01,  8.64901841e-02, -1.54895484e-01,\n",
            "       -2.51838379e-03, -2.70550586e-02,  1.51125327e-01,  5.76360762e-01,\n",
            "       -6.26897395e-01,  1.06794201e-01, -1.45123079e-01,  2.42555186e-01,\n",
            "       -4.39659506e-02,  1.56911805e-01, -1.05883613e-01,  2.15485338e-02,\n",
            "        1.50684178e-01,  7.50725865e-02,  9.47652683e-02,  2.08957464e-01,\n",
            "        1.07247055e-01,  4.18717951e-01,  2.20547661e-01,  1.43943682e-01,\n",
            "       -2.02091224e-02, -1.38742581e-01, -1.21977940e-01,  1.51701286e-01,\n",
            "        1.20341182e-01,  3.28979850e-01, -3.66289914e-02, -1.83239058e-01,\n",
            "        7.11818099e-01,  3.44269127e-01, -1.73846781e-01,  1.58954650e-01,\n",
            "       -3.62989008e-01, -1.79796759e-02,  4.14111793e-01,  3.54620218e-01,\n",
            "       -4.94210571e-02, -1.80118643e-02,  8.84587765e-02,  5.94131798e-02,\n",
            "        3.95955220e-02, -6.33824691e-02, -2.27452040e-01,  3.90422285e-01,\n",
            "       -2.37194598e-02, -1.87282994e-01, -5.00606716e-01, -2.01209448e-02,\n",
            "        8.95420462e-02,  1.48493305e-01, -2.61363745e-01, -3.18996400e-01,\n",
            "       -3.97797048e-01,  3.51833344e-01, -5.20804413e-02, -2.88115025e-01,\n",
            "       -3.07712913e-01,  3.19370031e-01,  6.32557347e-02, -1.54607594e-01,\n",
            "       -6.90998062e-02, -5.67685366e-01, -2.00153455e-01,  2.65815377e-01,\n",
            "       -1.72837377e-02,  3.86265740e-02, -2.51449883e-01,  3.60478908e-01,\n",
            "       -1.35233894e-01,  1.80953503e-01,  1.52499050e-01,  3.18899423e-01,\n",
            "        5.31257272e-01, -4.08546984e-01,  2.73326665e-01,  6.52396977e-02,\n",
            "       -4.59959954e-01, -1.48989677e-01, -2.22041130e-01, -1.26093641e-01,\n",
            "        2.35139549e-01, -4.36110198e-02, -6.68041110e-02,  1.95544153e-01,\n",
            "        4.25405025e-01,  8.55390076e-03, -2.19498366e-01,  1.02755278e-01,\n",
            "        3.52696553e-02, -5.53062782e-02, -3.22124004e-01, -2.61705279e-01,\n",
            "       -4.17985857e-01, -8.35764930e-02, -2.57673562e-02, -1.42817885e-01,\n",
            "       -6.24754429e-02,  6.88418448e-02,  9.52066407e-02,  6.22045137e-02,\n",
            "       -5.20646274e-01, -9.00290161e-03,  1.22861624e-01,  4.20381725e-01,\n",
            "       -1.24097846e-01, -2.26165324e-01, -9.02357176e-02,  4.33278620e-01,\n",
            "        6.45584822e-01, -6.58659711e-02, -2.23285496e-01, -1.19242832e-01,\n",
            "        2.26390272e-01,  4.26574796e-02, -1.19671308e-01, -1.07651591e-01,\n",
            "       -4.42295134e-01,  2.77568340e-01,  2.16617897e-01,  5.81613928e-02],\n",
            "      dtype=float32), array([ 6.86390176e-02, -5.94410188e-02,  6.77483855e-03,  6.44372344e-01,\n",
            "       -5.73421776e-01,  4.00697857e-01,  2.54843801e-01,  1.29434407e-01,\n",
            "        3.64528537e-01,  1.53599709e-01,  6.69269711e-02, -3.43470603e-01,\n",
            "        1.88351981e-02, -3.73047590e-02, -2.62962908e-01, -3.95782202e-01,\n",
            "       -3.26035708e-01, -2.77760983e-01,  7.81074073e-03,  1.10133559e-01,\n",
            "       -2.19820775e-02,  2.10377768e-01, -1.68253168e-01, -2.97595680e-01,\n",
            "        3.19196463e-01, -2.20885724e-01,  1.55318201e-01, -2.16150418e-01,\n",
            "       -2.96463162e-01,  1.81179821e-01, -8.25500414e-02, -7.92516693e-02,\n",
            "       -1.96549609e-01,  3.19586754e-01, -1.11389779e-01,  1.07936226e-01,\n",
            "       -1.26117095e-01,  1.22025676e-01,  4.43996303e-02,  7.84874558e-02,\n",
            "        1.67966112e-01,  7.51922727e-02,  2.24932224e-01,  3.89736533e-01,\n",
            "       -3.18788379e-01, -3.77626270e-01, -4.27380294e-01, -4.11542095e-02,\n",
            "        2.00914264e-01, -1.40010685e-01,  3.23962957e-01, -9.99446493e-03,\n",
            "       -3.81115787e-02, -7.80476779e-02, -8.57751146e-02,  1.09078020e-01,\n",
            "       -1.19696409e-02,  2.71965772e-01, -1.39223173e-01, -2.38347277e-01,\n",
            "        2.33550906e-01, -2.87603259e-01, -6.10920899e-02,  6.44343019e-01,\n",
            "        2.08578259e-02, -1.30535468e-01, -3.87135185e-02,  5.13874531e-01,\n",
            "       -9.56867635e-02,  3.58636342e-02, -1.00813799e-01, -2.96624959e-01,\n",
            "        2.23149419e-01, -2.35540688e-01,  1.50883317e-01,  5.86831093e-01,\n",
            "        1.98354855e-01, -2.47528583e-01, -2.23760307e-01,  4.01675731e-01,\n",
            "        2.19813008e-02, -2.22547323e-01,  9.68389958e-02,  2.37714261e-01,\n",
            "       -1.42463312e-01,  3.34702790e-01, -3.06860749e-02,  2.72205293e-01,\n",
            "        2.77144909e-01,  2.16316134e-01, -1.31281272e-01,  4.95275063e-03,\n",
            "       -4.95222241e-01, -4.72577900e-01,  2.20884502e-01, -2.43839309e-01,\n",
            "       -1.87304154e-01, -7.83503175e-01, -2.92201668e-01, -3.20628971e-01,\n",
            "       -1.85846210e-01, -5.03345616e-02, -8.63868177e-01,  4.15781677e-01,\n",
            "        1.14475109e-01,  4.03594337e-02,  2.94154286e-01, -9.49649215e-02,\n",
            "       -1.39910519e-01, -3.68175864e-01, -1.16991483e-01, -4.13629860e-02,\n",
            "       -4.81048971e-02,  7.05008209e-03,  1.26272827e-01, -3.83865982e-01,\n",
            "       -4.07257527e-02, -5.98581210e-02, -2.69819111e-01, -1.01099417e-01,\n",
            "        1.53970897e-01,  8.32358748e-02, -1.04331277e-01,  9.10409689e-02,\n",
            "       -5.74913800e-01,  1.04876883e-01,  1.15513742e-01, -1.77877232e-01,\n",
            "       -1.84639454e-01,  9.29782614e-02, -8.38252306e-02,  1.62511095e-01,\n",
            "       -2.57837802e-01, -4.27079171e-01,  3.63064371e-02, -8.46023858e-03,\n",
            "        8.15425292e-02,  5.16250031e-03, -1.52097881e-01, -9.87730846e-02,\n",
            "       -1.61088914e-01,  9.72548798e-02,  3.92797664e-02,  7.65344268e-03,\n",
            "        3.27987154e-03, -1.93718508e-01, -2.29134727e-02, -1.09043710e-01,\n",
            "        7.31610879e-02, -4.40956615e-02, -1.67909250e-01,  2.07777053e-01,\n",
            "        4.92413947e-03, -1.90907538e-01, -5.62488381e-03,  3.54559161e-02,\n",
            "        3.25398922e-01, -1.97320357e-01,  4.76612523e-02, -7.58863315e-02,\n",
            "       -9.48083922e-02,  2.06498593e-01, -1.96616828e-01,  3.00428420e-01,\n",
            "        3.14682722e-01, -5.82161881e-02,  1.65153995e-01,  2.20724031e-01,\n",
            "       -1.97018042e-01,  7.24045485e-02,  3.47010083e-02, -4.77448106e-01,\n",
            "       -1.80106714e-01, -3.57588492e-02, -3.28709096e-01,  1.39931306e-01,\n",
            "       -9.14581940e-02,  9.42253917e-02,  2.34249294e-01, -3.18805873e-02,\n",
            "        1.77233472e-01, -9.44701135e-02, -1.81541815e-01, -2.38063976e-01,\n",
            "        2.02101678e-01,  4.38675433e-01,  4.48432602e-02,  1.00068331e+00,\n",
            "       -5.73890388e-01, -3.06960614e-03, -7.25869238e-02,  2.21797183e-01,\n",
            "        1.82332820e-03, -3.11786592e-01,  4.31049347e-01, -4.58123207e-01,\n",
            "       -6.39599189e-03, -6.90642074e-02,  3.46351296e-01, -3.43123853e-01,\n",
            "       -2.95006223e-02,  2.63690084e-01,  2.53125250e-01,  1.28701046e-01,\n",
            "        4.32877958e-01, -8.55037943e-02,  2.30569065e-01,  1.89697981e-01,\n",
            "        2.59528369e-01,  2.51821160e-01,  7.79859871e-02,  3.63425791e-01,\n",
            "        8.35133269e-02,  5.84411860e-01, -6.84484392e-02, -2.55994201e-01,\n",
            "        7.39014000e-02,  7.76936173e-01,  1.06231228e-01,  2.89732546e-01,\n",
            "       -1.70997351e-01, -7.86153376e-02,  4.46985699e-02, -9.20867641e-03,\n",
            "        5.52583039e-01, -4.92953099e-02,  1.28334686e-01, -2.40278821e-02,\n",
            "       -1.11193419e-01, -1.48561433e-01,  4.34223078e-02,  1.95009738e-01,\n",
            "       -2.29515672e-01,  2.09730148e-01,  1.21220201e-01, -2.68095076e-01,\n",
            "       -1.70746282e-01,  3.15727681e-01, -8.81179199e-02,  6.64254352e-02,\n",
            "       -3.99917103e-02,  7.84747209e-03, -4.82417718e-02,  1.01279557e-01,\n",
            "        1.45997405e-01,  3.75642538e-01,  1.56118512e-01, -8.47905457e-01,\n",
            "        1.12526685e-01, -1.08623397e+00,  3.79755385e-02, -4.76509780e-01,\n",
            "       -9.99483094e-02, -5.62167056e-02,  8.57463419e-01, -4.30874914e-01,\n",
            "       -1.53771073e-01,  3.54370885e-02,  1.50037318e-01, -1.13742724e-01,\n",
            "       -2.12211475e-01,  2.89733857e-01, -8.12262073e-02,  3.52180839e-01,\n",
            "        9.57434252e-02,  4.91541803e-01, -2.02288553e-01, -4.43055540e-01,\n",
            "        2.62930006e-01,  1.20157726e-01,  3.30928475e-01,  7.82173052e-02,\n",
            "        3.16776156e-01, -3.83887261e-01,  2.75189336e-02,  1.73008889e-01,\n",
            "       -2.27167625e-02, -1.59356490e-01, -1.06733412e-01,  2.71152347e-01,\n",
            "       -2.78567255e-01, -2.13326260e-01, -2.26952672e-01, -2.73027942e-02,\n",
            "        1.60721853e-01, -1.43980905e-01,  6.85153157e-02, -6.85187340e-01,\n",
            "        2.19133839e-01,  6.94616511e-02,  1.24427229e-01, -6.68119192e-02,\n",
            "       -5.22687852e-01,  3.33729774e-01,  2.34152582e-02,  3.44894856e-01,\n",
            "        2.81915255e-02, -3.45731676e-02,  3.46143812e-01, -2.26780817e-01,\n",
            "       -1.18500918e-01,  2.96155214e-01,  2.11259216e-01, -5.17523102e-02,\n",
            "       -8.73210654e-02,  2.63608873e-01,  4.09887761e-01,  4.65552248e-02,\n",
            "       -5.34172595e-01, -3.12435806e-01, -6.81322664e-02, -3.70069593e-02,\n",
            "        2.21051440e-01, -1.91975430e-01,  9.75312144e-02,  3.65329623e-01,\n",
            "        5.18382907e-01,  2.09460363e-01, -1.29937768e-01, -5.06702103e-02,\n",
            "        6.68107495e-02,  2.94653445e-01,  4.11455818e-02,  7.13222250e-02,\n",
            "        4.14611459e-01,  5.70852775e-03,  2.85233557e-01, -6.73216045e-01,\n",
            "        1.94107726e-01, -1.74359560e-01,  4.74348426e-01, -1.66634824e-02,\n",
            "       -1.61104321e-01,  2.18465686e-01,  9.81136784e-02,  2.19827145e-01,\n",
            "        2.04230115e-01,  4.78669293e-02, -1.70568943e-01, -1.70804799e-01,\n",
            "        2.94623852e-01, -2.75723655e-02, -4.07695353e-01,  1.04622357e-01,\n",
            "       -4.29787725e-01, -1.39445886e-01, -2.83899546e-01,  2.45418604e-02,\n",
            "       -9.63682383e-02, -8.60480964e-02, -4.65959311e-03,  2.10709929e-01,\n",
            "       -8.20621699e-02, -2.50668436e-01, -2.90024728e-01, -1.99287370e-01,\n",
            "       -2.57467359e-01, -1.47362286e-03, -3.82015795e-01,  4.51822905e-03,\n",
            "        2.07167000e-01, -1.79896936e-01, -4.42639768e-01,  7.43860081e-02,\n",
            "        4.71330851e-01, -1.05566241e-01,  9.35896784e-02, -1.12946257e-01,\n",
            "        1.94565933e-02,  6.72469437e-02, -3.57035212e-02, -2.26022676e-01,\n",
            "        5.13328239e-02, -2.96291798e-01,  1.94453761e-01, -1.07951105e-01,\n",
            "        7.26122176e-03, -3.37718613e-02,  5.33985734e-01,  3.88779258e-03,\n",
            "       -2.56134123e-01,  2.20308304e-01,  4.67899516e-02,  6.17585983e-03,\n",
            "       -4.98932481e-01, -2.44288713e-01, -4.36354168e-02,  3.03888768e-01,\n",
            "        3.64400715e-01, -3.05547565e-02, -1.48425531e-02,  2.35434934e-01,\n",
            "        1.61167420e-02, -5.22212684e-01,  2.17065215e-01,  2.37324223e-01,\n",
            "       -1.64966270e-01, -9.18028474e-01, -2.73182273e-01,  1.91439524e-01,\n",
            "        7.17350617e-02, -2.14102939e-01,  8.46169740e-02,  1.93985268e-01,\n",
            "       -5.76814339e-02,  1.50539041e-01, -1.10093012e-01, -1.84899241e-01,\n",
            "        2.39529029e-01,  2.53584664e-02,  5.36087342e-02,  1.02487393e-01,\n",
            "        4.05472238e-03,  1.96296200e-02,  1.68366939e-01,  8.37624222e-02,\n",
            "        1.71729118e-01,  1.68808307e-02, -2.53416926e-01,  3.14624496e-02,\n",
            "        4.16667104e-01, -5.67377694e-02, -1.08019687e-01,  5.92030846e-02,\n",
            "        1.42330348e-01, -7.57966563e-02, -1.13052607e-01,  2.23195940e-01,\n",
            "        2.74140745e-01,  2.99533784e-01, -1.00622833e-01, -1.12913512e-01,\n",
            "       -2.17148140e-02, -2.36423418e-01, -4.40557450e-02,  4.26825844e-02,\n",
            "       -9.93053913e-02, -1.14511654e-01, -1.87469393e-01, -5.34981601e-02,\n",
            "       -4.46747011e-03,  4.04904261e-02, -2.85178483e-01,  3.07655692e-01,\n",
            "       -2.27604255e-01,  1.58117071e-01, -5.49443543e-01,  1.19843066e-01,\n",
            "        3.71897459e-01, -2.43809983e-01, -1.05552107e-01, -6.51709214e-02,\n",
            "       -5.72835863e-01,  1.11315414e-01, -3.42554510e-01,  1.29025981e-01,\n",
            "        1.66942418e-01,  8.68486390e-02, -1.42077327e-01,  1.96837587e-03,\n",
            "       -2.26632077e-02,  1.00422442e-01, -6.69413134e-02, -2.14686454e-03,\n",
            "       -3.28149378e-01,  1.31446809e-01,  2.04889387e-01,  5.05329575e-03,\n",
            "       -1.99848726e-01,  2.84122080e-02, -7.39877671e-02, -4.03140962e-01,\n",
            "       -1.06072806e-01, -3.76391485e-02, -9.51844361e-03,  3.38822246e-01,\n",
            "       -1.35649517e-01, -1.33009747e-01, -2.08377123e-01, -8.65506530e-02,\n",
            "        1.40327796e-01,  2.28480235e-01, -2.70268947e-01, -2.03047261e-01,\n",
            "       -1.64347976e-01, -3.65595408e-02,  1.18499883e-01,  1.17358483e-01,\n",
            "       -1.93109989e-01,  2.65537091e-02,  9.38972011e-02,  4.89947438e-01,\n",
            "       -6.43998161e-02,  1.41901284e-01,  1.06964743e+00,  6.98505044e-02,\n",
            "       -5.66143990e-01,  6.52168021e-02,  9.89354849e-02, -6.37544468e-02,\n",
            "       -2.69110829e-01, -1.25335185e-02, -7.09516779e-02,  2.09626779e-01,\n",
            "       -2.13022709e-01, -1.52719468e-01,  4.37903777e-03,  9.30411890e-02,\n",
            "        4.54278290e-01, -4.71541807e-02,  2.28601113e-01, -4.54610527e-01,\n",
            "        2.15013362e-02,  2.89541334e-01,  1.40598014e-01,  1.73362359e-01,\n",
            "        2.57150441e-01,  1.51688352e-01,  1.81598276e-01,  2.39446118e-01,\n",
            "        1.08561710e-01,  4.20894533e-01, -1.99626503e-03,  5.62483957e-03,\n",
            "        2.32116506e-02, -2.91747689e-01, -4.10359204e-01, -9.82036442e-02,\n",
            "        1.89344928e-01, -1.94921628e-01,  3.45985070e-02, -4.51742560e-01,\n",
            "       -1.34015203e-01, -2.84996599e-01,  4.42476422e-01,  1.61126301e-01,\n",
            "        5.81327379e-01,  1.48943529e-01,  1.26772478e-01, -3.76696512e-02,\n",
            "        8.45312119e-01,  1.91823587e-01,  5.43804467e-01, -3.52769226e-01,\n",
            "       -3.42782319e-01, -6.46690652e-02, -3.85025620e-01,  9.96706542e-03,\n",
            "        9.64329019e-02, -2.42011905e-01,  1.15918644e-01,  4.69122291e-01,\n",
            "        3.41991819e-02,  3.21460068e-01,  2.24733353e-01,  4.57474068e-02,\n",
            "       -1.19203120e-01,  4.31674458e-02, -7.35003576e-02,  4.55275625e-01,\n",
            "        1.73646212e-01, -5.34595191e-01,  2.02380475e-02,  5.08737341e-02,\n",
            "       -5.88460386e-01, -8.01788494e-02, -2.31066942e-01,  4.17040825e-01,\n",
            "        2.35827878e-01,  1.35908052e-01,  5.34909606e-01,  1.97585970e-01,\n",
            "        1.83929671e-02,  6.29960179e-01,  2.82116711e-01,  6.60669804e-02,\n",
            "        2.11638972e-01,  1.75303128e-02, -1.62086055e-01,  1.97846085e-01,\n",
            "        5.33833146e-01, -7.64978975e-02,  2.52387285e-01, -3.21331382e-01,\n",
            "        2.67278045e-01,  3.16912144e-01,  1.96046785e-01,  2.72523433e-01,\n",
            "        7.06232786e-02,  5.10421358e-02, -2.38878936e-01, -6.76585957e-02,\n",
            "       -2.58378804e-01,  4.30441871e-02,  2.89428383e-02, -6.26538321e-02,\n",
            "       -2.80345231e-01, -2.97799349e-01, -2.25826129e-01,  1.46629944e-01,\n",
            "        2.33023494e-01, -3.16874802e-01, -1.67049412e-02,  1.64346188e-01,\n",
            "        1.33210555e-01, -9.61925089e-02, -2.28358746e-01, -2.20367052e-02,\n",
            "       -1.71791781e-02, -4.37731355e-01,  1.23987406e-01, -3.06390911e-01,\n",
            "       -3.25217545e-01, -6.04077354e-02, -2.97956228e-01,  2.00015590e-01,\n",
            "        1.96567565e-01,  6.01990782e-02, -3.69179636e-01,  2.26308912e-01,\n",
            "       -1.09491289e-01, -4.69700813e-01,  6.17440231e-02,  1.87473074e-01,\n",
            "        5.41599572e-01, -8.73600468e-02,  2.59315461e-01,  4.80694056e-01,\n",
            "       -3.85245279e-04,  1.11250617e-01,  3.30657393e-01, -2.96102285e-01,\n",
            "       -6.57916218e-02, -2.41499431e-02,  5.18732667e-01, -1.62260905e-01,\n",
            "        1.38984799e-01,  2.85031110e-01,  1.41965210e-01,  2.15365291e-01,\n",
            "        2.07732975e-01, -2.02177409e-02, -6.86748549e-02, -3.76590490e-02,\n",
            "        2.85879791e-01,  1.15507655e-01,  3.12688559e-01, -4.74125985e-03,\n",
            "       -4.83062826e-02, -3.39931101e-01, -3.34715396e-02, -7.72485361e-02,\n",
            "       -2.14004174e-01, -3.99391323e-01, -7.45733231e-02,  3.45123202e-01,\n",
            "       -5.04350781e-01,  2.16026127e-01,  2.53707290e-01,  1.45168468e-01,\n",
            "        9.69937593e-02,  4.40110624e-01,  3.14657539e-01,  2.89844751e-01,\n",
            "        4.19293493e-01,  3.58325243e-02, -8.78275856e-02,  1.28345415e-01,\n",
            "        1.41943127e-01, -1.21396951e-01, -1.65197909e-01,  6.48945430e-03,\n",
            "        2.60250598e-01,  4.79989976e-01, -2.89508849e-01,  1.10535026e-01,\n",
            "        2.98664123e-01,  2.96227276e-01, -1.77826092e-01,  1.72506303e-01,\n",
            "       -1.60514355e-01, -2.55383521e-01,  5.25423646e-01, -3.67585160e-02,\n",
            "        5.65959178e-02, -3.17509915e-03,  3.40313524e-01, -3.69748920e-01,\n",
            "        1.80667475e-01, -1.42140254e-01,  1.67629674e-01,  3.17500204e-01,\n",
            "       -2.05144405e-01,  1.00177638e-01,  2.66989142e-01,  8.21878854e-03,\n",
            "       -7.24695176e-02, -6.63797334e-02, -2.94592768e-01, -7.44004473e-02,\n",
            "       -2.15994865e-01,  4.46616411e-02,  1.87143967e-01, -7.21301958e-02,\n",
            "        1.20662851e-02,  2.32011929e-01, -1.99061826e-01, -1.47005215e-01,\n",
            "       -3.91009599e-01, -2.09011793e-01, -7.36470371e-02,  5.75309359e-02,\n",
            "        3.59426826e-01,  3.58797684e-02,  2.91346982e-02,  1.80126369e-01,\n",
            "       -2.65140235e-01,  1.15062296e-01,  5.46789289e-01,  3.37317079e-01,\n",
            "        4.58625644e-01, -7.60841221e-02,  4.01461869e-01, -2.89530866e-02,\n",
            "       -5.37215531e-01, -1.68424159e-01,  1.35405024e-03,  6.10949695e-02,\n",
            "       -5.34322225e-02,  3.72662097e-02, -1.71215162e-02,  1.04811579e-01,\n",
            "        8.61719102e-02,  1.29133254e-01, -2.50253707e-01, -2.59685256e-02,\n",
            "       -2.63682958e-02, -2.49929100e-01,  2.02349406e-02, -2.33443111e-01,\n",
            "        1.04214728e-01,  4.77017947e-02, -4.77974981e-01, -1.74598113e-01,\n",
            "       -2.61152834e-01, -8.53420887e-03,  8.53898004e-02, -4.55070287e-02,\n",
            "       -4.13063645e-01, -5.13378561e-01, -1.97416097e-01, -2.64246076e-01,\n",
            "       -3.38877141e-01,  1.91996142e-01,  2.77290881e-01,  4.86940682e-01,\n",
            "        7.62933433e-01,  2.04524770e-01, -7.88221508e-02, -2.02057734e-01,\n",
            "        3.97423794e-03,  1.82293132e-01, -1.39890999e-01,  5.68823367e-02,\n",
            "        1.42034426e-01,  2.77825445e-01,  4.69035506e-01, -1.56924561e-01],\n",
            "      dtype=float32), array([ 2.41381019e-01,  2.58803546e-01,  4.91345733e-01, -1.09587915e-01,\n",
            "       -5.10892093e-01,  1.49066687e-01,  2.53155529e-01,  4.13446695e-01,\n",
            "        4.56457615e-01,  6.03178553e-02, -3.22660595e-01, -5.35167694e-01,\n",
            "       -3.70097309e-01,  7.89669305e-02, -1.14421003e-01, -8.32678676e-01,\n",
            "        4.04045165e-01,  2.86814839e-01,  1.88277915e-01, -1.03281200e-01,\n",
            "       -4.86048728e-01, -1.17172211e-01, -2.90463418e-01, -3.48867059e-01,\n",
            "        2.99857169e-01,  5.67751192e-03,  2.38316990e-02, -1.37704790e-01,\n",
            "       -4.78524864e-01,  4.07327950e-01,  1.93490624e-01, -1.69140548e-01,\n",
            "       -7.81055242e-02, -1.07743263e-01, -1.87553495e-01,  1.21956579e-02,\n",
            "       -6.54799044e-02, -1.29468530e-01, -8.04190874e-01,  6.11054413e-02,\n",
            "        3.86783600e-01,  2.08633505e-02,  1.48864910e-02,  1.87518582e-01,\n",
            "       -3.77688646e-01,  7.19364733e-02,  2.98421793e-02,  5.81221357e-02,\n",
            "        1.46070465e-01,  9.64699984e-02,  6.50860786e-01,  3.14485937e-01,\n",
            "       -3.61101657e-01,  8.12923610e-02, -3.08090776e-01, -6.11192733e-02,\n",
            "       -9.79224071e-02,  6.51094019e-01, -1.09629862e-01,  1.06205232e-01,\n",
            "        3.02514672e-01, -2.33448386e-01,  1.08327698e-02,  6.52183294e-01,\n",
            "        2.73999304e-01,  4.49314341e-02, -1.28617853e-01,  5.19968569e-01,\n",
            "        2.00952724e-01,  1.91839896e-02,  3.54940683e-01, -2.02582657e-01,\n",
            "        1.44918725e-01, -5.42701721e-01,  5.49217284e-01,  7.89912939e-01,\n",
            "       -3.20255943e-03,  2.06242949e-01, -1.31314710e-01,  6.00166619e-02,\n",
            "        9.86421555e-02, -3.11980247e-01, -5.38750067e-02,  9.77439359e-02,\n",
            "       -9.05827805e-03,  1.07398257e-01,  9.34463926e-03, -3.79344165e-01,\n",
            "        1.86172336e-01, -1.67260721e-01,  3.94015089e-02, -3.41922864e-02,\n",
            "       -3.91396880e-01, -4.52708691e-01,  1.71435326e-01,  2.58359350e-02,\n",
            "       -2.12125987e-01, -5.09793997e-01,  9.44413915e-02, -3.15418869e-01,\n",
            "        1.21109098e-01, -6.13502972e-02, -3.69343497e-02,  4.78657365e-01,\n",
            "        8.50329846e-02, -1.70295104e-01, -5.62111624e-02, -3.77752334e-01,\n",
            "        5.61566465e-02, -2.87433475e-01,  5.90678751e-02,  4.55742627e-02,\n",
            "        1.88365564e-01,  2.34095037e-01,  7.67555386e-02, -3.26138109e-01,\n",
            "       -2.29164939e-02,  3.44872698e-02, -1.30982637e-01, -3.04333642e-02,\n",
            "        6.44500367e-04,  1.71539992e-01,  4.98489514e-02,  2.80237705e-01,\n",
            "       -7.12267756e-01,  6.29902840e-01, -1.79860771e-01, -9.78871509e-02,\n",
            "       -3.25025380e-01,  2.17702612e-01,  1.26499265e-01,  2.98588902e-01,\n",
            "       -1.16783008e-01, -3.83358777e-01,  1.13547258e-01, -1.70891702e-01,\n",
            "       -2.96884589e-02,  1.80638418e-01, -1.05705462e-01, -1.14693478e-01,\n",
            "       -3.16586375e-01, -1.08772174e-01, -2.38275081e-01, -4.83426630e-01,\n",
            "        5.11082292e-01,  1.32923901e-01,  8.43504369e-02, -3.54092643e-02,\n",
            "        1.68355584e-01,  1.61904112e-01, -2.55580992e-01, -3.69382501e-01,\n",
            "        3.60830933e-01, -2.16765434e-01,  1.60508648e-01,  1.88896969e-01,\n",
            "        7.10886002e-01, -3.27739954e-01,  4.54491735e-01,  2.49555677e-01,\n",
            "        2.14305669e-01, -1.92593765e-02, -3.37845892e-01,  6.89959824e-02,\n",
            "       -1.00188076e-01,  2.07924545e-01,  1.20899282e-01,  9.51991230e-02,\n",
            "       -2.68243790e-01, -1.61465690e-01, -8.47928897e-02,  5.98881254e-03,\n",
            "       -2.32519329e-01, -1.60727203e-02, -2.32909933e-01,  2.20245570e-01,\n",
            "       -1.35650942e-02, -2.62253523e-01,  3.42217237e-01, -1.48377687e-01,\n",
            "        3.64496410e-02, -1.17531687e-01,  4.31050435e-02, -9.06286687e-02,\n",
            "       -2.10699998e-02,  2.10059732e-01,  2.53611561e-02,  9.67154145e-01,\n",
            "        8.24612379e-02,  2.88705826e-01,  8.40390753e-03, -1.69942543e-01,\n",
            "       -1.57707348e-01, -2.60717660e-01,  3.54470819e-01, -2.32038200e-01,\n",
            "       -1.17236853e-01, -3.42388824e-02,  2.60406077e-01, -3.99729729e-01,\n",
            "       -1.82652265e-01,  2.48424768e-01,  2.93411404e-01,  2.78454602e-01,\n",
            "        4.18716043e-01, -9.22356397e-02,  4.10237402e-01,  2.30861887e-01,\n",
            "        2.27328062e-01,  1.74604818e-01,  3.52936178e-01,  5.56664348e-01,\n",
            "        2.83740163e-01,  4.00587291e-01,  1.28857106e-01,  5.13973355e-01,\n",
            "        6.26763701e-02,  8.48726869e-01, -3.58783782e-01,  1.07241914e-01,\n",
            "       -1.70141935e-01,  1.79458052e-01,  3.36732090e-01,  1.59640446e-01,\n",
            "        3.11550915e-01,  2.03158841e-01,  1.15538448e-01, -9.69426036e-02,\n",
            "       -2.91337013e-01, -2.20646337e-01,  7.91722629e-03, -2.24161491e-01,\n",
            "        1.15779206e-01,  1.64759398e-01,  5.19200921e-01,  2.22090676e-01,\n",
            "        1.00813955e-01,  2.42262989e-01, -4.91321012e-02,  2.45531112e-01,\n",
            "        5.51109850e-01,  1.73221931e-01,  6.57937452e-02, -2.63021618e-01,\n",
            "       -1.74934089e-01,  1.18062228e-01,  2.57652014e-01, -5.15799880e-01,\n",
            "        2.80226052e-01, -5.24496555e-01, -7.63461813e-02, -5.86890936e-01,\n",
            "       -1.59977555e-01, -1.80724397e-01,  7.43745923e-01, -9.04266119e-01,\n",
            "        6.18047714e-02,  1.61044568e-01,  5.04874110e-01,  3.74598622e-01,\n",
            "        2.44905520e-02,  3.76445085e-01, -2.56334543e-01,  1.77840263e-01,\n",
            "        2.31489211e-01,  6.12952411e-01, -1.97505742e-01, -4.92294848e-01,\n",
            "        7.65649006e-02, -1.14585549e-01,  3.76003310e-02,  4.57754284e-01,\n",
            "        1.56597778e-01, -8.50306079e-02, -4.64176722e-02,  1.33361071e-01,\n",
            "       -5.42934574e-02, -1.43979430e-01, -1.51585221e-01,  1.85548380e-01,\n",
            "       -4.98317719e-01, -9.71474648e-02, -3.00163895e-01,  9.83704552e-02,\n",
            "        3.41313064e-01, -3.51157278e-01,  8.12504366e-02, -7.00613797e-01,\n",
            "        4.78668399e-02,  2.34884799e-01,  1.19635247e-01, -3.32815275e-02,\n",
            "       -1.48222551e-01,  1.83171794e-01,  4.71943505e-02,  1.64525099e-02,\n",
            "        2.64146298e-01, -2.65022695e-01, -1.72943607e-01,  4.30416800e-02,\n",
            "       -1.92514002e-01,  2.33142868e-01,  2.41013348e-01,  2.05363706e-01,\n",
            "       -3.04917283e-02,  4.02517244e-02,  7.69973755e-01,  1.45763591e-01,\n",
            "       -5.35558313e-02, -5.48671111e-02, -2.22995669e-01,  2.05855310e-01,\n",
            "       -2.59971887e-01,  8.89768600e-02,  2.18854379e-02,  1.29682049e-01,\n",
            "        5.10196209e-01, -9.37993228e-02, -9.56232697e-02, -1.30228311e-01,\n",
            "       -4.60464358e-02,  3.39380980e-01, -2.03532323e-01,  1.88948780e-01,\n",
            "       -1.03706129e-01,  6.82499111e-02,  2.22749278e-01, -6.10482693e-01,\n",
            "       -7.97188748e-03, -2.02126633e-02,  1.85887113e-01, -1.30313724e-01,\n",
            "       -1.83122486e-01,  5.71624301e-02, -5.63035905e-02,  4.53264192e-02,\n",
            "        5.91874942e-02,  1.52720138e-02, -1.54971346e-01, -1.58004090e-01,\n",
            "       -2.96508849e-01,  2.10856229e-01, -5.00263691e-01, -1.11896664e-01,\n",
            "       -4.68459398e-01, -1.03507087e-01,  1.24400474e-01,  9.50375795e-02,\n",
            "        2.16309577e-01, -1.98477015e-01,  3.35554600e-01, -7.19712153e-02,\n",
            "        1.68039910e-02,  4.23076391e-01, -8.94471481e-02, -5.93909204e-01,\n",
            "       -1.64771572e-01,  2.92046577e-01, -3.29620764e-02,  5.58822230e-02,\n",
            "        1.50171854e-02,  3.21408778e-01, -2.79096097e-01, -1.40775725e-01,\n",
            "        5.21098413e-02,  3.05956155e-01, -5.55859357e-02, -1.10361084e-01,\n",
            "       -2.21947916e-02, -5.09564430e-02, -1.16215639e-01, -1.71540514e-01,\n",
            "        2.24914283e-01, -1.79204658e-01,  4.23331559e-01, -1.77854523e-02,\n",
            "        1.52031168e-01, -1.06667548e-01,  1.64308861e-01,  8.55125412e-02,\n",
            "       -4.35817540e-01, -3.86812072e-03, -1.55793473e-01, -1.34338260e-01,\n",
            "       -4.14129347e-03, -3.12908590e-01,  3.79287124e-01,  1.69631511e-01,\n",
            "        4.41041470e-01, -8.36970583e-02, -2.83147525e-02,  2.18246318e-03,\n",
            "        1.58853963e-01, -3.15707475e-01,  1.49924189e-01,  2.93414116e-01,\n",
            "       -4.89522330e-02, -2.66178459e-01, -1.52371466e-01,  2.77717352e-01,\n",
            "       -2.22600959e-02, -4.66471672e-01,  1.39486650e-02, -1.29965082e-01,\n",
            "       -2.80609429e-01,  2.48368587e-02, -1.32145351e-02,  2.64854133e-01,\n",
            "        1.50790751e-01, -6.17719769e-01,  2.72301704e-01, -1.65604219e-01,\n",
            "        9.18658525e-02, -1.23094441e-02,  1.52704164e-01,  9.80250686e-02,\n",
            "       -3.41856390e-01, -2.58044571e-01,  1.80060625e-01, -1.18060419e-02,\n",
            "        3.41226280e-01,  3.48763652e-02, -6.67144060e-02,  5.98535165e-02,\n",
            "       -4.97155488e-02,  3.03423017e-01,  6.35378212e-02,  4.25572336e-01,\n",
            "       -8.25141847e-01, -9.95866358e-02, -5.96783273e-02, -3.93921971e-01,\n",
            "        6.55640066e-02,  6.59987405e-02,  2.00145096e-01, -1.52244031e-01,\n",
            "       -1.08832650e-01, -3.32835793e-01,  4.20902558e-02, -2.00058401e-01,\n",
            "       -1.65064074e-02,  1.15302086e-01,  4.68018353e-02, -1.63286924e-02,\n",
            "       -2.21931487e-01,  6.27759814e-01, -6.51007652e-01,  1.88595235e-01,\n",
            "       -4.49398533e-03, -3.18121672e-01, -1.05704889e-01, -5.41288614e-01,\n",
            "        1.99868958e-02,  4.96370733e-01, -3.20252538e-01,  1.81814730e-01,\n",
            "        3.66847217e-02,  1.77904665e-01,  1.72851205e-01,  2.81752825e-01,\n",
            "       -2.66506046e-01,  1.97476596e-01, -2.07365006e-01, -1.67320907e-01,\n",
            "        1.07816095e-02,  2.82830745e-01,  1.51670903e-01, -1.42442584e-01,\n",
            "        1.80556417e-01, -1.45966172e-01,  1.89342782e-01, -3.39723349e-01,\n",
            "       -7.21989572e-02, -7.64648337e-03, -2.08335053e-02, -1.98951095e-01,\n",
            "       -1.45851523e-01, -1.43725201e-01, -4.22746778e-01, -9.49130729e-02,\n",
            "       -1.97473634e-02,  9.11135375e-02,  8.09674039e-02, -1.48302674e-01,\n",
            "       -2.44191185e-01,  4.64638211e-02, -3.11767191e-01,  1.88620076e-01,\n",
            "       -1.81270167e-01,  2.59034127e-01,  1.44205511e-01,  3.01060945e-01,\n",
            "       -5.15979640e-02,  7.47370124e-02,  7.32320786e-01, -1.13100007e-01,\n",
            "       -5.88917911e-01,  1.49724364e-01, -2.90009752e-03,  2.04511791e-01,\n",
            "       -3.30459177e-01, -1.24290362e-01, -1.61094397e-01, -5.32326847e-02,\n",
            "       -3.80775660e-01,  5.30943498e-02,  1.98092759e-02, -2.60852993e-01,\n",
            "        1.58445127e-02, -8.02632868e-02,  2.70081818e-01, -3.51036750e-02,\n",
            "        2.26781145e-01,  3.32209378e-01,  7.00915903e-02,  9.34848189e-02,\n",
            "       -1.13639340e-01,  2.21308917e-01, -5.39508536e-02,  3.80934924e-01,\n",
            "       -1.56471327e-01,  5.38971543e-01, -1.25046328e-01,  2.06672534e-01,\n",
            "       -1.69128001e-01,  4.36268635e-02, -6.10140800e-01, -1.28046885e-01,\n",
            "        2.56877746e-02, -2.99436599e-01,  1.36135846e-01, -1.43695578e-01,\n",
            "        2.34504372e-01, -1.46067813e-01,  2.40304023e-01,  1.66595988e-02,\n",
            "        6.75726056e-01,  1.80720210e-01,  1.17913343e-01, -4.03377041e-02,\n",
            "        3.22710812e-01,  3.82529527e-01,  2.77779728e-01, -3.60441774e-01,\n",
            "       -3.01320502e-03, -2.39801615e-01,  3.78536195e-01, -1.75001577e-01,\n",
            "        3.06480110e-01, -2.30061114e-01, -8.15086067e-02,  2.75615841e-01,\n",
            "        1.83969468e-01,  2.32662141e-01,  3.22436780e-01, -5.04777059e-02,\n",
            "        8.97767544e-02, -2.24138632e-01, -3.63660157e-02,  4.48473632e-01,\n",
            "        1.69076920e-01, -7.38155961e-01, -5.33493161e-02, -2.52797008e-01,\n",
            "       -2.41842672e-01, -5.47378324e-02,  1.50287792e-01,  1.10600486e-01,\n",
            "        2.90162772e-01,  1.35265300e-02,  1.85854822e-01, -1.15795750e-02,\n",
            "       -2.01349229e-01,  3.88756186e-01, -2.44100153e-01, -8.07500854e-02,\n",
            "        5.57526201e-03,  1.50626466e-01, -4.12921429e-01,  2.91043550e-01,\n",
            "        4.82473612e-01, -1.15948275e-01,  1.55033842e-01, -5.90993762e-02,\n",
            "        3.06131601e-01, -5.55130653e-03,  1.60928577e-01, -1.63826928e-01,\n",
            "        6.21987164e-01,  1.65216789e-01, -3.07870001e-01,  1.69736639e-01,\n",
            "       -2.09743217e-01, -1.37661502e-01,  4.23902273e-02, -1.96766719e-01,\n",
            "       -4.94812667e-01, -5.21298766e-01, -2.29979932e-01,  1.83359966e-01,\n",
            "        3.34615186e-02, -1.67188376e-01, -1.05743371e-01,  1.40617162e-01,\n",
            "       -2.49714315e-01,  4.44314163e-03,  6.72194511e-02,  1.79660112e-01,\n",
            "       -1.60727546e-01, -9.48309973e-02, -1.65283680e-02,  1.79286107e-01,\n",
            "       -1.47590145e-01,  1.43472075e-01,  1.10577896e-01, -2.92441063e-02,\n",
            "        3.10292035e-01,  1.95948541e-01, -1.09958634e-01,  5.08178413e-01,\n",
            "        1.28840297e-01, -3.07881087e-01,  1.95974186e-01,  8.62799942e-01,\n",
            "        5.47197938e-01,  6.11666590e-04,  1.17164448e-01,  4.81167108e-01,\n",
            "       -3.61925289e-02,  7.63838412e-03,  9.26316679e-02, -2.08170056e-01,\n",
            "        4.83550429e-02, -1.43313169e-01,  6.60355091e-01, -1.08812049e-01,\n",
            "        1.81851014e-01,  4.11369264e-01,  1.64813846e-01, -8.53563771e-02,\n",
            "        6.57838210e-02, -1.45699710e-01,  7.00307637e-02,  6.49491251e-02,\n",
            "        1.59186289e-01, -2.54160762e-01, -9.63277072e-02,  7.52540976e-02,\n",
            "        1.91829056e-01, -2.02345654e-01, -8.96141529e-02, -6.22066204e-03,\n",
            "       -7.75563776e-01, -6.98268175e-01, -2.38894269e-01, -1.52876005e-01,\n",
            "       -7.09550381e-01,  4.48151499e-01,  1.63359299e-01, -2.42436692e-01,\n",
            "        7.85388052e-02,  4.07051384e-01,  4.48034495e-01,  3.03810537e-01,\n",
            "       -3.97887491e-02,  5.83864003e-02,  8.94143879e-02,  3.51908028e-01,\n",
            "        1.37101308e-01,  1.41264141e-01, -2.01912910e-01, -3.60855684e-02,\n",
            "        1.89400002e-01,  1.70822546e-01, -1.41863406e-01,  2.76285768e-01,\n",
            "        1.98630616e-01,  5.49933195e-01, -9.86501053e-02,  1.17445886e-01,\n",
            "       -2.40921393e-01, -3.63311283e-02,  5.27597904e-01,  3.73990357e-01,\n",
            "       -1.23720288e-01,  1.03293434e-01,  1.16801605e-01, -1.09129354e-01,\n",
            "        7.61867315e-02, -4.16066647e-01,  7.58117884e-02,  3.98874849e-01,\n",
            "       -6.41536340e-02,  1.77222654e-01,  3.46885324e-01,  3.27121317e-02,\n",
            "       -7.62592256e-02,  5.16420268e-02, -4.14560199e-01, -1.43292770e-01,\n",
            "       -1.45872653e-01,  3.95223588e-01,  5.35965025e-01, -1.32853940e-01,\n",
            "        4.56660353e-02, -8.99582654e-02, -2.36932822e-02,  2.41503939e-01,\n",
            "       -6.29455969e-02, -4.33237165e-01, -2.21972808e-01, -3.63932736e-02,\n",
            "        8.56020972e-02,  2.65720114e-02,  6.56523854e-02, -9.47256684e-02,\n",
            "       -4.51126635e-01,  1.28675669e-01,  3.20155114e-01,  1.82907075e-01,\n",
            "        4.87248927e-01, -2.72874206e-01,  1.07911453e-01,  1.12740576e-01,\n",
            "       -2.80304879e-01, -1.03916451e-01, -1.70214549e-01, -2.86318213e-01,\n",
            "       -2.13904947e-01, -1.06204394e-03, -1.27212862e-02, -8.77835043e-03,\n",
            "        1.11714847e-01,  9.38736573e-02, -3.42963308e-01, -5.84559500e-01,\n",
            "        1.74419004e-02, -1.15380533e-01, -1.50325179e-01, -1.67440847e-02,\n",
            "       -1.08074874e-01, -2.79196203e-01, -2.32157603e-01, -4.66368534e-02,\n",
            "       -2.20905051e-01, -9.28011239e-02,  4.55009565e-03,  1.96210802e-01,\n",
            "       -4.37749803e-01,  5.20157702e-02,  1.92065746e-01,  2.42443621e-01,\n",
            "       -1.65674061e-01,  8.16855878e-02, -1.64745018e-01,  4.08131540e-01,\n",
            "        1.68745518e-01,  1.13434516e-01, -1.94579601e-01, -9.87719838e-03,\n",
            "        3.08075607e-01, -7.26556173e-03, -1.92650616e-01,  6.84880465e-02,\n",
            "       -4.21871960e-01, -1.45961270e-01,  1.20099716e-01, -1.82776287e-01],\n",
            "      dtype=float32), array([ 1.44954324e-01,  3.18350315e-01,  3.28089535e-01, -1.89857289e-01,\n",
            "       -4.92180847e-02,  5.31950951e-01,  1.20951943e-01,  5.41184068e-01,\n",
            "        4.40475315e-01, -3.05566698e-01, -1.66927010e-01, -3.05198580e-01,\n",
            "       -1.54835805e-01, -6.91741705e-02,  7.90528953e-04, -5.02224743e-01,\n",
            "        1.46092504e-01, -5.99997155e-02,  1.25639379e-01, -1.45959526e-01,\n",
            "       -1.27895981e-01,  4.31898445e-01, -5.00931777e-02, -5.65185785e-01,\n",
            "        2.27061585e-01,  3.39710340e-02,  4.18004803e-02, -1.85753986e-01,\n",
            "       -2.60087222e-01, -1.81692436e-01, -4.05416965e-01, -7.24076033e-02,\n",
            "       -1.80268168e-01, -8.71198252e-04,  1.95719868e-01, -8.37432146e-02,\n",
            "       -1.40197882e-02, -2.22529888e-01, -6.57317817e-01,  2.75708456e-02,\n",
            "        4.04661208e-01, -6.67143008e-03, -1.36252731e-01,  2.71639973e-01,\n",
            "       -1.84863076e-01, -2.01195970e-01, -7.70347059e-01, -1.86908692e-01,\n",
            "        7.86149055e-02,  1.98409244e-01,  2.01069146e-01,  3.35435532e-02,\n",
            "        3.84543277e-02,  7.49420673e-02, -1.87538639e-01,  2.59628296e-01,\n",
            "        7.32321441e-02,  2.21455157e-01, -9.10821557e-02,  1.40247419e-01,\n",
            "        7.26678073e-02, -4.22737986e-01, -2.10271239e-01,  1.00905311e+00,\n",
            "        1.35682449e-01, -2.18657210e-01,  3.25289011e-01,  8.83614779e-01,\n",
            "        1.16029948e-01, -6.12217709e-02,  1.79521763e-03, -2.58014619e-01,\n",
            "        1.85638011e-01, -5.33414841e-01,  3.17668706e-01,  7.45557666e-01,\n",
            "        1.77993983e-01, -2.37215251e-01, -5.45647323e-01,  4.21475023e-02,\n",
            "        2.82692220e-02, -4.24418718e-01,  4.25432801e-01,  1.51990563e-01,\n",
            "       -6.99087009e-02, -2.21298933e-01,  1.00903489e-01, -2.29068458e-01,\n",
            "        1.57307371e-01,  3.03623855e-01, -2.58201063e-01, -1.57059610e-01,\n",
            "       -5.23503900e-01, -3.19605678e-01,  1.70744747e-01, -3.59331310e-01,\n",
            "       -1.79269284e-01, -6.77974448e-02, -1.67383831e-02, -2.04848185e-01,\n",
            "        1.70518607e-01, -1.41433477e-01, -2.36286726e-02,  4.52665836e-01,\n",
            "        6.94039837e-02, -1.82565197e-01, -7.94220194e-02, -4.58903730e-01,\n",
            "        2.76964605e-01, -2.42620483e-02, -5.72539158e-02, -9.03336853e-02,\n",
            "        1.39271334e-01,  7.70983696e-02,  3.36955301e-02,  1.35080591e-01,\n",
            "        2.02121921e-02, -2.85255402e-01, -1.74035162e-01, -8.49845558e-02,\n",
            "        2.20171496e-01,  4.21529859e-02, -7.78387338e-02,  1.08368054e-01,\n",
            "       -8.10097218e-01,  4.37280834e-01, -1.69583648e-01, -2.19138294e-01,\n",
            "       -5.12356916e-03,  3.45182538e-01,  4.34798673e-02,  4.10686105e-01,\n",
            "       -1.26564533e-01, -2.94459403e-01, -9.58164874e-03, -1.70672685e-01,\n",
            "       -2.73564160e-01, -1.29612964e-02, -3.06356221e-01, -1.61546499e-01,\n",
            "       -4.45438623e-02, -8.02354217e-02,  1.25247225e-01, -2.30080843e-01,\n",
            "        1.74066573e-01,  1.01465724e-01,  1.13119349e-01,  3.26246098e-02,\n",
            "        1.51531264e-01,  1.20592684e-01, -1.79460675e-01,  8.94532427e-02,\n",
            "        8.59949216e-02, -3.73484492e-01,  5.71614914e-02, -5.34219816e-02,\n",
            "        4.93663460e-01,  4.25405577e-02,  3.19800436e-01,  1.60856828e-01,\n",
            "       -1.61701605e-01, -5.49770653e-01, -4.06950474e-01, -2.03022063e-01,\n",
            "       -8.04456025e-02,  5.46640493e-02,  2.05651030e-01,  9.54290926e-02,\n",
            "       -4.00457889e-01,  4.21379181e-03, -7.44560808e-02, -2.13131532e-01,\n",
            "       -5.26108921e-01, -1.03791431e-03, -4.22922134e-01,  1.02500558e-01,\n",
            "       -1.80247232e-01, -7.19962865e-02,  3.41730207e-01, -6.12919703e-02,\n",
            "        2.56434202e-01,  7.35563971e-03,  5.69744967e-02, -5.15744269e-01,\n",
            "        2.12275565e-01,  2.13200182e-01, -1.59086511e-02,  9.51711833e-01,\n",
            "       -2.49333113e-01, -2.11305931e-01, -1.31633908e-01, -1.53428674e-01,\n",
            "       -2.90581882e-01, -2.40491360e-01,  1.82800710e-01,  1.89066693e-01,\n",
            "       -1.03483655e-01,  1.58377327e-02,  1.34818196e-01, -4.51799929e-01,\n",
            "        5.75936362e-02,  5.12636662e-01, -8.13090652e-02,  2.08311319e-01,\n",
            "        1.29823238e-01, -1.22902833e-01,  3.95686239e-01,  2.50445426e-01,\n",
            "        2.68685669e-01,  4.38261867e-01,  5.96978441e-02,  2.21016899e-01,\n",
            "       -1.25214085e-02,  5.13432562e-01,  3.08137536e-02,  1.84759006e-01,\n",
            "        1.25586212e-01,  1.29704666e+00, -3.42405289e-01,  4.41633254e-01,\n",
            "       -2.45728433e-01,  3.10553312e-02, -8.40763524e-02, -2.39238679e-01,\n",
            "       -8.64511132e-02, -4.67097819e-01,  1.16375305e-01, -5.26762158e-02,\n",
            "       -5.17987311e-01, -3.38053107e-01,  1.24972813e-01,  2.62356430e-01,\n",
            "       -2.05879807e-01,  2.75803924e-01,  1.44704908e-01,  4.45000648e-01,\n",
            "       -5.05287014e-02,  3.61611098e-01, -3.99709214e-03,  3.69626492e-01,\n",
            "       -1.19249791e-01, -4.10920531e-02,  7.01727718e-02,  2.03754306e-01,\n",
            "       -4.21195105e-02,  4.69827473e-01,  1.76512003e-01, -1.38590246e-01,\n",
            "       -1.31027475e-01, -4.64972854e-01, -2.49189250e-02, -2.03837708e-01,\n",
            "       -1.64464474e-01, -2.00845242e-01,  3.39736372e-01, -3.95876706e-01,\n",
            "       -9.30773020e-02,  1.97155088e-01,  3.66819501e-01,  3.62715691e-01,\n",
            "       -1.48615427e-02,  6.17310882e-01,  1.58039823e-01,  9.23860595e-02,\n",
            "        3.17010701e-01,  6.65453792e-01, -1.80786654e-01, -8.96868631e-02,\n",
            "        1.82043821e-01, -2.88375188e-02,  2.47239694e-01,  3.76611114e-01,\n",
            "        2.38852933e-01, -7.88837224e-02, -2.17719078e-02,  1.92200765e-01,\n",
            "        3.40529792e-02,  2.03869388e-01, -3.88449997e-01,  2.05660671e-01,\n",
            "       -1.70656726e-01, -8.02325383e-02,  7.25961402e-02,  7.76595995e-02,\n",
            "        3.15118045e-01, -5.18303692e-01, -8.69901553e-02, -2.60137826e-01,\n",
            "       -3.58882472e-02,  1.07507080e-01, -4.27897722e-02,  8.25644508e-02,\n",
            "       -8.76712799e-02,  9.85320657e-02, -1.58321425e-01,  2.73557514e-01,\n",
            "        1.26405910e-01, -3.19630742e-01,  9.00142360e-03, -1.74986690e-01,\n",
            "       -3.53921950e-01,  3.12927872e-01,  5.25016963e-01, -1.81603163e-01,\n",
            "       -1.49262398e-01, -7.06009269e-02,  4.85616624e-01,  9.87843871e-02,\n",
            "       -3.71953100e-01,  6.35439754e-02, -3.51214528e-01,  7.47579262e-02,\n",
            "       -1.78715348e-01, -6.50832476e-03, -1.61719888e-01,  2.96649694e-01,\n",
            "        2.82148123e-01, -1.12999648e-01, -4.24719155e-02,  1.43298969e-01,\n",
            "        1.65668800e-01, -2.30353121e-02, -5.20315468e-02,  1.14170834e-01,\n",
            "        3.74447763e-01,  5.85051160e-03,  2.38263041e-01, -5.02205253e-01,\n",
            "       -3.01103354e-01, -6.12477139e-02,  7.82505691e-01,  6.29204884e-02,\n",
            "        9.52299982e-02,  7.09805012e-01,  8.49196091e-02,  3.11095417e-01,\n",
            "        9.57328081e-02,  2.53546797e-03, -4.43003863e-01, -2.44463891e-01,\n",
            "       -1.05581515e-01, -2.43932121e-02, -7.12849438e-01, -2.14702636e-02,\n",
            "       -3.64704460e-01, -4.14756536e-02, -8.68782550e-02, -2.22218290e-01,\n",
            "       -8.01691115e-02,  3.34778398e-01,  3.01571518e-01,  6.08014166e-02,\n",
            "        3.61522138e-02,  2.98148781e-01, -1.21319383e-01, -3.52047980e-01,\n",
            "        5.92935793e-02,  9.69549119e-02, -1.54851943e-01,  1.95567101e-01,\n",
            "       -1.29128247e-01, -2.83346295e-01, -1.76556766e-01, -2.39949167e-01,\n",
            "       -9.58995521e-02,  3.27154666e-01, -1.40211105e-01,  2.11931691e-01,\n",
            "       -4.84577641e-02,  6.67500794e-02, -3.32724117e-02, -2.20342465e-02,\n",
            "        2.00589478e-01, -1.71361253e-01,  1.50106430e-01, -1.88178513e-02,\n",
            "        2.48701096e-01, -2.87046611e-01, -6.89103603e-02,  7.01104030e-02,\n",
            "       -2.84920126e-01, -8.92283097e-02, -1.74825758e-01, -2.06661075e-02,\n",
            "       -1.19755499e-01, -4.55081433e-01, -3.69909406e-02,  4.26666468e-01,\n",
            "        5.95053971e-01, -2.03514844e-01, -1.88158035e-01, -2.20392048e-01,\n",
            "        6.67149723e-02, -4.36682582e-01,  4.36824039e-02,  2.39504725e-01,\n",
            "       -7.43179098e-02, -6.39012814e-01, -1.40365630e-01, -5.87201454e-02,\n",
            "        7.09836036e-02, -9.62829664e-02, -3.67086716e-02,  4.24621195e-01,\n",
            "       -2.36297920e-01,  1.53682128e-01, -1.79145694e-01,  9.04937908e-02,\n",
            "        4.87164170e-01, -2.05240399e-01,  7.90664703e-02,  6.43147886e-01,\n",
            "        2.25989260e-02, -2.01929659e-01, -1.18042529e-03,  2.44963378e-01,\n",
            "        2.18910545e-01, -2.72828728e-01,  2.76545018e-01, -5.19976795e-01,\n",
            "        2.60131598e-01, -1.30975500e-01, -1.16566621e-01, -7.45075718e-02,\n",
            "       -1.61913246e-01,  2.43810371e-01, -4.24850732e-02,  1.97237626e-01,\n",
            "       -1.63456991e-01,  2.76221991e-01, -3.80024128e-02, -1.70743927e-01,\n",
            "       -2.05658656e-02,  2.00803950e-01, -1.73062503e-01, -3.09977494e-02,\n",
            "       -1.23506732e-01, -1.62414849e-01, -1.03660494e-01, -4.78232116e-01,\n",
            "        8.22885111e-02,  1.08132720e-01, -3.44886258e-02,  1.07098535e-01,\n",
            "        6.38314150e-03, -7.78660327e-02, -1.51850253e-01,  1.01695098e-01,\n",
            "       -4.43239659e-02, -3.36220622e-01,  2.25655004e-01, -2.38364235e-01,\n",
            "        2.37874873e-02,  2.86774188e-01, -2.56197602e-01,  9.52778533e-02,\n",
            "        1.43018728e-02,  3.00927430e-01,  5.24231121e-02,  1.17388912e-01,\n",
            "        8.70446712e-02, -5.96656650e-02, -2.01051101e-01, -3.62637669e-01,\n",
            "       -1.37017086e-01,  5.34339286e-02,  3.43394786e-01,  4.35078740e-02,\n",
            "       -1.30237892e-01,  1.99708641e-02, -1.69917509e-01, -3.16341490e-01,\n",
            "        3.28408089e-03,  3.78691852e-02,  1.49560124e-01, -8.42600837e-02,\n",
            "        4.20372337e-02, -8.68151784e-02, -2.44700328e-01,  7.44125396e-02,\n",
            "        1.22503243e-01,  2.49091327e-01,  1.24895602e-01, -7.63898119e-02,\n",
            "       -1.34682894e-01,  2.04224631e-01, -3.93097520e-01,  2.37165064e-01,\n",
            "       -1.24221601e-01,  2.30876088e-01, -1.67895392e-01, -1.31213948e-01,\n",
            "        5.69816492e-02,  3.70932460e-01,  7.16353714e-01, -2.90204082e-02,\n",
            "       -3.19815516e-01,  2.95452178e-01,  2.50314534e-01,  6.06244989e-02,\n",
            "       -4.03173208e-01, -1.67526141e-01, -2.87825525e-01,  4.14069027e-01,\n",
            "       -2.66937643e-01,  2.69645840e-01,  2.67198607e-02, -5.76848499e-02,\n",
            "        1.72161162e-01, -1.91874802e-02,  1.78644747e-01, -8.28076601e-02,\n",
            "        2.56103367e-01,  2.38138244e-01,  1.34581789e-01, -1.15681991e-01,\n",
            "        1.96946263e-01, -1.33960456e-01, -1.99765801e-01,  4.63409603e-01,\n",
            "        5.85214384e-02,  2.35380024e-01,  3.76038514e-02,  7.67343491e-03,\n",
            "       -2.17615694e-01, -2.87423804e-02, -2.77514368e-01, -3.15135974e-03,\n",
            "       -1.17927916e-01, -2.38982245e-01, -4.70426865e-04, -2.67467678e-01,\n",
            "        2.22644955e-01, -1.58572689e-01,  2.03999028e-01,  7.82442838e-02,\n",
            "        5.05985618e-01,  1.89316422e-01,  4.33817320e-02,  5.12868986e-02,\n",
            "        1.11479297e-01, -6.72352165e-02,  4.77635264e-01, -3.28466177e-01,\n",
            "       -1.54173940e-01, -1.55068055e-01, -1.48308143e-01, -1.30070969e-01,\n",
            "        5.24690390e-01, -6.74856454e-02, -2.96547338e-02,  5.18110454e-01,\n",
            "        7.65549764e-02,  1.37783378e-01,  2.76227534e-01, -1.54442890e-02,\n",
            "       -1.36007424e-02, -1.26746193e-01, -2.69815058e-01,  2.43805259e-01,\n",
            "        2.11277962e-01, -3.70622337e-01, -1.67108938e-01,  1.84453607e-01,\n",
            "        3.71590674e-01,  4.42000553e-02,  8.81257802e-02,  6.05446100e-02,\n",
            "        2.85357744e-01,  1.07272685e-01,  2.69683361e-01,  1.24201484e-01,\n",
            "       -1.29061803e-01,  1.73647881e-01, -1.12245567e-01,  3.99294406e-01,\n",
            "       -7.40736946e-02,  1.82244569e-01, -3.94955397e-01,  2.52342492e-01,\n",
            "        2.63300389e-01,  1.54609039e-01,  9.57315117e-02, -3.16585898e-01,\n",
            "        3.12599838e-01,  3.86761427e-01, -2.01183394e-01, -5.51870838e-02,\n",
            "        3.41442972e-01,  4.05324437e-02, -2.63788611e-01, -7.49245286e-02,\n",
            "       -5.40662169e-01, -1.81430608e-01,  9.56014097e-02, -4.85083461e-01,\n",
            "       -2.59638727e-02, -2.48569191e-01, -2.15146571e-01,  1.39357463e-01,\n",
            "        3.53060842e-01, -1.54604658e-01, -1.32775322e-01, -4.30444591e-02,\n",
            "       -1.47070184e-01, -1.06711693e-01, -1.11646459e-01,  1.84627682e-01,\n",
            "       -1.18279271e-02, -3.85578871e-01, -1.62354842e-01,  2.21652985e-01,\n",
            "       -1.12984002e-01, -5.24950996e-02, -2.61523962e-01,  2.17727989e-01,\n",
            "       -3.40146199e-02, -5.46069294e-02, -2.97939420e-01,  8.09515238e-01,\n",
            "        2.49317408e-01, -2.81633973e-01,  2.22509652e-01, -1.39227286e-02,\n",
            "        3.97466511e-01,  3.31982970e-01,  1.18659176e-01,  6.67288661e-01,\n",
            "        2.25169808e-02,  2.86288768e-01,  2.35652849e-01, -1.96014903e-02,\n",
            "        2.04099193e-01, -4.66377467e-01,  4.28042442e-01, -5.65031543e-03,\n",
            "        9.56114978e-02,  1.58158451e-01, -6.19565398e-02, -1.94804400e-01,\n",
            "        1.62709996e-01, -9.43659171e-02, -2.46411338e-01,  1.38105080e-01,\n",
            "       -8.58168900e-02, -9.31777582e-02,  1.80217952e-01, -8.69411752e-02,\n",
            "       -9.22092870e-02, -2.89455503e-01, -1.40356287e-01,  2.61417199e-02,\n",
            "       -5.49941778e-01, -5.82341790e-01, -3.31768751e-01,  3.27688217e-01,\n",
            "       -8.06055248e-01,  2.14791223e-01,  5.25003262e-02,  5.66788912e-02,\n",
            "        1.38279051e-01,  4.28499103e-01,  4.83406425e-01,  2.57005811e-01,\n",
            "        2.42156014e-01,  1.91993877e-01,  2.47279793e-01, -5.37342280e-02,\n",
            "        1.69328094e-01,  6.37003630e-02, -1.70621186e-01,  3.29276979e-01,\n",
            "        3.82620990e-01,  5.25424704e-02,  2.40485631e-02, -8.37427750e-02,\n",
            "        1.56888396e-01,  3.23085546e-01, -2.99489677e-01,  3.34850699e-01,\n",
            "       -2.02235118e-01, -2.31200457e-01,  6.30690455e-01,  3.35759819e-01,\n",
            "        1.58733040e-01,  9.50097367e-02,  3.07414621e-01, -3.72815460e-01,\n",
            "        1.52243674e-01, -3.08672488e-01, -7.10109025e-02,  2.67618448e-01,\n",
            "       -2.65307724e-01,  2.51965702e-01, -1.88392639e-01,  9.61438641e-02,\n",
            "        1.21182084e-01, -1.31950593e-02, -2.05438539e-01,  1.52117819e-01,\n",
            "        6.97629824e-02,  1.83445871e-01,  5.76401711e-01, -1.39077436e-02,\n",
            "       -1.20139848e-02,  1.47719145e-01,  1.29655346e-01, -1.18990645e-01,\n",
            "       -7.54415989e-02, -2.64383763e-01, -4.05755267e-02,  1.38707310e-01,\n",
            "       -3.69954556e-02,  2.63599694e-01,  3.63204405e-02, -2.59380966e-01,\n",
            "       -3.47743690e-01,  2.95977831e-01,  4.00352269e-01,  1.49542317e-01,\n",
            "        1.11345723e-01, -3.77459943e-01,  2.82573372e-01,  2.02972040e-01,\n",
            "       -2.78019816e-01, -2.34377190e-01, -1.72990605e-01, -3.52980107e-01,\n",
            "        6.18636422e-02, -1.45941511e-01,  5.58697805e-03,  3.45250368e-01,\n",
            "       -2.24839747e-02,  2.39308879e-01,  1.00117519e-01, -1.84013277e-01,\n",
            "        1.50621071e-01, -1.13217235e-02,  3.46017024e-03,  1.33420359e-02,\n",
            "       -3.49602580e-01, -1.70934260e-01,  8.86215940e-02, -1.02998167e-01,\n",
            "        3.93621139e-02,  2.44773328e-01,  5.46861291e-02, -1.83663163e-02,\n",
            "       -2.80272812e-01, -1.76632047e-01,  1.20782569e-01,  3.72259736e-01,\n",
            "        1.15486205e-01,  6.80536926e-02,  2.96095490e-01,  3.18987310e-01,\n",
            "        5.95159590e-01,  4.16068062e-02,  5.64669929e-02,  6.70296103e-02,\n",
            "        2.58924186e-01,  1.39464140e-01, -3.03631037e-01, -5.53081892e-02,\n",
            "        2.03891128e-01,  4.02423382e-01,  3.75631630e-01, -1.38626456e-01],\n",
            "      dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdwoTwtGs7Sn",
        "outputId": "9b83aba7-8342-4f7f-abd6-91262bfdbb31"
      },
      "source": [
        "len(embedd[0:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEBgrnAFquhL"
      },
      "source": [
        "list2_a = [['nan',\n",
        "  'study problem policy policy evaluation oppe contrast prior work consider estimate individual policy value average policy value accurately draw inspiration recent work causal reason propose new finite sample generalization error bind value estimate mdp model use upper bind objective develop learn algorithm mdp model balance representation show approach yield substantially lower mse common synthetic benchmarks hiv treatment simulation domain',\n",
        "  'nan',\n",
        "  'adapt ideas underlie success deep learn continuous action domain present actor critic model free algorithm base deterministic policy gradient operate continuous action space use learn algorithm network architecture hyper parameters algorithm robustly solve simulate physics task include classic problems cartpole swing dexterous manipulation legged locomotion car drive algorithm able find policies whose performance competitive find plan algorithm full access dynamics domain derivatives demonstrate many task algorithm learn policies end end directly raw pixel input',\n",
        "  'ideas enjoy large impact deep learn convolution problem involve pixels spatial representations common intuition hold convolutional neural network may appropriate paper show strike counterexample intuition via seemingly trivial coordinate transform problem simply require learn map coordinate cartesian space coordinate one hot pixel space although convolutional network would seem appropriate task show fail spectacularly demonstrate carefully analyze failure first toy problem point simple fix become obvious call solution coordconv work give convolution access input coordinate use extra coordinate channel without sacrifice computational parametric efficiency ordinary convolution coordconv allow network learn either complete translation invariance vary degrees translation dependence require end task coordconv solve coordinate transform problem perfect generalization time faster time fewer parameters convolution stark contrast raise question extent inability convolution persist insidiously inside task subtly hamper performance within complete answer question require investigation show preliminary evidence swap convolution coordconv improve model diverse set task use coordconv gin produce less mode collapse transform high level spatial latents pixels become easier learn faster cnn detection model train mnist detection show better iou use coordconv reinforcement learn domain agents play atari game benefit significantly use coordconv layer',\n",
        "  'introduce new approach decomposable submodular function minimization dsfm exploit incidence relations incidence relations describe variables effectively influence component function properly utilize allow improve convergence rat dsfm solvers main result include precise parametrization dsfm problem base incidence relations development new scalable alternative projections parallel coordinate descent methods accompany rigorous analysis convergence rat',\n",
        "  'propose structure adaptive variant state art stochastic variance reduce gradient algorithm katyusha regularize empirical risk minimization propose method able exploit intrinsic low dimensional structure solution sparsity low rank enforce non smooth regularization achieve even faster convergence rate provable algorithmic improvement do restart katyusha algorithm accord restrict strong convexity constants demonstrate effectiveness approach via numerical experiment',\n",
        "  'give rigorous analysis statistical behavior gradients randomly initialize fully connect network relu activations result show empirical variance square entries input output jacobian exponential simple architecture dependent constant beta give sum reciprocals hide layer widths beta large gradients compute initialization vary wildly approach complement mean field theory analysis random network point view rigorously compute finite width corrections statistics gradients edge chaos',\n",
        "  'present first accelerate randomize algorithm solve linear systems euclidean space one essential problem type matrix inversion problem particular algorithm specialize invert positive definite matrices way iterate approximate solutions generate algorithm positive definite matrices open way many applications field optimization machine learn application general theory develop first accelerate deterministic stochastic quasi newton update update lead provably aggressive approximations inverse hessian lead speed up classical non accelerate rule numerical experiment experiment empirical risk minimization show rule accelerate train machine learn model',\n",
        "  'paper address graph match problem follow recent work cite zaslavskiy path vestner analyze generalize idea concave relaxations introduce concepts emph conditionally concave emph probably conditionally concave energies polytopes show encapsulate many instance graph match problem include match euclidean graph graph surface prove local minima probably conditionally concave energies general match polytopes doubly stochastic high probability extreme point match polytope permutations',\n",
        "  'work introduce interactive structure learn framework unify many different interactive learn task present generalization query committee active learn algorithm set study consistency rate convergence theoretically empirically without noise',\n",
        "  'convolutional neural network cnns recently achieve great success single image super resolution sisr however methods tend produce smooth output miss textural detail solve problems propose super resolution cliquenet srcliquenet reconstruct high resolution image better textural detail wavelet domain propose srcliquenet firstly extract set feature map low resolution image clique block group send set feature map clique sample module reconstruct image clique sample module consist four sub net predict high resolution wavelet coefficients four sub band since consider edge feature properties four sub band four sub net connect others learn coefficients four sub band jointly finally apply inverse discrete wavelet transform idwt output four sub net end clique sample module increase resolution reconstruct image extensive quantitative qualitative experiment benchmark datasets show method achieve superior performance state art methods'],\n",
        " ['nan',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'policy gradient methods widely use control reinforcement learn particularly continuous action set host theoretically sound algorithms propose policy set due existence policy gradient theorem provide simplify form gradient policy learn however behaviour policy necessarily attempt learn follow optimal policy give task existence theorem elusive work solve open problem provide first policy policy gradient theorem key derivation use emphatic weight develop new actor critic algorithm call actor critic emphatic weight ace approximate simplify gradients provide theorem demonstrate simple counterexample previous policy policy gradient methods particularly offpac dpg converge wrong solution whereas ace find optimal solution',\n",
        "  'nan',\n",
        "  'large scale network model use neurons static nonlinearities produce analog output despite fact information process brain predominantly carry dynamic neurons produce discrete pulse call spike research spike base computation impede lack efficient supervise learn algorithm spike neural network present gradient descent method optimize spike network model introduce differentiable formulation spike dynamics derive exact gradient calculation demonstration train recurrent spike network two dynamic task one require optimize fast millisecond spike base interactions efficient encode information delay memory task extend duration second result show gradient descent approach indeed optimize network dynamics time scale individual spike well behavioral time scale conclusion method yield general purpose supervise learn algorithm spike neural network facilitate investigations spike base computations',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'adapt ideas underlie success deep learn continuous action domain present actor critic model free algorithm base deterministic policy gradient operate continuous action space use learn algorithm network architecture hyper parameters algorithm robustly solve simulate physics task include classic problems cartpole swing dexterous manipulation legged locomotion car drive algorithm able find policies whose performance competitive find plan algorithm full access dynamics domain derivatives demonstrate many task algorithm learn policies end end directly raw pixel input',\n",
        "  'accurate exposure key capture high quality photos computational photography especially mobile phone limit size camera modules inspire luminosity mask usually apply professional photographers paper develop novel algorithm learn local exposures deep reinforcement adversarial learn specific segment image sub image reflect variations dynamic range exposures accord raw low level feature base sub image local exposure sub image automatically learn virtue policy network sequentially reward learn globally design strike balance overall exposures aesthetic evaluation function approximate discriminator generative adversarial network reinforcement learn adversarial learn train collaboratively asynchronous deterministic policy gradient generative loss approximation simply algorithmic architecture also prove feasibility leverage discriminator value function employ local exposure retouch raw input image respectively thus deliver multiple retouch image different exposures fuse exposure blend extensive experiment verify algorithms superior state art methods term quantitative accuracy visual illustration',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'study problem policy policy evaluation oppe contrast prior work consider estimate individual policy value average policy value accurately draw inspiration recent work causal reason propose new finite sample generalization error bind value estimate mdp model use upper bind objective develop learn algorithm mdp model balance representation show approach yield substantially lower mse common synthetic benchmarks hiv treatment simulation domain',\n",
        "  'semantic scene completion predict volumetric occupancy object category scene help intelligent agents understand interact surround work propose disentangle framework sequentially carry semantic segmentation reprojection semantic scene completion three stage framework three advantage explicit semantic segmentation significantly boost performance flexible fusion ways sensor data bring good extensibility progress subtask promote holistic performance experimental result show regardless input single depth rgb framework generate high quality semantic scene completion outperform state art approach synthetic real datasets',\n",
        "  'propose novel randomize first order optimization method sega sketch gradient method progressively throughout iterations build variance reduce estimate gradient random linear measurements sketch gradient provide iteration oracle iteration sega update current estimate gradient sketch project operation use information provide latest sketch subsequently use compute unbiased estimate true gradient random relaxation procedure unbiased estimate use perform gradient step unlike standard subspace descent methods coordinate descent sega use optimization problems non separable proximal term provide general convergence analysis prove linear convergence strongly convex objectives special case coordinate sketch sega enhance various techniques importance sample minibatching acceleration rate small constant factor identical best know rate coordinate descent',\n",
        "  'despite remarkable advance image synthesis research exist work often fail manipulate image context large geometric transformations synthesize person image condition arbitrary pose one representative examples generation quality largely rely capability identify model arbitrary transformations different body part current generative model often build local convolutions overlook key challenge heavy occlusions different view dramatic appearance change distinct geometric change happen part cause arbitrary pose manipulations paper aim resolve challenge induce geometric variability spatial displacements via new soft gate warp generative adversarial network warp gin compose two stag first synthesize target part segmentation map give target pose depict region level spatial layouts guide image synthesis higher level structure constraints warp gin equip soft gate warp block learn feature level map render textures original image generate segmentation map warp gin capable control different transformation degrees give distinct target pose moreover propose warp block light weight flexible enough inject network human perceptual study quantitative evaluations demonstrate superiority warp gin significantly outperform exist methods two large datasets',\n",
        "  'tremendous recent progress equilibrium find algorithms zero sum imperfect information extensive form game puzzle gap theory practice first order methods significantly better theoretical convergence rat counterfactual regret minimization cfr variant despite cfr variants favor practice experiment first order methods conduct small medium size game methods complicate implement set cfr variants enhance extensively decade perform well practice paper show particular first order method state art variant excessive gap technique instantiate dilate entropy distance function efficiently solve large real world problems competitively cfr variants show large endgames encounter libratus poker recently beat top human poker specialist professionals limit texas hold show experimental result variant excessive gap technique well prior version introduce numerically friendly implementation smooth best response computation associate first order methods extensive form game solve present knowledge first gpu implementation first order method extensive form game present comparisons several excessive gap technique cfr variants',\n",
        "  'convolutional neural network cnns inherently subject invariable filter aggregate local input topological structure cause cnns allow manage data euclidean grid like structure image ones non euclidean graph structure traffic network broaden reach cnns develop structure aware convolution eliminate invariance yield unify mechanism deal euclidean non euclidean structure data technically filter structure aware convolution generalize univariate function capable aggregate local input diverse topological structure since infinite parameters require determine univariate function parameterize filter number learnable parameters context function approximation theory replace classical convolution cnns structure aware convolution structure aware convolutional neural network sacnns readily establish extensive experiment eleven datasets strongly evidence sacnns outperform current model various machine learn task include image classification cluster text categorization skeleton base action recognition molecular activity detection taxi flow prediction',\n",
        "  'recent years supervise learn convolutional network cnns see huge adoption computer vision applications comparatively unsupervised learn cnns receive less attention work hope help bridge gap success cnns supervise learn unsupervised learn introduce class cnns call deep convolutional generative adversarial network dcgans certain architectural constraints demonstrate strong candidate unsupervised learn train various image datasets show convince evidence deep convolutional adversarial pair learn hierarchy representations object part scenes generator discriminator additionally use learn feature novel task demonstrate applicability general image representations',\n",
        "  'recurrent neural network powerful tool understand model computation representation populations neurons continuous variable rate model network analyze apply extensively purpose however neurons fire action potentials discrete nature spike important feature neural circuit dynamics despite significant advance train recurrently connect spike neural network remain challenge present procedure train recurrently connect spike network generate dynamical pattern autonomously produce complex temporal output base integrate network input model physiological data procedure make use continuous variable network identify target train input spike model neurons surprisingly able construct spike network duplicate task perform continuous variable network relatively minor expansion number neurons approach provide novel view significance appropriate use fire rate model useful approach build model spike network use address important question representation computation neural systems'],\n",
        " ['present novel newton type method distribute optimization particularly well suit stochastic optimization learn problems quadratic objectives method enjoy linear rate convergence provably emph improve data size require essentially constant number iterations reasonable assumptions provide theoretical empirical evidence advantage method compare approach one shoot parameter average admm',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'propose structure adaptive variant state art stochastic variance reduce gradient algorithm katyusha regularize empirical risk minimization propose method able exploit intrinsic low dimensional structure solution sparsity low rank enforce non smooth regularization achieve even faster convergence rate provable algorithmic improvement do restart katyusha algorithm accord restrict strong convexity constants demonstrate effectiveness approach via numerical experiment',\n",
        "  'introduce new approach decomposable submodular function minimization dsfm exploit incidence relations incidence relations describe variables effectively influence component function properly utilize allow improve convergence rat dsfm solvers main result include precise parametrization dsfm problem base incidence relations development new scalable alternative projections parallel coordinate descent methods accompany rigorous analysis convergence rat'],\n",
        " ['describe iterative procedure optimize policies guarantee monotonic improvement make several approximations theoretically justify procedure develop practical algorithm call trust region policy optimization trpo algorithm similar natural policy gradient methods effective optimize large nonlinear policies neural network experiment demonstrate robust performance wide variety task learn simulate robotic swim hop walk gaits play atari game use image screen input despite approximations deviate theory trpo tend give monotonic improvement little tune hyperparameters',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'attention network multimodal learn provide efficient way utilize give visual information selectively however computational cost learn attention distributions every pair multimodal input channel prohibitively expensive solve problem attention build two separate attention distributions modality neglect interaction multimodal input paper propose bilinear attention network ban find bilinear attention distributions utilize give vision language information seamlessly ban consider bilinear interactions among two group input channel low rank bilinear pool extract joint representations pair channel furthermore propose variant multimodal residual network exploit eight attention map ban efficiently quantitatively qualitatively evaluate model visual question answer vqa flickr entities datasets show ban significantly outperform previous methods achieve new state arts datasets',\n",
        "  'recent work suggest enhance bloom filter use pre filter base apply machine learn determine function model data set bloom filter mean represent model learn bloom filter follow outcomes clarify guarantee associate structure show estimate size learn function must obtain order obtain improve performance provide simple method sandwich optimize learn bloom filter propose design analysis approach learn bloomier filter base model approach',\n",
        "  'stochastic gradient descent sgd popular algorithm achieve state art performance variety machine learn task several researchers recently propose scheme parallelize sgd require performance destroy memory lock synchronization work aim show use novel theoretical analysis algorithms implementation sgd implement without lock present update scheme call hogwild allow processors access share memory possibility overwrite work show associate optimization problem sparse mean gradient update modify small part decision variable hogwild achieve nearly optimal rate convergence demonstrate experimentally hogwild outperform alternative scheme use lock order magnitude',\n",
        "  'paper introduce versatile filter construct efficient convolutional neural network consider demand efficient deep learn techniques run cost effective hardware number methods develop learn compact neural network work aim slim filter different ways investigate small sparse binarized filter contrast treat filter additive perspective series secondary filter derive primary filter secondary filter inherit primary filter without occupy storage unfold computation could significantly enhance capability filter integrate information extract different receptive field besides spatial versatile filter additionally investigate versatile filter channel perspective new techniques general upgrade filter exist cnns experimental result benchmark datasets neural network demonstrate cnns construct versatile filter able achieve comparable accuracy original filter require less memory flop'],\n",
        " ['consider problem online learn linear contextual bandits set also strong individual fairness constraints govern unknown similarity metric constraints demand select similar action individuals approximately equal probability dhprz may odds optimize reward thus model settings profit social policy tension assume learn unknown mahalanobis similarity metric weak feedback identify fairness violations quantify extent intend represent interventions regulator know unfairness see nevertheless enunciate quantitative fairness metric individuals main result algorithm adversarial context set number fairness violations depend logarithmically obtain optimal sqrt regret bind best fair policy',\n",
        "  'deep reinforcement learn successfully solve many challenge control task real world applicability limit inability ensure safety learn policies propose approach verifiable reinforcement learn train decision tree policies represent complex policies since nonparametric yet efficiently verify use exist techniques since highly structure challenge decision tree policies difficult train propose viper algorithm combine ideas model compression imitation learn learn decision tree policies guide dnn policy call oracle function show substantially outperform two baselines use viper learn provably robust decision tree policy variant atari pong symbolic state space learn decision tree policy toy game base pong provably never lose iii learn provably stable decision tree policy cart pole case decision tree policy achieve performance equal original dnn policy',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'contextual bandit literature traditionally focus algorithms address exploration exploitation tradeoff particular greedy algorithms exploit current estimate without exploration may sub optimal general however exploration free greedy algorithms desirable practical settings exploration may costly unethical clinical trials surprisingly find simple greedy algorithm rate optimal achieve asymptotically optimal regret sufficient randomness observe contexts covariates prove always case two arm bandit general class context distributions satisfy condition term covariate diversity furthermore even absent condition show greedy algorithm rate optimal positive probability thus standard bandit algorithms may unnecessarily explore motivate result introduce greedy first new algorithm use observe contexts reward determine whether follow greedy algorithm explore prove algorithm rate optimal without additional assumptions context distribution number arm extensive simulations demonstrate greedy first successfully reduce exploration outperform exist exploration base contextual bandit algorithms thompson sample upper confidence bind ucb',\n",
        "  'consider problem online learn linear contextual bandits set also strong individual fairness constraints govern unknown similarity metric constraints demand select similar action individuals approximately equal probability dhprz may odds optimize reward thus model settings profit social policy tension assume learn unknown mahalanobis similarity metric weak feedback identify fairness violations quantify extent intend represent interventions regulator know unfairness see nevertheless enunciate quantitative fairness metric individuals main result algorithm adversarial context set number fairness violations depend logarithmically obtain optimal sqrt regret bind best fair policy',\n",
        "  'humans routinely retrace path novel environment forward backwards despite uncertainty motion paper present approach give demonstration path first network generate path equip second network observe world decide act order retrace path noisy actuation change environment two network optimize end end train time evaluate method two realistic simulators perform path follow forward backwards experiment show approach outperform classical approach solve task well number baselines',\n",
        "  'humans make repeat choices among options imperfectly know reward outcomes important problem psychology neuroscience often study use multi arm bandits also frequently study machine learn present data human stationary bandit experiment vary average abundance variability reward availability mean variance reward rate distributions surprisingly find subject significantly underestimate prior mean reward rat base self report end game reward expectation non choose arm previously human learn bandit task find well capture bayesian ideal learn model dynamic belief model dbm albeit incorrect generative assumption temporal structure humans assume reward rat change time even though actually fix find pessimism bias bandit task well capture prior mean dbm fit human choices poorly capture prior mean fix belief model fbm alternative bayesian model correctly assume reward rat constants pessimism bias also incompletely capture simple reinforcement learn model commonly use neuroscience psychology term fit initial value seem sub optimal thus mysterious humans underestimate prior reward expectation simulations show underestimate prior mean help maximize long term gain observer assume volatility reward rat stable utilize softmax decision policy instead optimal one obtainable dynamic program raise intrigue possibility brain underestimate reward rat compensate incorrect non stationarity assumption generative model simplify decision policy'],\n",
        " ['nan',\n",
        "  'nan',\n",
        "  'algorithmically construct multi output gaussian process priors satisfy linear differential equations approach attempt parametrize solutions equations use bner base successful push forward gaussian process along paramerization desire prior consider several examples physics geomathmatics control among full inhomogeneous system maxwell equations bring together stochastic learn computeralgebra novel way combine noisy observations precise algebraic computations',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'note consider normalize gradient descent ngd natural modification classical gradient descent optimization problems serious shortcoming non convex problems may take arbitrarily long escape neighborhood saddle point issue make convergence arbitrarily slow particularly high dimensional non convex problems relative number saddle point often large paper focus continuous time descent show contrary standard ngd escape saddle point quickly particular show ngd almost never converge saddle point time require ngd escape ball radius saddle point sqrt kappa kappa condition number hessian application result global convergence time bind establish ngd mild assumptions',\n",
        "  'nan',\n",
        "  'present shapenet richly annotate large scale repository shape represent cad model object shapenet contain model multitude semantic categories organize wordnet taxonomy collection datasets provide many semantic annotations model consistent rigid alignments part bilateral symmetry plan physical size keywords well plan annotations annotations make available public web base interface enable data visualization object attribute promote data drive geometric analysis provide large scale quantitative benchmark research computer graphics vision time technical report shapenet index model model classify categories wordnet synsets report describe shapenet effort whole provide detail currently available datasets summarize future plan',\n",
        "  'nan',\n",
        "  'wide adoption dnns give birth unrelenting compute requirements force datacenter operators adopt domain specific accelerators train accelerators typically employ densely pack full precision float point arithmetic maximize performance per area ongoing research efforts seek increase performance density replace float point fix point arithmetic however significant roadblock attempt fix point narrow dynamic range insufficient dnn train convergence identify block float point bfp promise alternative representation since exhibit wide dynamic range enable majority dnn operations perform fix point logic unfortunately bfp alone introduce several limitations preclude direct applicability work introduce hbfp hybrid bfp approach perform dot products bfp operations float point hbfp deliver best worlds high accuracy float point superior hardware density fix point wide variety model show hbfp match float point accuracy enable hardware implementations deliver higher throughput',\n",
        "  'nan',\n",
        "  'nan'],\n",
        " ['machine learn model vulnerable adversarial examples small change image cause computer vision model make mistake identify school bus ostrich however still open question whether humans prone similar mistake address question leverage recent techniques transfer adversarial examples computer vision model know parameters architecture model unknown parameters architecture match initial process human visual system find adversarial examples strongly transfer across computer vision model influence classifications make time limit human observers'],\n",
        " ['paper address graph match problem follow recent work cite zaslavskiy path vestner analyze generalize idea concave relaxations introduce concepts emph conditionally concave emph probably conditionally concave energies polytopes show encapsulate many instance graph match problem include match euclidean graph graph surface prove local minima probably conditionally concave energies general match polytopes doubly stochastic high probability extreme point match polytope permutations',\n",
        "  'ensembles randomize decision tree usually refer random forest widely use classification regression task machine learn statistics random forest achieve competitive predictive performance computationally efficient train test make excellent candidates real world prediction task popular random forest variants breiman random forest extremely randomize tree operate batch train data online methods greater demand exist online random forest however require train data batch counterpart achieve comparable predictive performance work use mondrian process roy teh construct ensembles random decision tree call mondrian forest mondrian forest grow incremental online fashion remarkably distribution online mondrian forest batch mondrian forest mondrian forest achieve competitive predictive performance comparable exist online random forest periodically train batch random forest order magnitude faster thus represent better computation accuracy tradeoff',\n",
        "  'nan',\n",
        "  'nan',\n",
        "  'deep reinforcement learn successfully solve many challenge control task real world applicability limit inability ensure safety learn policies propose approach verifiable reinforcement learn train decision tree policies represent complex policies since nonparametric yet efficiently verify use exist techniques since highly structure challenge decision tree policies difficult train propose viper algorithm combine ideas model compression imitation learn learn decision tree policies guide dnn policy call oracle function show substantially outperform two baselines use viper learn provably robust decision tree policy variant atari pong symbolic state space learn decision tree policy toy game base pong provably never lose iii learn provably stable decision tree policy cart pole case decision tree policy achieve performance equal original dnn policy'],\n",
        " ['paper try organize machine teach coherent set ideas idea present vary along dimension collection dimension form problem space machine teach exist teach problems characterize space hope organization allow gain deeper understand individual teach problems discover connections among identify gap field'],\n",
        " ['goal orient dialog give attention due numerous applications artificial intelligence goal orient dialogue task occur questioner ask action orient question answerer respond intent let questioner know correct action take ask adequate question deep learn reinforcement learn recently apply however approach struggle find competent recurrent neural questioner owe complexity learn series sentence motivate theory mind propose answerer questioner mind aqm novel information theoretic algorithm goal orient dialog aqm questioner ask infer base approximate probabilistic model answerer questioner figure answerer intention via select plausible question explicitly calculate information gain candidate intentions possible answer question test framework two goal orient visual dialog task mnist count dialog guesswhat experiment aqm outperform comparative algorithms large margin'],\n",
        " ['nan',\n",
        "  'bilinear model show achieve impressive performance wide range visual task semantic segmentation fine grain recognition face recognition however bilinear feature high dimensional typically order hundreds thousands million make impractical subsequent analysis propose two compact bilinear representations discriminative power full bilinear representation thousand dimension compact representations allow back propagation classification errors enable end end optimization visual recognition system compact bilinear representations derive novel kernelized analysis bilinear pool provide insights discriminative power bilinear pool platform research compact pool methods experimentation illustrate utility propose representations image classification shoot learn across several datasets',\n",
        "  'nan'],\n",
        " ['aim obtain interpretable expressive disentangle scene representation contain comprehensive structural textural information object previous scene representations learn neural network often uninterpretable limit single object lack knowledge work propose scene render network sdn address issue integrate disentangle representations semantics geometry appearance deep generative model scene encoder perform inverse graphics translate scene structure object wise representation decoder two components differentiable shape renderer neural texture generator disentanglement semantics geometry appearance support aware scene manipulation rotate move object freely keep consistent shape texture change object appearance without affect shape experiment demonstrate edit scheme base sdn superior counterpart',\n",
        "  'accurately answer question give image require combine observations general knowledge effortless humans reason general knowledge remain algorithmic challenge advance research direction novel reason correct answer jointly consider entities show challenge fvqa dataset lead improvement accuracy around compare state art',\n",
        "  'reason play essential role visual question answer vqa multi step dynamic reason often necessary answer complex question example question place next bus right picture talk compound object bus right generate relation'],\n",
        " ['hash one popular powerful approximate nearest neighbor search techniques large scale image retrieval traditional hash methods first represent image shelf visual feature produce hash cod separate stage however shelf visual feature may optimally compatible hash code learn procedure may result sub optimal hash cod recently deep hash methods propose simultaneously learn image feature hash cod use deep neural network show superior performance traditional hash methods deep hash methods give supervise information form pairwise label triplet label current state art deep hash method dpsh cite feature base pairwise label perform image feature learn hash code learn simultaneously maximize likelihood pairwise similarities inspire dpsh cite feature propose triplet label base deep hash method aim maximize likelihood give triplet label experimental result show method outperform baselines cifar nus wide datasets include state art method dpsh cite feature previous triplet label base deep hash methods'],\n",
        " ['learn decision tree data difficult optimization problem widespread algorithm practice date base greedy growth tree structure recursively split nod possibly prune back final tree parameters decision function internal node approximately estimate minimize impurity measure give algorithm give input tree structure parameter value nod produce new tree smaller structure new parameter value provably lower leave unchanged misclassification error apply axis align oblique tree experiment show consistently outperform various algorithms highly scalable large datasets tree algorithm handle sparsity penalty learn sparse oblique tree structure subset original tree nonzero parameters combine best axis align oblique tree flexibility model correlate data low generalization error fast inference interpretable nod involve feature decision',\n",
        "  'paper compare different type recurrent units recurrent neural network rnns especially focus sophisticate units implement gate mechanism long short term memory lstm unit recently propose gate recurrent unit gru evaluate recurrent units task polyphonic music model speech signal model experiment reveal advance recurrent units indeed better traditional recurrent units tanh units also find gru comparable lstm'],\n",
        " ['tremendous recent progress equilibrium find algorithms zero sum imperfect information extensive form game puzzle gap theory practice first order methods significantly better theoretical convergence rat counterfactual regret minimization cfr variant despite cfr variants favor practice experiment first order methods conduct small medium size game methods complicate implement set cfr variants enhance extensively decade perform well practice paper show particular first order method state art variant excessive gap technique instantiate dilate entropy distance function efficiently solve large real world problems competitively cfr variants show large endgames encounter libratus poker recently beat top human poker specialist professionals limit texas hold show experimental result variant excessive gap technique well prior version introduce numerically friendly implementation smooth best response computation associate first order methods extensive form game solve present knowledge first gpu implementation first order method extensive form game present comparisons several excessive gap technique cfr variants',\n",
        "  'contextual bandit literature traditionally focus algorithms address exploration exploitation tradeoff particular greedy algorithms exploit current estimate without exploration may sub optimal general however exploration free greedy algorithms desirable practical settings exploration may costly unethical clinical trials surprisingly find simple greedy algorithm rate optimal achieve asymptotically optimal regret sufficient randomness observe contexts covariates prove always case two arm bandit general class context distributions satisfy condition term covariate diversity furthermore even absent condition show greedy algorithm rate optimal positive probability thus standard bandit algorithms may unnecessarily explore motivate result introduce greedy first new algorithm use observe contexts reward determine whether follow greedy algorithm explore prove algorithm rate optimal without additional assumptions context distribution number arm extensive simulations demonstrate greedy first successfully reduce exploration outperform exist exploration base contextual bandit algorithms thompson sample upper confidence bind ucb',\n",
        "  'societies often rely human experts take wide variety decisions affect members jail release decisions take judge stop frisk decisions take police officer accept reject decisions take academics context decision take expert typically choose uniformly random pool experts however decisions may imperfect due limit experience implicit bias faulty probabilistic reason improve accuracy fairness overall decision make process optimize assignment experts decisions'],\n",
        " ['draw attention important yet largely overlook aspect evaluate fairness automate decision make systems namely risk welfare considerations propose family measure correspond long establish formulations cardinal social welfare economics justify rawlsian conception fairness behind veil ignorance convex formulation welfare base measure fairness allow integrate constraint convex loss minimization pipeline empirical analysis reveal interest trade off proposal prediction accuracy group discrimination dwork notion individual fairness furthermore perhaps importantly work provide heuristic justification empirical evidence suggest lower bind measure often lead bound inequality algorithmic outcomes hence present first computationally feasible mechanism bound individual level inequality'],\n",
        " ['technical challenge deep learn recognize target class without see data zero shoot learn leverage semantic representations attribute class prototypes bridge source target class exist standard zero shoot learn methods may prone overfitting see data source class blind semantic representations target class paper study generalize zero shoot learn assume accessible target class unseen data train prediction unseen data make search source target class propose novel deep calibration network dcn approach towards generalize zero shoot learn paradigm enable simultaneous calibration deep network confidence source class uncertainty target class approach map visual feature image semantic representations class prototypes common embed space compatibility see data source target class maximize show superior accuracy approach state art benchmark datasets generalize zero shoot learn include awa cub sun apy',\n",
        "  'paper propose conceptually simple general framework call metagan shoot learn problems state art shoot classification model integrate metagan principled straightforward way introduce adversarial generator condition task augment vanilla shoot classification model ability discriminate real fake data argue gin base approach help shoot classifiers learn sharper decision boundary could generalize better show metagan framework extend supervise shoot learn model naturally cope unsupervised data different previous work semi supervise shoot learn algorithms deal semi supervision sample level task level give theoretical justifications strength metagan validate effectiveness metagan challenge shoot image classification benchmarks'],\n",
        " ['nan',\n",
        "  'nan',\n",
        "  'recurrent network spike neurons rsnns underlie astound compute learn capabilities brain compute learn capabilities rsnn model remain poor least comparison anns address two possible reason one rsnns brain randomly connect design accord simple rule start learn tabula rasa network rather rsnns brain optimize task evolution development prior experience detail optimization process largely unknown functional contribution approximate powerful optimization methods backpropagation time bptt'],\n",
        " ['suppose design matrix linear regression problem give response point hide unless explicitly request goal sample small number responses produce weight vector whose sum square loss point epsilon time minimum small jointly sample diverse subsets point crucial one method call volume sample unique desirable property weight vector produce unbiased estimate optimum therefore natural ask method offer optimal unbiased estimate term number responses need achieve epsilon loss approximation',\n",
        "  'consider problem learn optimal reserve price repeat auction non myopic bidders may bid strategically order gain future round even single round auction truthful previous algorithms empirical price provide non trivial regret round set general introduce algorithms obtain small regret non myopic bidders either market large bidder appear constant fraction round bidders impatient discount future utility factor mildly bound away one approach carefully control information reveal bidder build techniques differentially private online learn well recent line work jointly differentially private algorithms',\n",
        "  'duplicate removal critical step accomplish reasonable amount predictions prevalent proposal base object detection frameworks albeit simple effective previous algorithms utilize greedy process without make sufficient use properties input data work design new two stage framework effectively select appropriate proposal candidate object first stage suppress easy negative object proposals second stage select true positives reduce proposal set two stag share network structure encoder decoder form recurrent neural network rnn global attention context gate encoder scan proposal candidates sequential manner capture global context information feed decoder extract optimal proposals extensive experiment propose method outperform alternatives large margin'],\n",
        " ['significant interest able predict crimes happen example aid efficient task police protective measure aim model temporal spatial dependencies often exhibit violent crimes order make predictions temporal variation crimes typically follow pattern familiar time series analysis spatial pattern irregular vary smoothly across area instead find spatially disjoint regions exhibit correlate crime pattern indeterminate inter region correlation structure along low count discrete nature count serious crimes motivate propose forecast tool particular propose model crime count region use integer value first order autoregressive process take bayesian nonparametric approach flexibly discover cluster region specific time series describe account covariates within framework approach adjust seasonality demonstrate approach analysis weekly report violent crimes washington forecast outperform standard methods additionally provide useful tool prediction intervals',\n",
        "  'decision tree random forest well establish model offer good predictive performance also provide rich feature importance information practitioners often employ variable importance methods rely impurity base information methods remain poorly characterize theoretical perspective provide novel insights performance methods derive finite sample performance guarantee high dimensional set various model assumptions demonstrate effectiveness impurity base methods via extensive set simulations',\n",
        "  'several large scale deployments differential privacy use collect statistical information users however deployments periodically recollect data recompute statistics use algorithms design single use result systems provide meaningful privacy guarantee long time scale moreover exist techniques mitigate effect apply local model differential privacy systems use'],\n",
        " ['propose novel flexible anchor mechanism name metaanchor object detection frameworks unlike many previous detectors model anchor via predefined manner metaanchor anchor function could dynamically generate arbitrary customize prior box take advantage weight prediction metaanchor able work anchor base object detection systems retinanet compare predefined anchor scheme empirically find metaanchor robust anchor settings bound box distributions addition also show potential transfer task experiment coco detection task show metaanchor consistently outperform counterparts various scenarios'],\n",
        " ['nan',\n",
        "  'accurately answer question give image require combine observations general knowledge effortless humans reason general knowledge remain algorithmic challenge advance research direction novel reason correct answer jointly consider entities show challenge fvqa dataset lead improvement accuracy around compare state art'],\n",
        " ['despite remarkable advance image synthesis research exist work often fail manipulate image context large geometric transformations synthesize person image condition arbitrary pose one representative examples generation quality largely rely capability identify model arbitrary transformations different body part current generative model often build local convolutions overlook key challenge heavy occlusions different view dramatic appearance change distinct geometric change happen part cause arbitrary pose manipulations paper aim resolve challenge induce geometric variability spatial displacements via new soft gate warp generative adversarial network warp gin compose two stag first synthesize target part segmentation map give target pose depict region level spatial layouts guide image synthesis higher level structure constraints warp gin equip soft gate warp block learn feature level map render textures original image generate segmentation map warp gin capable control different transformation degrees give distinct target pose moreover propose warp block light weight flexible enough inject network human perceptual study quantitative evaluations demonstrate superiority warp gin significantly outperform exist methods two large datasets',\n",
        "  'generative adversarial network gans technique learn generative model complex data distributions sample despite remarkable advance generate realistic image major shortcoming gans fact tend produce sample little diversity even train diverse datasets phenomenon know mode collapse focus much recent work study principled approach handle mode collapse call pack main idea modify discriminator make decisions base multiple sample class either real artificially generate draw analysis tool binary hypothesis test particular seminal result blackwell prove fundamental connection pack mode collapse show pack naturally penalize generators mode collapse thereby favor generator distributions less mode collapse train process numerical experiment benchmark datasets suggest pack provide significant improvements',\n",
        "  'nan'],\n",
        " ['recent years supervise learn convolutional network cnns see huge adoption computer vision applications comparatively unsupervised learn cnns receive less attention work hope help bridge gap success cnns supervise learn unsupervised learn introduce class cnns call deep convolutional generative adversarial network dcgans certain architectural constraints demonstrate strong candidate unsupervised learn train various image datasets show convince evidence deep convolutional adversarial pair learn hierarchy representations object part scenes generator discriminator additionally use learn feature novel task demonstrate applicability general image representations',\n",
        "  'accurate exposure key capture high quality photos computational photography especially mobile phone limit size camera modules inspire luminosity mask usually apply professional photographers paper develop novel algorithm learn local exposures deep reinforcement adversarial learn specific segment image sub image reflect variations dynamic range exposures accord raw low level feature base sub image local exposure sub image automatically learn virtue policy network sequentially reward learn globally design strike balance overall exposures aesthetic evaluation function approximate discriminator generative adversarial network reinforcement learn adversarial learn train collaboratively asynchronous deterministic policy gradient generative loss approximation simply algorithmic architecture also prove feasibility leverage discriminator value function employ local exposure retouch raw input image respectively thus deliver multiple retouch image different exposures fuse exposure blend extensive experiment verify algorithms superior state art methods term quantitative accuracy visual illustration',\n",
        "  'people belong multiple communities word belong multiple topics book cover multiple genres overlap cluster commonplace many exist overlap cluster methods model person word book non negative weight combination exemplars belong solely one community small noise geometrically person point cone whose corner exemplars basic form encompass widely use mix membership stochastic blockmodel network degree correct variants well topic model lda show simple one class svm yield provably consistent parameter inference model scale large datasets experimental result several simulate real datasets show algorithm call svm cone accurate scalable'],\n",
        " ['convolutional neural network cnns recently achieve great success single image super resolution sisr however methods tend produce smooth output miss textural detail solve problems propose super resolution cliquenet srcliquenet reconstruct high resolution image better textural detail wavelet domain propose srcliquenet firstly extract set feature map low resolution image clique block group send set feature map clique sample module reconstruct image clique sample module consist four sub net predict high resolution wavelet coefficients four sub band since consider edge feature properties four sub band four sub net connect others learn coefficients four sub band jointly finally apply inverse discrete wavelet transform idwt output four sub net end clique sample module increase resolution reconstruct image extensive quantitative qualitative experiment benchmark datasets show method achieve superior performance state art methods',\n",
        "  'paper introduce versatile filter construct efficient convolutional neural network consider demand efficient deep learn techniques run cost effective hardware number methods develop learn compact neural network work aim slim filter different ways investigate small sparse binarized filter contrast treat filter additive perspective series secondary filter derive primary filter secondary filter inherit primary filter without occupy storage unfold computation could significantly enhance capability filter integrate information extract different receptive field besides spatial versatile filter additionally investigate versatile filter channel perspective new techniques general upgrade filter exist cnns experimental result benchmark datasets neural network demonstrate cnns construct versatile filter able achieve comparable accuracy original filter require less memory flop',\n",
        "  'humans routinely retrace path novel environment forward backwards despite uncertainty motion paper present approach give demonstration path first network generate path equip second network observe world decide act order retrace path noisy actuation change environment two network optimize end end train time evaluate method two realistic simulators perform path follow forward backwards experiment show approach outperform classical approach solve task well number baselines',\n",
        "  'generative recurrent neural network quickly train unsupervised manner model popular reinforcement learn environments compress spatio temporal representations world model extract feature feed compact simple policies train evolution achieve state art result various environments also train agent entirely inside environment generate internal world model transfer policy back actual environment interactive version paper available https worldmodels github',\n",
        "  'introduce approach convert mono audio record video camera spatial audio representation distribution sound full view sphere spatial audio important component immersive video view spatial audio microphones still rare current video production system consist end end trainable neural network separate individual sound source localize view sphere condition multi modal analysis audio video frame introduce several datasets include one film one collect wild youtube consist videos upload spatial audio train grind truth spatial audio serve self supervision mix mono track form input network use approach show possible infer spatial localization sound base synchronize video mono audio track'],\n",
        " ['propose simple yet effective approach spatiotemporal feature learn use deep dimensional convolutional network convnets train large scale supervise video dataset find three fold convnets suitable spatiotemporal feature learn compare convnets homogeneous architecture small convolution kernels layer among best perform architectures convnets learn feature namely convolutional simple linear classifier outperform state art methods different benchmarks comparable current best methods benchmarks addition feature compact achieve accuracy ucf dataset dimension also efficient compute due fast inference convnets finally conceptually simple easy train use',\n",
        "  'softmax output activation function model categorical probability distributions many applications deep learn however recent study reveal softmax bottleneck representational capacity neural network language model softmax bottleneck paper propose output activation function break softmax bottleneck without additional parameters analyze softmax bottleneck perspective output set log softmax identify cause softmax bottleneck basis analysis propose sigsoftmax compose multiplication exponential function sigmoid function sigsoftmax break softmax bottleneck experiment language model demonstrate sigsoftmax mixture sigsoftmax outperform softmax mixture softmax respectively'],\n",
        " ['real world applications education effective teacher adaptively choose next example teach base learner current state however exist work algorithmic machine teach focus batch set adaptivity play role paper study case teach consistent version space learners interactive set time step teacher provide example learner perform update teacher observe learner new state highlight adaptivity speed teach process consider exist model version space learners worst case model learner pick next hypothesis randomly version space preference base model learner pick hypothesis accord global preference inspire human teach propose new model learner pick hypotheses accord local preference define current hypothesis show model exhibit several desirable properties adaptivity play key role learner transition hypotheses smooth interpretable develop adaptive teach algorithms demonstrate result via simulation user study'],\n",
        " ['nan'],\n",
        " ['nan'],\n",
        " ['nan']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq4tRZNkvdki"
      },
      "source": [
        "list2_a2 = []\n",
        "length = len(list2_a)\n",
        "for i in range(length):\n",
        "  list2_a2.append([])\n",
        "for i in range(len(list2_a)):\n",
        "  for j in range(len(list2_a[i])):\n",
        "    if list2_a[i][j] == 'nan':\n",
        "      pass\n",
        "    \n",
        "    else:\n",
        "      list2_a2[i].append(list2_a[i][j])\n",
        "list2_a22 = []\n",
        "for i in list2_a2:\n",
        "  if len(i) == 0:\n",
        "    pass\n",
        "  else:\n",
        "    list2_a22.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSWerUXqwKkP",
        "outputId": "1e60fa1f-7b7d-48bd-d281-d3f9c0313b2e"
      },
      "source": [
        "list2_a22"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['study problem policy policy evaluation oppe contrast prior work consider estimate individual policy value average policy value accurately draw inspiration recent work causal reason propose new finite sample generalization error bind value estimate mdp model use upper bind objective develop learn algorithm mdp model balance representation show approach yield substantially lower mse common synthetic benchmarks hiv treatment simulation domain',\n",
              "  'adapt ideas underlie success deep learn continuous action domain present actor critic model free algorithm base deterministic policy gradient operate continuous action space use learn algorithm network architecture hyper parameters algorithm robustly solve simulate physics task include classic problems cartpole swing dexterous manipulation legged locomotion car drive algorithm able find policies whose performance competitive find plan algorithm full access dynamics domain derivatives demonstrate many task algorithm learn policies end end directly raw pixel input',\n",
              "  'ideas enjoy large impact deep learn convolution problem involve pixels spatial representations common intuition hold convolutional neural network may appropriate paper show strike counterexample intuition via seemingly trivial coordinate transform problem simply require learn map coordinate cartesian space coordinate one hot pixel space although convolutional network would seem appropriate task show fail spectacularly demonstrate carefully analyze failure first toy problem point simple fix become obvious call solution coordconv work give convolution access input coordinate use extra coordinate channel without sacrifice computational parametric efficiency ordinary convolution coordconv allow network learn either complete translation invariance vary degrees translation dependence require end task coordconv solve coordinate transform problem perfect generalization time faster time fewer parameters convolution stark contrast raise question extent inability convolution persist insidiously inside task subtly hamper performance within complete answer question require investigation show preliminary evidence swap convolution coordconv improve model diverse set task use coordconv gin produce less mode collapse transform high level spatial latents pixels become easier learn faster cnn detection model train mnist detection show better iou use coordconv reinforcement learn domain agents play atari game benefit significantly use coordconv layer',\n",
              "  'introduce new approach decomposable submodular function minimization dsfm exploit incidence relations incidence relations describe variables effectively influence component function properly utilize allow improve convergence rat dsfm solvers main result include precise parametrization dsfm problem base incidence relations development new scalable alternative projections parallel coordinate descent methods accompany rigorous analysis convergence rat',\n",
              "  'propose structure adaptive variant state art stochastic variance reduce gradient algorithm katyusha regularize empirical risk minimization propose method able exploit intrinsic low dimensional structure solution sparsity low rank enforce non smooth regularization achieve even faster convergence rate provable algorithmic improvement do restart katyusha algorithm accord restrict strong convexity constants demonstrate effectiveness approach via numerical experiment',\n",
              "  'give rigorous analysis statistical behavior gradients randomly initialize fully connect network relu activations result show empirical variance square entries input output jacobian exponential simple architecture dependent constant beta give sum reciprocals hide layer widths beta large gradients compute initialization vary wildly approach complement mean field theory analysis random network point view rigorously compute finite width corrections statistics gradients edge chaos',\n",
              "  'present first accelerate randomize algorithm solve linear systems euclidean space one essential problem type matrix inversion problem particular algorithm specialize invert positive definite matrices way iterate approximate solutions generate algorithm positive definite matrices open way many applications field optimization machine learn application general theory develop first accelerate deterministic stochastic quasi newton update update lead provably aggressive approximations inverse hessian lead speed up classical non accelerate rule numerical experiment experiment empirical risk minimization show rule accelerate train machine learn model',\n",
              "  'paper address graph match problem follow recent work cite zaslavskiy path vestner analyze generalize idea concave relaxations introduce concepts emph conditionally concave emph probably conditionally concave energies polytopes show encapsulate many instance graph match problem include match euclidean graph graph surface prove local minima probably conditionally concave energies general match polytopes doubly stochastic high probability extreme point match polytope permutations',\n",
              "  'work introduce interactive structure learn framework unify many different interactive learn task present generalization query committee active learn algorithm set study consistency rate convergence theoretically empirically without noise',\n",
              "  'convolutional neural network cnns recently achieve great success single image super resolution sisr however methods tend produce smooth output miss textural detail solve problems propose super resolution cliquenet srcliquenet reconstruct high resolution image better textural detail wavelet domain propose srcliquenet firstly extract set feature map low resolution image clique block group send set feature map clique sample module reconstruct image clique sample module consist four sub net predict high resolution wavelet coefficients four sub band since consider edge feature properties four sub band four sub net connect others learn coefficients four sub band jointly finally apply inverse discrete wavelet transform idwt output four sub net end clique sample module increase resolution reconstruct image extensive quantitative qualitative experiment benchmark datasets show method achieve superior performance state art methods'],\n",
              " ['policy gradient methods widely use control reinforcement learn particularly continuous action set host theoretically sound algorithms propose policy set due existence policy gradient theorem provide simplify form gradient policy learn however behaviour policy necessarily attempt learn follow optimal policy give task existence theorem elusive work solve open problem provide first policy policy gradient theorem key derivation use emphatic weight develop new actor critic algorithm call actor critic emphatic weight ace approximate simplify gradients provide theorem demonstrate simple counterexample previous policy policy gradient methods particularly offpac dpg converge wrong solution whereas ace find optimal solution',\n",
              "  'large scale network model use neurons static nonlinearities produce analog output despite fact information process brain predominantly carry dynamic neurons produce discrete pulse call spike research spike base computation impede lack efficient supervise learn algorithm spike neural network present gradient descent method optimize spike network model introduce differentiable formulation spike dynamics derive exact gradient calculation demonstration train recurrent spike network two dynamic task one require optimize fast millisecond spike base interactions efficient encode information delay memory task extend duration second result show gradient descent approach indeed optimize network dynamics time scale individual spike well behavioral time scale conclusion method yield general purpose supervise learn algorithm spike neural network facilitate investigations spike base computations',\n",
              "  'adapt ideas underlie success deep learn continuous action domain present actor critic model free algorithm base deterministic policy gradient operate continuous action space use learn algorithm network architecture hyper parameters algorithm robustly solve simulate physics task include classic problems cartpole swing dexterous manipulation legged locomotion car drive algorithm able find policies whose performance competitive find plan algorithm full access dynamics domain derivatives demonstrate many task algorithm learn policies end end directly raw pixel input',\n",
              "  'accurate exposure key capture high quality photos computational photography especially mobile phone limit size camera modules inspire luminosity mask usually apply professional photographers paper develop novel algorithm learn local exposures deep reinforcement adversarial learn specific segment image sub image reflect variations dynamic range exposures accord raw low level feature base sub image local exposure sub image automatically learn virtue policy network sequentially reward learn globally design strike balance overall exposures aesthetic evaluation function approximate discriminator generative adversarial network reinforcement learn adversarial learn train collaboratively asynchronous deterministic policy gradient generative loss approximation simply algorithmic architecture also prove feasibility leverage discriminator value function employ local exposure retouch raw input image respectively thus deliver multiple retouch image different exposures fuse exposure blend extensive experiment verify algorithms superior state art methods term quantitative accuracy visual illustration',\n",
              "  'study problem policy policy evaluation oppe contrast prior work consider estimate individual policy value average policy value accurately draw inspiration recent work causal reason propose new finite sample generalization error bind value estimate mdp model use upper bind objective develop learn algorithm mdp model balance representation show approach yield substantially lower mse common synthetic benchmarks hiv treatment simulation domain',\n",
              "  'semantic scene completion predict volumetric occupancy object category scene help intelligent agents understand interact surround work propose disentangle framework sequentially carry semantic segmentation reprojection semantic scene completion three stage framework three advantage explicit semantic segmentation significantly boost performance flexible fusion ways sensor data bring good extensibility progress subtask promote holistic performance experimental result show regardless input single depth rgb framework generate high quality semantic scene completion outperform state art approach synthetic real datasets',\n",
              "  'propose novel randomize first order optimization method sega sketch gradient method progressively throughout iterations build variance reduce estimate gradient random linear measurements sketch gradient provide iteration oracle iteration sega update current estimate gradient sketch project operation use information provide latest sketch subsequently use compute unbiased estimate true gradient random relaxation procedure unbiased estimate use perform gradient step unlike standard subspace descent methods coordinate descent sega use optimization problems non separable proximal term provide general convergence analysis prove linear convergence strongly convex objectives special case coordinate sketch sega enhance various techniques importance sample minibatching acceleration rate small constant factor identical best know rate coordinate descent',\n",
              "  'despite remarkable advance image synthesis research exist work often fail manipulate image context large geometric transformations synthesize person image condition arbitrary pose one representative examples generation quality largely rely capability identify model arbitrary transformations different body part current generative model often build local convolutions overlook key challenge heavy occlusions different view dramatic appearance change distinct geometric change happen part cause arbitrary pose manipulations paper aim resolve challenge induce geometric variability spatial displacements via new soft gate warp generative adversarial network warp gin compose two stag first synthesize target part segmentation map give target pose depict region level spatial layouts guide image synthesis higher level structure constraints warp gin equip soft gate warp block learn feature level map render textures original image generate segmentation map warp gin capable control different transformation degrees give distinct target pose moreover propose warp block light weight flexible enough inject network human perceptual study quantitative evaluations demonstrate superiority warp gin significantly outperform exist methods two large datasets',\n",
              "  'tremendous recent progress equilibrium find algorithms zero sum imperfect information extensive form game puzzle gap theory practice first order methods significantly better theoretical convergence rat counterfactual regret minimization cfr variant despite cfr variants favor practice experiment first order methods conduct small medium size game methods complicate implement set cfr variants enhance extensively decade perform well practice paper show particular first order method state art variant excessive gap technique instantiate dilate entropy distance function efficiently solve large real world problems competitively cfr variants show large endgames encounter libratus poker recently beat top human poker specialist professionals limit texas hold show experimental result variant excessive gap technique well prior version introduce numerically friendly implementation smooth best response computation associate first order methods extensive form game solve present knowledge first gpu implementation first order method extensive form game present comparisons several excessive gap technique cfr variants',\n",
              "  'convolutional neural network cnns inherently subject invariable filter aggregate local input topological structure cause cnns allow manage data euclidean grid like structure image ones non euclidean graph structure traffic network broaden reach cnns develop structure aware convolution eliminate invariance yield unify mechanism deal euclidean non euclidean structure data technically filter structure aware convolution generalize univariate function capable aggregate local input diverse topological structure since infinite parameters require determine univariate function parameterize filter number learnable parameters context function approximation theory replace classical convolution cnns structure aware convolution structure aware convolutional neural network sacnns readily establish extensive experiment eleven datasets strongly evidence sacnns outperform current model various machine learn task include image classification cluster text categorization skeleton base action recognition molecular activity detection taxi flow prediction',\n",
              "  'recent years supervise learn convolutional network cnns see huge adoption computer vision applications comparatively unsupervised learn cnns receive less attention work hope help bridge gap success cnns supervise learn unsupervised learn introduce class cnns call deep convolutional generative adversarial network dcgans certain architectural constraints demonstrate strong candidate unsupervised learn train various image datasets show convince evidence deep convolutional adversarial pair learn hierarchy representations object part scenes generator discriminator additionally use learn feature novel task demonstrate applicability general image representations',\n",
              "  'recurrent neural network powerful tool understand model computation representation populations neurons continuous variable rate model network analyze apply extensively purpose however neurons fire action potentials discrete nature spike important feature neural circuit dynamics despite significant advance train recurrently connect spike neural network remain challenge present procedure train recurrently connect spike network generate dynamical pattern autonomously produce complex temporal output base integrate network input model physiological data procedure make use continuous variable network identify target train input spike model neurons surprisingly able construct spike network duplicate task perform continuous variable network relatively minor expansion number neurons approach provide novel view significance appropriate use fire rate model useful approach build model spike network use address important question representation computation neural systems'],\n",
              " ['present novel newton type method distribute optimization particularly well suit stochastic optimization learn problems quadratic objectives method enjoy linear rate convergence provably emph improve data size require essentially constant number iterations reasonable assumptions provide theoretical empirical evidence advantage method compare approach one shoot parameter average admm',\n",
              "  'propose structure adaptive variant state art stochastic variance reduce gradient algorithm katyusha regularize empirical risk minimization propose method able exploit intrinsic low dimensional structure solution sparsity low rank enforce non smooth regularization achieve even faster convergence rate provable algorithmic improvement do restart katyusha algorithm accord restrict strong convexity constants demonstrate effectiveness approach via numerical experiment',\n",
              "  'introduce new approach decomposable submodular function minimization dsfm exploit incidence relations incidence relations describe variables effectively influence component function properly utilize allow improve convergence rat dsfm solvers main result include precise parametrization dsfm problem base incidence relations development new scalable alternative projections parallel coordinate descent methods accompany rigorous analysis convergence rat'],\n",
              " ['describe iterative procedure optimize policies guarantee monotonic improvement make several approximations theoretically justify procedure develop practical algorithm call trust region policy optimization trpo algorithm similar natural policy gradient methods effective optimize large nonlinear policies neural network experiment demonstrate robust performance wide variety task learn simulate robotic swim hop walk gaits play atari game use image screen input despite approximations deviate theory trpo tend give monotonic improvement little tune hyperparameters',\n",
              "  'attention network multimodal learn provide efficient way utilize give visual information selectively however computational cost learn attention distributions every pair multimodal input channel prohibitively expensive solve problem attention build two separate attention distributions modality neglect interaction multimodal input paper propose bilinear attention network ban find bilinear attention distributions utilize give vision language information seamlessly ban consider bilinear interactions among two group input channel low rank bilinear pool extract joint representations pair channel furthermore propose variant multimodal residual network exploit eight attention map ban efficiently quantitatively qualitatively evaluate model visual question answer vqa flickr entities datasets show ban significantly outperform previous methods achieve new state arts datasets',\n",
              "  'recent work suggest enhance bloom filter use pre filter base apply machine learn determine function model data set bloom filter mean represent model learn bloom filter follow outcomes clarify guarantee associate structure show estimate size learn function must obtain order obtain improve performance provide simple method sandwich optimize learn bloom filter propose design analysis approach learn bloomier filter base model approach',\n",
              "  'stochastic gradient descent sgd popular algorithm achieve state art performance variety machine learn task several researchers recently propose scheme parallelize sgd require performance destroy memory lock synchronization work aim show use novel theoretical analysis algorithms implementation sgd implement without lock present update scheme call hogwild allow processors access share memory possibility overwrite work show associate optimization problem sparse mean gradient update modify small part decision variable hogwild achieve nearly optimal rate convergence demonstrate experimentally hogwild outperform alternative scheme use lock order magnitude',\n",
              "  'paper introduce versatile filter construct efficient convolutional neural network consider demand efficient deep learn techniques run cost effective hardware number methods develop learn compact neural network work aim slim filter different ways investigate small sparse binarized filter contrast treat filter additive perspective series secondary filter derive primary filter secondary filter inherit primary filter without occupy storage unfold computation could significantly enhance capability filter integrate information extract different receptive field besides spatial versatile filter additionally investigate versatile filter channel perspective new techniques general upgrade filter exist cnns experimental result benchmark datasets neural network demonstrate cnns construct versatile filter able achieve comparable accuracy original filter require less memory flop'],\n",
              " ['consider problem online learn linear contextual bandits set also strong individual fairness constraints govern unknown similarity metric constraints demand select similar action individuals approximately equal probability dhprz may odds optimize reward thus model settings profit social policy tension assume learn unknown mahalanobis similarity metric weak feedback identify fairness violations quantify extent intend represent interventions regulator know unfairness see nevertheless enunciate quantitative fairness metric individuals main result algorithm adversarial context set number fairness violations depend logarithmically obtain optimal sqrt regret bind best fair policy',\n",
              "  'deep reinforcement learn successfully solve many challenge control task real world applicability limit inability ensure safety learn policies propose approach verifiable reinforcement learn train decision tree policies represent complex policies since nonparametric yet efficiently verify use exist techniques since highly structure challenge decision tree policies difficult train propose viper algorithm combine ideas model compression imitation learn learn decision tree policies guide dnn policy call oracle function show substantially outperform two baselines use viper learn provably robust decision tree policy variant atari pong symbolic state space learn decision tree policy toy game base pong provably never lose iii learn provably stable decision tree policy cart pole case decision tree policy achieve performance equal original dnn policy',\n",
              "  'contextual bandit literature traditionally focus algorithms address exploration exploitation tradeoff particular greedy algorithms exploit current estimate without exploration may sub optimal general however exploration free greedy algorithms desirable practical settings exploration may costly unethical clinical trials surprisingly find simple greedy algorithm rate optimal achieve asymptotically optimal regret sufficient randomness observe contexts covariates prove always case two arm bandit general class context distributions satisfy condition term covariate diversity furthermore even absent condition show greedy algorithm rate optimal positive probability thus standard bandit algorithms may unnecessarily explore motivate result introduce greedy first new algorithm use observe contexts reward determine whether follow greedy algorithm explore prove algorithm rate optimal without additional assumptions context distribution number arm extensive simulations demonstrate greedy first successfully reduce exploration outperform exist exploration base contextual bandit algorithms thompson sample upper confidence bind ucb',\n",
              "  'consider problem online learn linear contextual bandits set also strong individual fairness constraints govern unknown similarity metric constraints demand select similar action individuals approximately equal probability dhprz may odds optimize reward thus model settings profit social policy tension assume learn unknown mahalanobis similarity metric weak feedback identify fairness violations quantify extent intend represent interventions regulator know unfairness see nevertheless enunciate quantitative fairness metric individuals main result algorithm adversarial context set number fairness violations depend logarithmically obtain optimal sqrt regret bind best fair policy',\n",
              "  'humans routinely retrace path novel environment forward backwards despite uncertainty motion paper present approach give demonstration path first network generate path equip second network observe world decide act order retrace path noisy actuation change environment two network optimize end end train time evaluate method two realistic simulators perform path follow forward backwards experiment show approach outperform classical approach solve task well number baselines',\n",
              "  'humans make repeat choices among options imperfectly know reward outcomes important problem psychology neuroscience often study use multi arm bandits also frequently study machine learn present data human stationary bandit experiment vary average abundance variability reward availability mean variance reward rate distributions surprisingly find subject significantly underestimate prior mean reward rat base self report end game reward expectation non choose arm previously human learn bandit task find well capture bayesian ideal learn model dynamic belief model dbm albeit incorrect generative assumption temporal structure humans assume reward rat change time even though actually fix find pessimism bias bandit task well capture prior mean dbm fit human choices poorly capture prior mean fix belief model fbm alternative bayesian model correctly assume reward rat constants pessimism bias also incompletely capture simple reinforcement learn model commonly use neuroscience psychology term fit initial value seem sub optimal thus mysterious humans underestimate prior reward expectation simulations show underestimate prior mean help maximize long term gain observer assume volatility reward rat stable utilize softmax decision policy instead optimal one obtainable dynamic program raise intrigue possibility brain underestimate reward rat compensate incorrect non stationarity assumption generative model simplify decision policy'],\n",
              " ['algorithmically construct multi output gaussian process priors satisfy linear differential equations approach attempt parametrize solutions equations use bner base successful push forward gaussian process along paramerization desire prior consider several examples physics geomathmatics control among full inhomogeneous system maxwell equations bring together stochastic learn computeralgebra novel way combine noisy observations precise algebraic computations',\n",
              "  'note consider normalize gradient descent ngd natural modification classical gradient descent optimization problems serious shortcoming non convex problems may take arbitrarily long escape neighborhood saddle point issue make convergence arbitrarily slow particularly high dimensional non convex problems relative number saddle point often large paper focus continuous time descent show contrary standard ngd escape saddle point quickly particular show ngd almost never converge saddle point time require ngd escape ball radius saddle point sqrt kappa kappa condition number hessian application result global convergence time bind establish ngd mild assumptions',\n",
              "  'present shapenet richly annotate large scale repository shape represent cad model object shapenet contain model multitude semantic categories organize wordnet taxonomy collection datasets provide many semantic annotations model consistent rigid alignments part bilateral symmetry plan physical size keywords well plan annotations annotations make available public web base interface enable data visualization object attribute promote data drive geometric analysis provide large scale quantitative benchmark research computer graphics vision time technical report shapenet index model model classify categories wordnet synsets report describe shapenet effort whole provide detail currently available datasets summarize future plan',\n",
              "  'wide adoption dnns give birth unrelenting compute requirements force datacenter operators adopt domain specific accelerators train accelerators typically employ densely pack full precision float point arithmetic maximize performance per area ongoing research efforts seek increase performance density replace float point fix point arithmetic however significant roadblock attempt fix point narrow dynamic range insufficient dnn train convergence identify block float point bfp promise alternative representation since exhibit wide dynamic range enable majority dnn operations perform fix point logic unfortunately bfp alone introduce several limitations preclude direct applicability work introduce hbfp hybrid bfp approach perform dot products bfp operations float point hbfp deliver best worlds high accuracy float point superior hardware density fix point wide variety model show hbfp match float point accuracy enable hardware implementations deliver higher throughput'],\n",
              " ['machine learn model vulnerable adversarial examples small change image cause computer vision model make mistake identify school bus ostrich however still open question whether humans prone similar mistake address question leverage recent techniques transfer adversarial examples computer vision model know parameters architecture model unknown parameters architecture match initial process human visual system find adversarial examples strongly transfer across computer vision model influence classifications make time limit human observers'],\n",
              " ['paper address graph match problem follow recent work cite zaslavskiy path vestner analyze generalize idea concave relaxations introduce concepts emph conditionally concave emph probably conditionally concave energies polytopes show encapsulate many instance graph match problem include match euclidean graph graph surface prove local minima probably conditionally concave energies general match polytopes doubly stochastic high probability extreme point match polytope permutations',\n",
              "  'ensembles randomize decision tree usually refer random forest widely use classification regression task machine learn statistics random forest achieve competitive predictive performance computationally efficient train test make excellent candidates real world prediction task popular random forest variants breiman random forest extremely randomize tree operate batch train data online methods greater demand exist online random forest however require train data batch counterpart achieve comparable predictive performance work use mondrian process roy teh construct ensembles random decision tree call mondrian forest mondrian forest grow incremental online fashion remarkably distribution online mondrian forest batch mondrian forest mondrian forest achieve competitive predictive performance comparable exist online random forest periodically train batch random forest order magnitude faster thus represent better computation accuracy tradeoff',\n",
              "  'deep reinforcement learn successfully solve many challenge control task real world applicability limit inability ensure safety learn policies propose approach verifiable reinforcement learn train decision tree policies represent complex policies since nonparametric yet efficiently verify use exist techniques since highly structure challenge decision tree policies difficult train propose viper algorithm combine ideas model compression imitation learn learn decision tree policies guide dnn policy call oracle function show substantially outperform two baselines use viper learn provably robust decision tree policy variant atari pong symbolic state space learn decision tree policy toy game base pong provably never lose iii learn provably stable decision tree policy cart pole case decision tree policy achieve performance equal original dnn policy'],\n",
              " ['paper try organize machine teach coherent set ideas idea present vary along dimension collection dimension form problem space machine teach exist teach problems characterize space hope organization allow gain deeper understand individual teach problems discover connections among identify gap field'],\n",
              " ['goal orient dialog give attention due numerous applications artificial intelligence goal orient dialogue task occur questioner ask action orient question answerer respond intent let questioner know correct action take ask adequate question deep learn reinforcement learn recently apply however approach struggle find competent recurrent neural questioner owe complexity learn series sentence motivate theory mind propose answerer questioner mind aqm novel information theoretic algorithm goal orient dialog aqm questioner ask infer base approximate probabilistic model answerer questioner figure answerer intention via select plausible question explicitly calculate information gain candidate intentions possible answer question test framework two goal orient visual dialog task mnist count dialog guesswhat experiment aqm outperform comparative algorithms large margin'],\n",
              " ['bilinear model show achieve impressive performance wide range visual task semantic segmentation fine grain recognition face recognition however bilinear feature high dimensional typically order hundreds thousands million make impractical subsequent analysis propose two compact bilinear representations discriminative power full bilinear representation thousand dimension compact representations allow back propagation classification errors enable end end optimization visual recognition system compact bilinear representations derive novel kernelized analysis bilinear pool provide insights discriminative power bilinear pool platform research compact pool methods experimentation illustrate utility propose representations image classification shoot learn across several datasets'],\n",
              " ['aim obtain interpretable expressive disentangle scene representation contain comprehensive structural textural information object previous scene representations learn neural network often uninterpretable limit single object lack knowledge work propose scene render network sdn address issue integrate disentangle representations semantics geometry appearance deep generative model scene encoder perform inverse graphics translate scene structure object wise representation decoder two components differentiable shape renderer neural texture generator disentanglement semantics geometry appearance support aware scene manipulation rotate move object freely keep consistent shape texture change object appearance without affect shape experiment demonstrate edit scheme base sdn superior counterpart',\n",
              "  'accurately answer question give image require combine observations general knowledge effortless humans reason general knowledge remain algorithmic challenge advance research direction novel reason correct answer jointly consider entities show challenge fvqa dataset lead improvement accuracy around compare state art',\n",
              "  'reason play essential role visual question answer vqa multi step dynamic reason often necessary answer complex question example question place next bus right picture talk compound object bus right generate relation'],\n",
              " ['hash one popular powerful approximate nearest neighbor search techniques large scale image retrieval traditional hash methods first represent image shelf visual feature produce hash cod separate stage however shelf visual feature may optimally compatible hash code learn procedure may result sub optimal hash cod recently deep hash methods propose simultaneously learn image feature hash cod use deep neural network show superior performance traditional hash methods deep hash methods give supervise information form pairwise label triplet label current state art deep hash method dpsh cite feature base pairwise label perform image feature learn hash code learn simultaneously maximize likelihood pairwise similarities inspire dpsh cite feature propose triplet label base deep hash method aim maximize likelihood give triplet label experimental result show method outperform baselines cifar nus wide datasets include state art method dpsh cite feature previous triplet label base deep hash methods'],\n",
              " ['learn decision tree data difficult optimization problem widespread algorithm practice date base greedy growth tree structure recursively split nod possibly prune back final tree parameters decision function internal node approximately estimate minimize impurity measure give algorithm give input tree structure parameter value nod produce new tree smaller structure new parameter value provably lower leave unchanged misclassification error apply axis align oblique tree experiment show consistently outperform various algorithms highly scalable large datasets tree algorithm handle sparsity penalty learn sparse oblique tree structure subset original tree nonzero parameters combine best axis align oblique tree flexibility model correlate data low generalization error fast inference interpretable nod involve feature decision',\n",
              "  'paper compare different type recurrent units recurrent neural network rnns especially focus sophisticate units implement gate mechanism long short term memory lstm unit recently propose gate recurrent unit gru evaluate recurrent units task polyphonic music model speech signal model experiment reveal advance recurrent units indeed better traditional recurrent units tanh units also find gru comparable lstm'],\n",
              " ['tremendous recent progress equilibrium find algorithms zero sum imperfect information extensive form game puzzle gap theory practice first order methods significantly better theoretical convergence rat counterfactual regret minimization cfr variant despite cfr variants favor practice experiment first order methods conduct small medium size game methods complicate implement set cfr variants enhance extensively decade perform well practice paper show particular first order method state art variant excessive gap technique instantiate dilate entropy distance function efficiently solve large real world problems competitively cfr variants show large endgames encounter libratus poker recently beat top human poker specialist professionals limit texas hold show experimental result variant excessive gap technique well prior version introduce numerically friendly implementation smooth best response computation associate first order methods extensive form game solve present knowledge first gpu implementation first order method extensive form game present comparisons several excessive gap technique cfr variants',\n",
              "  'contextual bandit literature traditionally focus algorithms address exploration exploitation tradeoff particular greedy algorithms exploit current estimate without exploration may sub optimal general however exploration free greedy algorithms desirable practical settings exploration may costly unethical clinical trials surprisingly find simple greedy algorithm rate optimal achieve asymptotically optimal regret sufficient randomness observe contexts covariates prove always case two arm bandit general class context distributions satisfy condition term covariate diversity furthermore even absent condition show greedy algorithm rate optimal positive probability thus standard bandit algorithms may unnecessarily explore motivate result introduce greedy first new algorithm use observe contexts reward determine whether follow greedy algorithm explore prove algorithm rate optimal without additional assumptions context distribution number arm extensive simulations demonstrate greedy first successfully reduce exploration outperform exist exploration base contextual bandit algorithms thompson sample upper confidence bind ucb',\n",
              "  'societies often rely human experts take wide variety decisions affect members jail release decisions take judge stop frisk decisions take police officer accept reject decisions take academics context decision take expert typically choose uniformly random pool experts however decisions may imperfect due limit experience implicit bias faulty probabilistic reason improve accuracy fairness overall decision make process optimize assignment experts decisions'],\n",
              " ['draw attention important yet largely overlook aspect evaluate fairness automate decision make systems namely risk welfare considerations propose family measure correspond long establish formulations cardinal social welfare economics justify rawlsian conception fairness behind veil ignorance convex formulation welfare base measure fairness allow integrate constraint convex loss minimization pipeline empirical analysis reveal interest trade off proposal prediction accuracy group discrimination dwork notion individual fairness furthermore perhaps importantly work provide heuristic justification empirical evidence suggest lower bind measure often lead bound inequality algorithmic outcomes hence present first computationally feasible mechanism bound individual level inequality'],\n",
              " ['technical challenge deep learn recognize target class without see data zero shoot learn leverage semantic representations attribute class prototypes bridge source target class exist standard zero shoot learn methods may prone overfitting see data source class blind semantic representations target class paper study generalize zero shoot learn assume accessible target class unseen data train prediction unseen data make search source target class propose novel deep calibration network dcn approach towards generalize zero shoot learn paradigm enable simultaneous calibration deep network confidence source class uncertainty target class approach map visual feature image semantic representations class prototypes common embed space compatibility see data source target class maximize show superior accuracy approach state art benchmark datasets generalize zero shoot learn include awa cub sun apy',\n",
              "  'paper propose conceptually simple general framework call metagan shoot learn problems state art shoot classification model integrate metagan principled straightforward way introduce adversarial generator condition task augment vanilla shoot classification model ability discriminate real fake data argue gin base approach help shoot classifiers learn sharper decision boundary could generalize better show metagan framework extend supervise shoot learn model naturally cope unsupervised data different previous work semi supervise shoot learn algorithms deal semi supervision sample level task level give theoretical justifications strength metagan validate effectiveness metagan challenge shoot image classification benchmarks'],\n",
              " ['recurrent network spike neurons rsnns underlie astound compute learn capabilities brain compute learn capabilities rsnn model remain poor least comparison anns address two possible reason one rsnns brain randomly connect design accord simple rule start learn tabula rasa network rather rsnns brain optimize task evolution development prior experience detail optimization process largely unknown functional contribution approximate powerful optimization methods backpropagation time bptt'],\n",
              " ['suppose design matrix linear regression problem give response point hide unless explicitly request goal sample small number responses produce weight vector whose sum square loss point epsilon time minimum small jointly sample diverse subsets point crucial one method call volume sample unique desirable property weight vector produce unbiased estimate optimum therefore natural ask method offer optimal unbiased estimate term number responses need achieve epsilon loss approximation',\n",
              "  'consider problem learn optimal reserve price repeat auction non myopic bidders may bid strategically order gain future round even single round auction truthful previous algorithms empirical price provide non trivial regret round set general introduce algorithms obtain small regret non myopic bidders either market large bidder appear constant fraction round bidders impatient discount future utility factor mildly bound away one approach carefully control information reveal bidder build techniques differentially private online learn well recent line work jointly differentially private algorithms',\n",
              "  'duplicate removal critical step accomplish reasonable amount predictions prevalent proposal base object detection frameworks albeit simple effective previous algorithms utilize greedy process without make sufficient use properties input data work design new two stage framework effectively select appropriate proposal candidate object first stage suppress easy negative object proposals second stage select true positives reduce proposal set two stag share network structure encoder decoder form recurrent neural network rnn global attention context gate encoder scan proposal candidates sequential manner capture global context information feed decoder extract optimal proposals extensive experiment propose method outperform alternatives large margin'],\n",
              " ['significant interest able predict crimes happen example aid efficient task police protective measure aim model temporal spatial dependencies often exhibit violent crimes order make predictions temporal variation crimes typically follow pattern familiar time series analysis spatial pattern irregular vary smoothly across area instead find spatially disjoint regions exhibit correlate crime pattern indeterminate inter region correlation structure along low count discrete nature count serious crimes motivate propose forecast tool particular propose model crime count region use integer value first order autoregressive process take bayesian nonparametric approach flexibly discover cluster region specific time series describe account covariates within framework approach adjust seasonality demonstrate approach analysis weekly report violent crimes washington forecast outperform standard methods additionally provide useful tool prediction intervals',\n",
              "  'decision tree random forest well establish model offer good predictive performance also provide rich feature importance information practitioners often employ variable importance methods rely impurity base information methods remain poorly characterize theoretical perspective provide novel insights performance methods derive finite sample performance guarantee high dimensional set various model assumptions demonstrate effectiveness impurity base methods via extensive set simulations',\n",
              "  'several large scale deployments differential privacy use collect statistical information users however deployments periodically recollect data recompute statistics use algorithms design single use result systems provide meaningful privacy guarantee long time scale moreover exist techniques mitigate effect apply local model differential privacy systems use'],\n",
              " ['propose novel flexible anchor mechanism name metaanchor object detection frameworks unlike many previous detectors model anchor via predefined manner metaanchor anchor function could dynamically generate arbitrary customize prior box take advantage weight prediction metaanchor able work anchor base object detection systems retinanet compare predefined anchor scheme empirically find metaanchor robust anchor settings bound box distributions addition also show potential transfer task experiment coco detection task show metaanchor consistently outperform counterparts various scenarios'],\n",
              " ['accurately answer question give image require combine observations general knowledge effortless humans reason general knowledge remain algorithmic challenge advance research direction novel reason correct answer jointly consider entities show challenge fvqa dataset lead improvement accuracy around compare state art'],\n",
              " ['despite remarkable advance image synthesis research exist work often fail manipulate image context large geometric transformations synthesize person image condition arbitrary pose one representative examples generation quality largely rely capability identify model arbitrary transformations different body part current generative model often build local convolutions overlook key challenge heavy occlusions different view dramatic appearance change distinct geometric change happen part cause arbitrary pose manipulations paper aim resolve challenge induce geometric variability spatial displacements via new soft gate warp generative adversarial network warp gin compose two stag first synthesize target part segmentation map give target pose depict region level spatial layouts guide image synthesis higher level structure constraints warp gin equip soft gate warp block learn feature level map render textures original image generate segmentation map warp gin capable control different transformation degrees give distinct target pose moreover propose warp block light weight flexible enough inject network human perceptual study quantitative evaluations demonstrate superiority warp gin significantly outperform exist methods two large datasets',\n",
              "  'generative adversarial network gans technique learn generative model complex data distributions sample despite remarkable advance generate realistic image major shortcoming gans fact tend produce sample little diversity even train diverse datasets phenomenon know mode collapse focus much recent work study principled approach handle mode collapse call pack main idea modify discriminator make decisions base multiple sample class either real artificially generate draw analysis tool binary hypothesis test particular seminal result blackwell prove fundamental connection pack mode collapse show pack naturally penalize generators mode collapse thereby favor generator distributions less mode collapse train process numerical experiment benchmark datasets suggest pack provide significant improvements'],\n",
              " ['recent years supervise learn convolutional network cnns see huge adoption computer vision applications comparatively unsupervised learn cnns receive less attention work hope help bridge gap success cnns supervise learn unsupervised learn introduce class cnns call deep convolutional generative adversarial network dcgans certain architectural constraints demonstrate strong candidate unsupervised learn train various image datasets show convince evidence deep convolutional adversarial pair learn hierarchy representations object part scenes generator discriminator additionally use learn feature novel task demonstrate applicability general image representations',\n",
              "  'accurate exposure key capture high quality photos computational photography especially mobile phone limit size camera modules inspire luminosity mask usually apply professional photographers paper develop novel algorithm learn local exposures deep reinforcement adversarial learn specific segment image sub image reflect variations dynamic range exposures accord raw low level feature base sub image local exposure sub image automatically learn virtue policy network sequentially reward learn globally design strike balance overall exposures aesthetic evaluation function approximate discriminator generative adversarial network reinforcement learn adversarial learn train collaboratively asynchronous deterministic policy gradient generative loss approximation simply algorithmic architecture also prove feasibility leverage discriminator value function employ local exposure retouch raw input image respectively thus deliver multiple retouch image different exposures fuse exposure blend extensive experiment verify algorithms superior state art methods term quantitative accuracy visual illustration',\n",
              "  'people belong multiple communities word belong multiple topics book cover multiple genres overlap cluster commonplace many exist overlap cluster methods model person word book non negative weight combination exemplars belong solely one community small noise geometrically person point cone whose corner exemplars basic form encompass widely use mix membership stochastic blockmodel network degree correct variants well topic model lda show simple one class svm yield provably consistent parameter inference model scale large datasets experimental result several simulate real datasets show algorithm call svm cone accurate scalable'],\n",
              " ['convolutional neural network cnns recently achieve great success single image super resolution sisr however methods tend produce smooth output miss textural detail solve problems propose super resolution cliquenet srcliquenet reconstruct high resolution image better textural detail wavelet domain propose srcliquenet firstly extract set feature map low resolution image clique block group send set feature map clique sample module reconstruct image clique sample module consist four sub net predict high resolution wavelet coefficients four sub band since consider edge feature properties four sub band four sub net connect others learn coefficients four sub band jointly finally apply inverse discrete wavelet transform idwt output four sub net end clique sample module increase resolution reconstruct image extensive quantitative qualitative experiment benchmark datasets show method achieve superior performance state art methods',\n",
              "  'paper introduce versatile filter construct efficient convolutional neural network consider demand efficient deep learn techniques run cost effective hardware number methods develop learn compact neural network work aim slim filter different ways investigate small sparse binarized filter contrast treat filter additive perspective series secondary filter derive primary filter secondary filter inherit primary filter without occupy storage unfold computation could significantly enhance capability filter integrate information extract different receptive field besides spatial versatile filter additionally investigate versatile filter channel perspective new techniques general upgrade filter exist cnns experimental result benchmark datasets neural network demonstrate cnns construct versatile filter able achieve comparable accuracy original filter require less memory flop',\n",
              "  'humans routinely retrace path novel environment forward backwards despite uncertainty motion paper present approach give demonstration path first network generate path equip second network observe world decide act order retrace path noisy actuation change environment two network optimize end end train time evaluate method two realistic simulators perform path follow forward backwards experiment show approach outperform classical approach solve task well number baselines',\n",
              "  'generative recurrent neural network quickly train unsupervised manner model popular reinforcement learn environments compress spatio temporal representations world model extract feature feed compact simple policies train evolution achieve state art result various environments also train agent entirely inside environment generate internal world model transfer policy back actual environment interactive version paper available https worldmodels github',\n",
              "  'introduce approach convert mono audio record video camera spatial audio representation distribution sound full view sphere spatial audio important component immersive video view spatial audio microphones still rare current video production system consist end end trainable neural network separate individual sound source localize view sphere condition multi modal analysis audio video frame introduce several datasets include one film one collect wild youtube consist videos upload spatial audio train grind truth spatial audio serve self supervision mix mono track form input network use approach show possible infer spatial localization sound base synchronize video mono audio track'],\n",
              " ['propose simple yet effective approach spatiotemporal feature learn use deep dimensional convolutional network convnets train large scale supervise video dataset find three fold convnets suitable spatiotemporal feature learn compare convnets homogeneous architecture small convolution kernels layer among best perform architectures convnets learn feature namely convolutional simple linear classifier outperform state art methods different benchmarks comparable current best methods benchmarks addition feature compact achieve accuracy ucf dataset dimension also efficient compute due fast inference convnets finally conceptually simple easy train use',\n",
              "  'softmax output activation function model categorical probability distributions many applications deep learn however recent study reveal softmax bottleneck representational capacity neural network language model softmax bottleneck paper propose output activation function break softmax bottleneck without additional parameters analyze softmax bottleneck perspective output set log softmax identify cause softmax bottleneck basis analysis propose sigsoftmax compose multiplication exponential function sigmoid function sigsoftmax break softmax bottleneck experiment language model demonstrate sigsoftmax mixture sigsoftmax outperform softmax mixture softmax respectively'],\n",
              " ['real world applications education effective teacher adaptively choose next example teach base learner current state however exist work algorithmic machine teach focus batch set adaptivity play role paper study case teach consistent version space learners interactive set time step teacher provide example learner perform update teacher observe learner new state highlight adaptivity speed teach process consider exist model version space learners worst case model learner pick next hypothesis randomly version space preference base model learner pick hypothesis accord global preference inspire human teach propose new model learner pick hypotheses accord local preference define current hypothesis show model exhibit several desirable properties adaptivity play key role learner transition hypotheses smooth interpretable develop adaptive teach algorithms demonstrate result via simulation user study']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwlT-GfH6SYb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZwzmO4ht2N-",
        "outputId": "ad088661-0547-4112-dc22-ec35586240a6"
      },
      "source": [
        "#Just to see how many data points changed their position \n",
        "\n",
        "count = []\n",
        "for i in list2:\n",
        "  count.append(len(i))\n",
        "print(Reverse(count))\n",
        "count = []\n",
        "for i in list2_a:\n",
        "  count.append(len(i))\n",
        "print(count)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 16, 3, 3, 5, 17, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[9, 28, 8, 8, 12, 19, 1, 4, 1, 3, 3, 3, 1, 2, 3, 2, 2, 4, 3, 2, 1, 3, 2, 1, 1, 1, 4, 1, 6, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zok759ZUZPb"
      },
      "source": [
        "#Predicting the training data with KMeans\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "num_clusters = 7\n",
        "# Define kmeans model\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "# Fit the embedding with kmeans clustering.\n",
        "clustering_model.fit(embeddings)\n",
        "# Get the cluster id assigned to each news headline.\n",
        "clusterr = clustering_model.get_params\n",
        "cluster_assignment = clustering_model.labels_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dURFvYa5WDTj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbWrPqcvyGUX"
      },
      "source": [
        "predicted = clustering_model.predict(check_list3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkZrum9hyPKO",
        "outputId": "064689d7-e420-4849-8347-ffc5371a8689"
      },
      "source": [
        "predicted"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 5, 1, 1, 3, 1, 3, 1, 5, 1, 3, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 6,\n",
              "       2, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 1, 1, 2, 3, 1, 3, 1, 1,\n",
              "       1, 3, 3, 2, 3, 3, 1, 3, 0, 1, 3, 6, 3, 1, 1, 1, 3, 1, 2, 1, 3, 5,\n",
              "       3, 3, 3, 3, 5, 3, 3, 3, 1, 6, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 5,\n",
              "       5, 3, 0, 2, 1, 0, 1, 1, 1, 5, 1, 1, 3, 2, 2, 1, 1, 3, 1, 3, 1, 1,\n",
              "       3, 1, 1, 1, 3, 1, 3, 3, 1, 1, 3, 1, 1, 1, 1, 3, 5, 1, 1, 1, 1, 1,\n",
              "       1, 3, 3, 5, 3, 3, 1, 1, 1, 3, 3, 1, 5, 1, 1, 5, 1, 3, 1, 1, 5, 3,\n",
              "       3, 3, 1, 1, 1, 3, 5, 1, 3, 4, 3, 5, 1, 3, 1, 1, 5, 1, 6, 3, 3, 1,\n",
              "       1, 5, 1, 5, 3, 2, 3, 3, 5, 3, 3, 1, 1, 1, 1, 3, 3, 1, 3, 1, 3, 1,\n",
              "       1, 1, 2, 5, 1, 1, 1, 3, 2, 3, 3, 3, 3, 3, 3], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_S_gTZ9zTa0",
        "outputId": "80408273-7383-4822-8c21-94fc2e6a8053"
      },
      "source": [
        "# As I said before there are 2 lists ( 1 which will be clustered from KMeans and other from cosine similarity)\n",
        "# We clustered one list with cosine\n",
        "# Now we will cluster the other list with KMeans (trained on training data)\n",
        "\n",
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(predicted):\n",
        "    clustered_sentences[cluster_id].append(mydata_list[sentence_id])\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster  1\n",
            "['great progress make recently automatic image manipulation limit object centric image like face structure scene datasets work take step towards general scene level image edit develop automatic interaction free object removal model model learn find remove object general scene image use image level label unpaired data generative adversarial network gin framework achieve two key contributions two stage editor architecture consist mask generator image painter operate remove object novel gin base prior mask generator allow flexibly incorporate knowledge object shape experimentally show two datasets method effectively remove wide variety object use weak supervision', 'exist methods interactive image retrieval demonstrate merit integrate user feedback improve retrieval result however current systems rely restrict form user feedback binary relevance responses feedback base fix set relative attribute limit impact paper introduce new approach interactive image search enable users provide feedback via natural language allow natural effective interaction formulate task dialog base interactive image retrieval reinforcement learn problem reward dialog system improve rank target image dialog turn mitigate cumbersome costly process collect human machine conversations dialog system learn train system user simulator train describe differences target candidate image efficacy approach demonstrate footwear retrieval application experiment simulate real world data show propose learn framework achieve better accuracy supervise reinforcement learn baselines user feedback base natural language rather pre specify attribute lead effective retrieval result natural expressive communication interface', 'graph match receive persistent attention decades formulate quadratic assignment problem qap show large family function define separable function approximate discrete graph match continuous domain asymptotically vary approximation control parameters also study properties global optimality devise convex concave preserve extensions widely use lawler qap form theoretical find show potential derive new algorithms techniques graph match deliver solvers base two specific instance separable function state art performance method verify popular benchmarks', 'convert input binary code hash algorithm widely use approximate nearest neighbor search large scale image set due computation storage efficiency deep hash improve retrieval quality combine hash cod deep neural network however major difficulty deep hash lie discrete constraints impose network output generally make optimization hard work adopt greedy principle tackle hard problem iteratively update network toward probable optimal discrete solution iteration hash cod layer design implement approach strictly use sign function forward propagation maintain discrete constraints back propagation gradients transmit intactly front layer avoid vanish gradients addition theoretical derivation provide new perspective visualize understand effectiveness efficiency algorithm experiment benchmark datasets show scheme outperform state art hash methods supervise unsupervised task', 'recent advance deep learn show excite promise fill large hole natural image semantically plausible context aware detail impact fundamental image manipulation task object removal learn base methods significantly effective capture high level feature prior techniques handle low resolution input due memory limitations difficulty train even slightly larger image inpainted regions would appear blurry unpleasant boundaries become visible propose multi scale neural patch synthesis approach base joint optimization image content texture constraints preserve contextual structure also produce high frequency detail match adapt patch similar mid layer feature correlations deep classification network evaluate method imagenet paris streetview datasets achieve state art inpainting accuracy show approach produce sharper coherent result prior methods especially high resolution image']\n",
            "\n",
            "Cluster  2\n",
            "['risk management dynamic decision problems primary concern many field include financial investment autonomous drive healthcare mean variance function one widely use objective function risk management due simplicity interpretability exist algorithms mean variance optimization base multi time scale stochastic approximation whose learn rate schedule often hard tune asymptotic convergence proof paper develop model free policy search framework mean variance optimization finite sample error bind analysis local optima start point reformulation original mean variance function fenchel dual propose stochastic block coordinate ascent policy search algorithm asymptotic convergence guarantee last iteration solution convergence rate randomly pick solution provide applicability demonstrate several benchmark domains', 'multiagent domains cop non stationary agents change behaviors time time challenge problem agent usually require able quickly detect agent policy online interaction adapt policy accordingly paper study efficient policy detect reuse techniques play non stationary agents markov game propose new deep bpr algorithm extend recent bpr algorithm neural network value function approximator detect policy accurately propose textit rectify belief model take advantage textit opponent model infer agent policy reward signal behaviors instead directly store individual policies bpr introduce textit distil policy network serve policy library bpr use policy distillation achieve efficient online policy learn reuse deep bpr inherit advantage bpr empirically show better performance term detection accuracy cumulative reward speed convergence compare exist algorithms complex markov game raw visual input', 'propose novel adaptive test goodness fit computational cost linear number sample learn test feature best indicate differences observe sample reference model minimize false negative rate feature construct via stein method mean necessary compute normalise constant model analyse asymptotic bahadur efficiency new test prove mean shift alternative test always greater relative efficiency previous linear time kernel test regardless choice parameters test experiment performance method exceed earlier linear time test match exceed power quadratic time kernel test high dimension model structure may exploit goodness fit test perform far better quadratic time two sample test base maximum mean discrepancy sample draw model', 'mainstream caption model often follow sequential structure generate cap tions lead issue introduction irrelevant semantics lack diversity generate caption inadequate generalization performance paper present alternative paradigm image caption factorize caption procedure two stag extract explicit semantic representation give image construct caption base recursive compositional procedure bottom manner compare conventional ones paradigm better preserve semantic content explicit factorization semantics syntax use compositional generation procedure caption construction follow recursive structure naturally fit properties human language moreover propose compositional procedure require less data train generalize better yield diverse caption', 'sequential prediction problems imitation learn future observations depend previous predictions action violate common assumptions make statistical learn lead poor performance theory often practice recent approach provide stronger guarantee set remain somewhat unsatisfactory train either non stationary stochastic policies require large number iterations paper propose new iterative algorithm train stationary deterministic policy see regret algorithm online learn set show regret algorithm combine additional reduction assumptions must find policy good performance distribution observations induce sequential settings demonstrate new approach outperform previous approach two challenge imitation learn problems benchmark sequence label problem', 'probabilistic generative model provide powerful framework represent data avoid expense manual annotation typically need discriminative approach model selection generative set challenge however particularly likelihoods easily accessible address issue introduce statistical test relative similarity use determine two model generate sample significantly closer real world reference dataset interest use test statistic difference maximum mean discrepancies mmds reference dataset model dataset derive powerful low variance test base joint asymptotic distribution mmds reference model pair experiment deep generative model include variational auto encoder generative moment match network test provide meaningful rank model performance function parameter train settings', 'present novel unify deep learn framework capable learn domain invariant representation data across multiple domains realize adversarial train additional ability exploit domain specific information propose network able perform continuous cross domain image translation manipulation produce desirable output image accordingly addition result feature representation exhibit superior performance unsupervised domain adaptation also verify effectiveness propose model learn disentangle feature describe cross domain data', 'learn capture long range relations fundamental image video recognition exist cnn model generally rely increase depth model relations highly inefficient work propose double attention block novel component aggregate propagate informative global feature entire spatio temporal space input image videos enable subsequent convolution layer access feature entire space efficiently component design double attention mechanism two step first step gather feature entire space compact set second order attention pool second step adaptively select distribute feature location via another attention propose double attention block easy adopt plug exist deep neural network conveniently conduct extensive ablation study experiment image video recognition task evaluate performance image recognition task resnet equip double attention block outperform much larger resnet architecture imagenet dataset less number parameters less flop action recognition task propose model achieve state art result kinetics ucf datasets significantly higher efficiency recent work', 'active learn task use label data select additional point label goal fit accurate model fix budget label point binary classification active learn know produce faster rat passive learn broad range settings however regression restrictive structure tailor methods previously need obtain theoretically superior performance paper propose intuitive tree base active learn algorithm non parametric regression provable improvement random sample implement mondrian tree algorithm tune parameter free consistent minimax optimal lipschitz function', 'despite achieve impressive performance state art classifiers remain highly vulnerable small imperceptible adversarial perturbations vulnerability prove empirically intricate address paper study phenomenon adversarial perturbations assumption data generate smooth generative model derive fundamental upper bound robustness perturbations classification function prove existence adversarial perturbations transfer well across different classifiers small risk analysis robustness also provide insights onto key properties generative model smoothness dimensionality latent space conclude numerical experimental result show bound provide informative baselines maximal achievable robustness several datasets', 'study generalization classic isotonic regression problem allow separable nonconvex objective function focus case estimators use robust regression simple dynamic program approach allow solve problem within accuracy global minimum time linear dimension combine techniques convex case branch bind ideas form new algorithm problem naturally exploit shape objective function algorithm achieve best bound general nonconvex convex case linear log perform much faster practice straightforward dynamic program approach especially desire accuracy increase', 'generative adversarial network gin powerful subclass generative model despite rich research activity lead numerous interest gin algorithms still hard assess algorithm perform better others conduct neutral multi faceted large scale empirical study state art model evaluation measure find model reach similar score enough hyperparameter optimization random restart suggest improvements arise higher computational budget tune fundamental algorithmic change overcome limitations current metrics also propose several data set precision recall compute experimental result suggest future gin research base systematic objective evaluation procedures finally find evidence test algorithms consistently outperform non saturate gin introduce cite goodfellow generative', 'residual network resnet standard deep neural net architecture state art performance across numerous applications main premise resnets allow train layer focus fit residual previous layer output target output thus expect train network worse obtain remove residual layer train shallower network instead however due non convexity optimization problem clear resnets indeed achieve behavior rather get stick arbitrarily poor local minimum paper rigorously prove arbitrarily deep nonlinear residual units indeed exhibit behavior sense optimization landscape contain local minima value obtain linear predictor namely layer network notably show minimal assumptions precise network architecture data distribution loss function use also provide quantitative analysis approximate stationary point problem finally show certain tweak architecture train network standard stochastic gradient descent achieve objective value close better linear predictor', 'perform efficient inference learn direct probabilistic model presence continuous latent variables intractable posterior distributions large datasets introduce stochastic variational inference learn algorithm scale large datasets mild differentiability condition even work intractable case contributions two fold first show reparameterization variational lower bind yield lower bind estimator straightforwardly optimize use standard stochastic gradient methods second show datasets continuous latent variables per datapoint posterior inference make especially efficient fit approximate inference model also call recognition model intractable posterior use propose lower bind estimator theoretical advantage reflect experimental result', 'train deep neural network complicate fact distribution layer input change train parameters previous layer change slow train require lower learn rat careful parameter initialization make notoriously hard train model saturate nonlinearities refer phenomenon internal covariate shift address problem normalize layer input method draw strength make normalization part model architecture perform normalization train mini batch batch normalization allow use much higher learn rat less careful initialization also act regularizer case eliminate need dropout apply state art image classification model batch normalization achieve accuracy time fewer train step beat original model significant margin use ensemble batch normalize network improve upon best publish result imagenet classification reach top validation error test error exceed accuracy human raters', 'real world image recognition often challenge variability visual style include object textures light condition filter effect etc although variations deem implicitly handle train data deeper network recent advance image style transfer suggest also possible explicitly manipulate style information extend idea general visual recognition problems present batch instance normalization bin explicitly normalize unnecessary style image consider certain style feature play essential role discriminative task bin learn selectively normalize disturb style preserve useful style propose normalization module easily incorporate exist network architectures residual network surprisingly improve recognition performance various scenarios furthermore experiment verify bin effectively adapt completely different task like object classification style transfer control trade preserve remove style variations bin implement line code use popular deep learn frameworks', 'base non local prior distributions propose bayesian model selection bms procedure boundary detection sequence data multiple systematic mean change bms method effectively suppress non boundary spike point large instantaneous change speed algorithm reduce multiple change point series single change point detection problems establish consistency estimate number locations change point various prior distributions extensive simulation study conduct compare bms exist methods approach illustrate application magnetic resonance image guide radiation therapy data', 'long live autonomous agent able respond online novel instance task familiar domain act online require fast responses term rapid convergence especially task instance short duration applications involve interactions humans requirements problematic many establish methods learn act domains agent know task instance draw family relate task albeit without access label give instance choose act process policy reuse library rather policy learn scratch policy reuse agent prior knowledge class task form library policies learn sample task instance offline train phase formalise problem policy reuse present algorithm efficiently respond novel task instance reuse policy library exist policies choice base observe signal correlate policy performance achieve pose problem bayesian choice problem correspond notion optimal response computation response many case intractable therefore reduce computation cost posterior follow bayesian optimisation approach define set policy selection function balance exploration policy library exploitation previously try policies together model expect performance policy library correspond task instance validate method several simulate domains interactive short duration episodic task show rapid convergence unknown task variations', 'propose data efficient gaussian process base bayesian approach semi supervise learn problem graph propose model show extremely competitive performance compare state art graph neural network semi supervise learn benchmark experiment outperform neural network active learn experiment label scarce furthermore model require validation data set early stop control fit model view instance empirical distribution regression weight locally network connectivity motivate intuitive construction model bayesian linear model interpretation node feature filter operator relate graph laplacian method easily implement adapt shelf scalable variational inference algorithms gaussian process', 'propose sparse low rank tensor regression model relate univariate outcome feature tensor unit rank tensor decomposition coefficient tensor assume sparse structure parsimonious highly interpretable imply outcome relate feature distinct pathways may involve subsets feature dimension take divide conquer strategy simplify task set sparse unit rank tensor regression problems make computation efficient scalable unit rank tensor regression propose stagewise estimation procedure efficiently trace entire solution path show step size go zero stagewise solution paths converge exactly correspond regularize regression superior performance approach demonstrate various real world synthetic examples', 'approximate probability density tractable manner central task bayesian statistics variational inference popular technique achieve tractability choose relatively simple variational approximation borrow ideas classic boost framework recent approach attempt emph boost replace selection single density iteratively construct mixture densities order guarantee convergence previous work impose stringent assumptions require significant effort practitioners specifically require custom implementation greedy step call lmo every probabilistic model respect unnatural variational family truncate distributions work fix issue novel theoretical algorithmic insights theoretical side show boost satisfy relax smoothness assumption sufficient convergence functional frank wolfe algorithm furthermore rephrase lmo problem propose maximize residual elbo relbo replace standard elbo optimization theoretical enhancements allow black box implementation boost subroutine finally present stop criterion draw duality gap classic analyse exhaustive experiment illustrate usefulness theoretical algorithmic contributions', 'present learn base approach compute solutions certain hard problems approach combine deep learn techniques useful algorithmic elements classic heuristics central component graph convolutional network train estimate likelihood vertex graph whether vertex part optimal solution network design train synthesize diverse set solutions enable rapid exploration solution space via tree search present approach evaluate four canonical hard problems five datasets include benchmark satisfiability problems real social network graph hundred thousand nod experimental result demonstrate present approach substantially outperform recent deep learn work perform par highly optimize state art heuristic solvers hard problems experiment indicate approach generalize across datasets scale graph order magnitude larger use train', 'new class dependent random measure call compound random measure propose use normalize versions random measure priors bayesian nonparametric mixture model consider tractability allow properties compound random measure normalize compound random measure derive particular show compound random measure construct gamma sigma stable generalize gamma process marginals also derive several form laplace exponent characterize dependence evy copula correlation function slice sampler augment olya urn scheme sampler describe posterior inference normalize compound random measure use mix measure nonparametric mixture model data example discuss', 'introduce principled approach unsupervised structure learn deep neural network propose new interpretation depth inter layer connectivity conditional independencies input distribution encode hierarchically network structure thus depth network determine inherently propose method cast problem neural network structure learn problem bayesian network structure learn instead directly learn discriminative structure learn generative graph construct stochastic inverse construct discriminative graph prove conditional dependency relations among latent variables generative graph preserve class conditional discriminative graph demonstrate image classification benchmarks deepest layer convolutional dense common network replace significantly smaller learn structure maintain classification accuracy state art test benchmarks structure learn algorithm require small computational cost run efficiently standard desktop cpu', 'goal precipitation nowcasting predict future rainfall intensity local region relatively short period time previous study examine crucial challenge weather forecast problem machine learn perspective paper formulate precipitation nowcasting spatiotemporal sequence forecast problem input prediction target spatiotemporal sequence extend fully connect lstm lstm convolutional structure input state state state transition propose convolutional lstm convlstm use build end end trainable model precipitation nowcasting problem experiment show convlstm network capture spatiotemporal correlations better consistently outperform lstm state art operational rover algorithm precipitation nowcasting', 'holistic indoor scene understand refer jointly recover object bound box room layout iii camera pose exist methods either ineffective tackle problem partially paper propose end end model simultaneously solve three task real time give single rgb image essence propose method improve prediction parametrizing target box instead directly estimate target cooperative train across different modules contrast train modules individually specifically parametrize object bound box predictions several modules camera pose object attribute propose method provide two major advantage parametrization help maintain consistency image world thus largely reduce prediction variances coordinate constraints impose parametrization train different modules simultaneously call constraints cooperative losses enable joint train inference employ three cooperative losses bound box projections physical constraints estimate geometrically consistent physically plausible scene experiment sun rgb dataset show propose method significantly outperform prior approach layout estimation object detection camera pose estimation holistic scene understand', 'visual attention derive cognitive neuroscience facilitate human perception pertinent subset sensory data recently significant efforts make exploit attention scheme advance computer vision systems visual track often challenge track target object undergo large appearance change attention map facilitate visual track selectively pay attention temporal robust feature exist track detection approach mainly use additional attention modules generate feature weight classifiers equip mechanisms paper propose reciprocative learn algorithm exploit visual attention train deep classifiers propose algorithm consist fee forward backward operations generate attention map serve regularization term couple original classification loss function train deep classifier learn attend regions target object robust appearance change extensive experiment large scale benchmark datasets show propose attentive track method perform favorably state art approach', 'deep learn see remarkable developments last years many inspire neuroscience however main learn mechanism behind advance error backpropagation appear odds neurobiology introduce multilayer neuronal network model simplify dendritic compartments error drive synaptic plasticity adapt network towards global desire output contrast previous work model require separate phase synaptic learn drive local dendritic prediction errors continuously time errors originate apical dendrites occur due mismatch predictive input lateral interneurons activity actual top feedback use simple dendritic compartments different cell type model represent error normal activity within pyramidal neuron demonstrate learn capabilities model regression classification task show analytically approximate error backpropagation algorithm moreover framework consistent recent observations learn brain areas architecture cortical microcircuits overall introduce novel view learn dendritic cortical circuit brain may solve long stand synaptic credit assignment problem', 'deep neural network dnns recently show state art performance semantic segmentation task however still suffer problems poor boundary localization spatial fragment predictions difficulties lie requirement make dense predictions long path model since detail hard keep data go deeper layer instead work decompose difficult task two relative simple sub task seed detection require predict initial predictions without need wholeness preciseness similarity estimation measure possibility two nod belong class without need know class use one branch network one sub task apply cascade random walk base hierarchical semantics approximate complex diffusion process propagate seed information whole image accord estimate similarities propose difnet consistently produce improvements baseline model depth equivalent number parameters also achieve promise performance pascal voc pascal context dataset ourdifnet train end end without complex loss function', 'simple way improve performance almost machine learn algorithm train many different model data average predictions unfortunately make predictions use whole ensemble model cumbersome may computationally expensive allow deployment large number users especially individual model large neural net caruana collaborators show possible compress knowledge ensemble single model much easier deploy develop approach use different compression technique achieve surprise result mnist show significantly improve acoustic model heavily use commercial system distil knowledge ensemble model single model also introduce new type ensemble compose one full model many specialist model learn distinguish fine grain class full model confuse unlike mixture experts specialist model train rapidly parallel', 'stochastic convex optimization algorithms popular way train machine learn model large scale data scale train process model crucial popular algorithm stochastic gradient descent sgd serial method surprisingly hard parallelize paper propose efficient distribute stochastic optimization method combine adaptivity variance reduction techniques analysis yield linear speedup number machine constant memory footprint logarithmic number communication round critically approach black box reduction parallelize serial online learn algorithm streamline prior analysis allow leverage significant progress make design adaptive algorithms particular achieve optimal convergence rat without prior knowledge smoothness parameters yield robust algorithm reduce need hyperparameter tune implement algorithm spark distribute framework exhibit dramatic performance gain large scale logistic regression problems', 'zero shoot learn zsl aim recognize unseen object class without train sample regard form transfer learn see class unseen ones make possible learn projection feature space semantic space attribute space key zsl thus learn projection function robust often large domain gap see unseen class paper propose novel zsl model term domain invariant projection learn dipl model two novel components domain invariant feature self reconstruction task introduce see unseen class data result simple linear formulation cast zsl min min optimization problem solve problem non trivial novel iterative algorithm formulate solver rigorous theoretic algorithm analysis provide align two domains via learn projection share semantic structure among see unseen class explore via form superclasses semantic space extensive experiment show model outperform state art alternatives significant margins', 'recent methods learn linear subspace data corrupt outliers base convex nuclear norm optimization require dimension subspace number outliers sufficiently small sharp contrast recently propose dual principal component pursuit dpcp method provably handle subspaces high dimension solve non convex optimization problem sphere however geometric analysis base quantities difficult interpret amenable statistical analysis paper provide refine geometric analysis new statistical analysis show dpcp tolerate many outliers square number inliers thus improve upon provably correct robust pca methods also propose scalable project sub gradient descent method dpcp psgd solve dpcp problem show admit linear convergence even though underlie optimization problem non convex non smooth experiment road plane detection point cloud data demonstrate dpcp psgd efficient traditional ransac algorithm one popular methods computer vision applications', 'train discrete latent variable model remain challenge pass gradient information discrete units difficult propose new class smooth transformations base mixture two overlap distributions show propose transformation use train binary latent model either direct undirected priors derive new variational bind efficiently train boltzmann machine priors use bind develop dvae generative model global discrete prior hierarchy convolutional continuous variables experiment several benchmarks show overlap transformations outperform recent continuous relaxations discrete latent variables include gumbel softmax maddison jang discrete variational autoencoders rolfe', 'deep convolutional neural network demonstrate powerfulness variety applications however storage computational requirements largely restrict extensions mobile devices recently prune unimportant parameters use network compression acceleration consider spatial redundancy within filter cnn propose frequency domain dynamic prune scheme exploit spatial correlations frequency domain coefficients prune dynamically iteration different frequency band prune discriminatively give different importance accuracy experimental result demonstrate propose scheme outperform previous spatial domain counterparts large margin specifically achieve compression ratio theoretical inference speed resnet accuracy even better reference model cifar', 'conditional density estimation cde model deal estimate conditional distributions condition impose distribution input model cde challenge task fundamental trade model complexity representational capacity overfitting work propose extend model input latent variables use gaussian process map augment input onto sample conditional distribution bayesian approach allow model small datasets also provide machinery apply big data use stochastic variational inference approach use model densities even sparse data regions allow share learn structure condition illustrate effectiveness wide reach applicability model variety real world problems spatio temporal density estimation taxi drop off non gaussian noise model shoot learn omniglot image', 'present generative framework generalize zero shoot learn train test class necessarily disjoint build upon variational autoencoder base architecture consist probabilistic encoder probabilistic conditional decoder model generate novel exemplars see unseen class give respective class attribute exemplars subsequently use train shelf classification model one key aspects encoder decoder architecture feedback drive mechanism discriminator multivariate regressor learn map generate exemplars correspond class attribute vectors lead improve generator model ability generate leverage examples unseen class train classification model naturally help mitigate bias towards predict see class generalize zero shoot learn settings comprehensive set experiment show model outperform several state art methods several benchmark datasets standard well generalize zero shoot learn', 'probability estimation one fundamental task statistics machine learn however standard methods probability estimation discrete object handle object structure satisfactory manner paper derive general bayesian network formulation probability estimation leaf label tree enable flexible approximations generalize beyond observations show efficient algorithms learn bayesian network easily extend probability estimation challenge structure space experiment synthetic real data show methods greatly outperform current practice use empirical distribution well previous effort probability estimation tree', 'responses generate neural conversational model tend lack informativeness diversity present adversarial information maximization aim adversarial learn framework address two relate distinct problems foster response diversity leverage adversarial train allow distributional match synthetic real responses improve informativeness framework explicitly optimize variational lower bind pairwise mutual information query response empirical result automatic human evaluations demonstrate methods significantly boost informativeness diversity', 'propose new framework estimate generative model via adversarial net simultaneously train two model generative model capture data distribution discriminative model estimate probability sample come train data rather train procedure maximize probability make mistake framework correspond minimax two player game space arbitrary function unique solution exist recover train data distribution equal everywhere case define multilayer perceptrons entire system train backpropagation need markov chain unroll approximate inference network either train generation sample experiment demonstrate potential framework qualitative quantitatively evaluation generate sample', 'recent deep learn base approach show promise result challenge task inpainting large miss regions image methods generate visually plausible image structure textures often create distort structure blurry textures inconsistent surround areas mainly due ineffectiveness convolutional neural network explicitly borrow copy information distant spatial locations hand traditional texture patch synthesis approach particularly suitable need borrow textures surround regions motivate observations propose new deep generative model base approach synthesize novel image structure also explicitly utilize surround image feature reference network train make better predictions model fee forward fully convolutional neural network process image multiple hole arbitrary locations variable size test time experiment multiple datasets include face celeba celeba textures dtd natural image imagenet place demonstrate propose approach generate higher quality inpainting result exist ones code demo model available https github com jiahuiyu generative inpainting', 'introduce genetic gate network simple neural network combine gate vector compose binary genetic genes hide layer network method take advantage gradient free optimization gradient base optimization methods former effective problems multiple local minima latter quickly find local minima addition multiple chromosomes define different model make easy construct multiple model effectively apply problems require multiple model show apply typical reinforcement learn algorithms achieve large improvement sample efficiency performance', 'monte carlo sample high dimensional low sample settings important many machine learn task improve current methods sample euclidean space avoid independence instead consider ways couple sample show fundamental connections optimal transport theory lead novel sample algorithms provide new theoretical ground exist strategies compare new strategies prior methods improve sample efficiency include qmc study discrepancy explore find empirically observe benefit sample scheme reinforcement learn generative model', 'propose new type generative model high dimensional data learn manifold geometry data rather density generate point evenly along manifold contrast exist generative model represent data density strongly affect noise artifacts data collection demonstrate approach correct sample bias artifacts thus improve several downstream data analysis task cluster classification finally demonstrate approach especially useful biology despite advent single cell technologies rare subpopulations gene interaction relationships affect bias sample show sugar generate hypothetical populations able reveal intrinsic pattern mutual information relationships genes single cell rna sequence dataset hematopoiesis', 'consider high dimensional linear regression problem goal efficiently recover unknown vector beta noisy linear observations beta instead adopt regularization base assume underlie vectors beta rational entries denominator call rationality assumption propose new polynomial time algorithm task base seminal lenstra lenstra lovasz lll lattice basis reduction algorithm establish rationality assumption algorithm recover exactly vector beta large class distributions iid entries non zero noise prove successful small noise even learner access one observation furthermore prove case gaussian white noise log sufficiently large algorithm tolerate nearly optimal information theoretic level noise', 'quantization promise technique reduce model size memory footprint massive computation operations recurrent neural network rnns embed devices limit resources although extreme low bite quantization achieve impressive success convolutional neural network still suffer huge accuracy degradation rnns low bite precision paper first investigate accuracy degradation rnn model different quantization scheme distribution tensor value full precision model observation reveal due difference distributions weight activations different quantization methods suitable different part model base observation propose hitnet hybrid ternary recurrent neural network bridge accuracy gap full precision model quantize model hitnet develop hybrid quantization method quantize weight activations moreover introduce slop factor motivate prior work boltzmann machine activation function close accuracy gap full precision model quantize model overall hitnet quantize rnn model ternary value outperform state art quantization methods rnn model significantly test typical rnn model long short term memory lstm gate recurrent units gru result outperform previous work significantly example improve perplexity per word ppw ternary lstm penn tree bank ptb corpus state art result best knowledge full precision model ternary gru full precision model', 'batch normalization batchnorm widely adopt technique enable faster stable train deep neural network dnns despite pervasiveness exact reason batchnorm effectiveness still poorly understand popular belief effectiveness stem control change layer input distributions train reduce call internal covariate shift work demonstrate distributional stability layer input little success batchnorm instead uncover fundamental impact batchnorm train process make optimization landscape significantly smoother smoothness induce predictive stable behavior gradients allow faster train', 'identify study two common failure modes early train deep relu net give rigorous proof occur avoid fully connect convolutional residual architectures show first failure mode explode vanish mean activation length avoid initialize weight symmetric distribution variance fan resnets correctly scale residual modules prove second failure mode exponentially large variance activation length never occur residual net first failure mode avoid contrast fully connect net prove failure mode happen avoid keep constant sum reciprocals layer widths demonstrate empirically effectiveness theoretical result predict network able start train particular note many popular initializations fail criteria whereas correct initialization architecture allow much deeper network train', 'dominant object detection approach treat recognition region separately overlook crucial semantic correlations object one scene paradigm lead substantial performance drop face heavy long tail problems sample available rare class plenty confuse categories exist exploit diverse human commonsense knowledge reason large scale object categories reach semantic coherency within one image particularly present hybrid knowledge rout modules hkrm incorporate reason rout two kinds knowledge form explicit knowledge module structure constraints summarize linguistic knowledge share attribute relationships concepts implicit knowledge module depict implicit constraints common spatial layouts function region region graph modules individualize adapt coordinate visual pattern image guide specific knowledge form hkrm light weight general purpose extensible easily incorporate multiple knowledge endow detection network ability global semantic reason experiment large scale object detection benchmarks show hkrm obtain around improvement visualgenome categories ade term map', 'generate long coherent report describe medical image pose challenge bridge visual pattern informative human linguistic descriptions propose novel hybrid retrieval generation reinforce agent hrgr agent reconcile traditional retrieval base approach populate human prior knowledge modern learn base approach achieve structure robust diverse report generation hrgr agent employ hierarchical decision make procedure sentence high level retrieval policy module choose either retrieve template sentence shelf template database invoke low level generation module generate new sentence hrgr agent update via reinforcement learn guide sentence level word level reward experiment show approach achieve state art result two medical report datasets generate well balance structure sentence robust coverage heterogeneous medical report content addition model achieve highest detection precision medical abnormality terminologies improve human evaluation performance', 'provide simple efficient way compute low variance gradients continuous random variables reparameterization trick become technique choice train variety latent variable model however applicable number important continuous distributions introduce alternative approach compute reparameterization gradients base implicit differentiation demonstrate broader applicability apply gamma beta dirichlet von mises distributions use classic reparameterization trick experiment show propose approach faster accurate exist gradient estimators distributions', 'generative adversarial network gans powerful generative model suffer train instability recently propose wasserstein gin wgan make progress toward stable train gans sometimes still generate low quality sample fail converge find problems often due use weight clip wgan enforce lipschitz constraint critic lead undesired behavior propose alternative clip weight penalize norm gradient critic respect input propose method perform better standard wgan enable stable train wide variety gin architectures almost hyperparameter tune include layer resnets language model discrete data also achieve high quality generations cifar lsun bedrooms', 'give two candidate model set target observations address problem measure relative goodness fit two model propose two new statistical test nonparametric computationally efficient runtime complexity linear sample size interpretable unique advantage test produce set examples informative feature indicate regions data domain one model fit significantly better real world problem compare gin model test power new test match state art test relative goodness fit one order magnitude faster', 'deep neural network highly expressive model recently achieve state art performance speech visual recognition task expressiveness reason succeed also cause learn uninterpretable solutions could counter intuitive properties paper report two properties first find distinction individual high level units random linear combinations high level units accord various methods unit analysis suggest space rather individual units contain semantic information high layer neural network second find deep neural network learn input output mappings fairly discontinuous significant extend cause network misclassify image apply certain imperceptible perturbation find maximize network prediction error addition specific nature perturbations random artifact learn perturbation cause different network train different subset dataset misclassify input', 'present novel introspective variational autoencoder introvae model synthesize high resolution photographic image introvae capable self evaluate quality generate sample improve accordingly inference generator model jointly train introspective way one hand generator require reconstruct input image noisy output inference model normal vaes hand inference model encourage classify generate real sample generator try fool gans two famous generative frameworks integrate simple yet efficient single stream architecture train single stage introvae preserve advantage vaes stable train nice latent manifold unlike hybrid model vaes gans introvae require extra discriminators inference model serve discriminator distinguish generate real sample experiment demonstrate method produce high resolution photo realistic image celeba image comparable better state art gans', 'consider problem active feature acquisition goal sequentially select subset feature order achieve maximum prediction performance cost effective way test time work formulate active feature acquisition jointly learn problem train classifier environment agent decide either collect new feature test time cost sensitive manner also introduce novel encode scheme represent acquire subsets feature propose order invariant set encode feature level also significantly reduce search space agent evaluate model carefully design synthetic dataset active feature acquisition well several medical datasets framework show meaningful feature acquisition process diagnosis comply human knowledge outperform baselines term prediction performance well feature acquisition cost', 'indispensable component batch normalization successfully improve train deep neural network dnns mini batch normalize distribution internal representation hide layer however effectiveness would diminish scenario micro batch less sample mini batch since estimate statistics mini batch reliable insufficient sample limit room train larger model segmentation detection video relate problems require small batch constrain memory consumption paper present novel normalization method call kalman normalization improve accelerate train dnns particularly context micro batch specifically unlike exist solutions treat hide layer isolate system treat layer network whole system estimate statistics certain layer consider distributions precede layer mimic merit kalman filter resnet train imagenet lower error counterpart use batch size even use typical batch size still maintain advantage variants suffer performance degradation moreover naturally generalize many exist normalization variants obtain gain equip group normalization group kalman normalization gkn outperform variants large scale object detection segmentation task coco', 'knowledge distillation aim train lightweight classifier suitable provide accurate inference constrain resources multi label learn instead directly consume feature label pair classifier train teacher high capacity model whose train may resource hungry accuracy classifier train way usually suboptimal difficult learn true data distribution teacher alternative method adversarially train classifier discriminator two player game akin generative adversarial network gin ensure classifier learn true data distribution equilibrium game however may take excessively long time two player game reach equilibrium due high variance gradient update address limitations propose three player game name kdgan consist classifier teacher discriminator classifier teacher learn via distillation losses adversarially train discriminator via adversarial losses simultaneously optimize distillation adversarial losses classifier learn true data distribution equilibrium approximate discrete distribution learn classifier teacher concrete distribution concrete distribution generate continuous sample obtain low variance gradient update speed train extensive experiment use real datasets confirm superiority kdgan accuracy train speed', 'present framework learn disentangle interpretable jointly continuous discrete representations unsupervised manner augment continuous latent distribution variational autoencoders relax discrete distribution control amount information encode latent unit show continuous categorical factor variation discover automatically data experiment show framework disentangle continuous discrete generative factor various datasets outperform current disentangle methods discrete generative factor prominent', 'address problem learn semantic representation question measure similarity pair continuous distance metric work naturally extend word mover distance wmd represent text document normal distributions instead bag embed word learn metric measure dissimilarity two question minimum amount distance intent hide representation one question need travel match intent another question first learn repeat reformulate question infer intents normal distributions deep generative model variational auto encoder semantic similarity pair learn discriminatively optimal transport distance metric wasserstein novel variational siamese framework among know model read sentence individually propose framework achieve competitive result quora duplicate question dataset work shed light deep generative model approximate distributions semantic representations effectively measure semantic similarity meaningful distance metrics information theory', 'human scene understand use variety visual non visual cue perform inference object type pose relations physics rich universal cue exploit enhance scene understand integrate physical cue stability learn process use reinforce approach couple physics engine apply problem produce bound box pose object scene first show apply physics supervision exist scene understand model increase performance produce stable predictions allow train equivalent performance level fewer annotate train examples present novel architecture scene parse name prim cnn learn predict bound box well size translation rotation physics supervision prim cnn outperform exist scene understand approach problem finally show apply physics supervision unlabeled real image improve real domain transfer model train synthetic data', 'single image humans able perceive full shape object exploit learn shape priors everyday life contemporary single image reconstruction algorithms aim solve task similar fashion often end priors highly bias train class present algorithm generalizable reconstruction genre design capture generic class agnostic shape priors achieve inference network train procedure combine representations visible surface depth silhouette spherical shape representations visible non visible surface voxel base representations principled manner exploit causal structure shape give rise image experiment demonstrate genre perform well single view shape reconstruction generalize diverse novel object categories see train', 'object relationships critical content image understand scene graph provide structure description capture properties image however reason relationships object challenge recent work attempt solve problem generate scene graph image paper present novel method improve scene graph generation explicitly model inter dependency among entire object instance design simple effective relational embed module enable model jointly represent connections among relate object rather focus object isolation novel method significantly benefit two main part scene graph generation task object classification relationship classification use top basic faster cnn model achieve state art result visual genome benchmark push performance introduce global context encode module geometrical layout encode module validate final model linknet extensive ablation study demonstrate efficacy scene graph generation', 'deep neural network suffer fit catastrophic forget train small data one natural remedy problem data augmentation recently show effective however previous work either assume intra class variances always generalize new class employ naive generation methods hallucinate finite examples without model latent distributions work propose covariance preserve adversarial augmentation network overcome exist limit low shoot learn specifically novel generative adversarial network design model latent distribution novel class give relate base counterparts since direct estimation novel class inductively bias explicitly preserve covariance information variability base examples generation process empirical result show model generate realistic yet diverse examples lead substantial improvements imagenet benchmark state art', 'learn examples remain key challenge machine learn despite recent advance important domains vision language standard supervise deep learn paradigm offer satisfactory solution learn new concepts rapidly little data work employ ideas metric learn base deep neural feature recent advance augment neural network external memories framework learn network map small label support set unlabelled example label obviate need fine tune adapt new class type define one shoot learn problems vision use omniglot imagenet language task algorithm improve one shoot accuracy imagenet omniglot compare compete approach also demonstrate usefulness model language model introduce one shoot task penn treebank', 'train model generalize new domains test time problem fundamental importance machine learn work encode notion domain generalization use novel regularization function pose problem find regularization function learn learn meta learn framework objective domain generalization explicitly model learn regularizer make model train one domain perform well another domain experimental validations computer vision natural language datasets indicate method learn regularizers achieve good cross domain generalization', 'consider problem sample constrain distributions pose significant challenge non asymptotic analysis algorithmic design propose unify framework inspire classical mirror descent derive novel first order sample scheme prove general target distribution strongly convex potential framework imply existence first order algorithm achieve epsilon convergence suggest state art epsilon vastly improve important latent dirichlet allocation lda application mind specialize algorithm sample dirichlet posteriors derive first non asymptotic epsilon rate first order sample extend framework mini batch set prove convergence rat stochastic gradients available finally report promise experimental result lda real datasets', 'complete data matrix become ubiquitous problem modern data science motivations recommender systems computer vision network inference name one typical assumption low rank general model assume column correspond one several low rank matrices paper generalize model call mixture matrix completion mmc case entry correspond one several low rank matrices mmc accurate model recommender systems bring flexibility completion cluster problems make four fundamental contributions new model first show mmc theoretically possible well pose second give precise information theoretic identifiability condition third derive sample complexity mmc finally give practical algorithm mmc performance comparable state art simpler relate problems synthetic real data', 'model interpretability increasingly important component practical machine learn common form interpretability systems example base local global explanations one main challenge interpretability design explanation systems capture aspects explanation type order develop thorough understand model address challenge novel model call maple use local linear model techniques along dual interpretation random forest supervise neighborhood approach feature selection method maple two fundamental advantage exist interpretability systems first effective black box explanation system maple highly accurate predictive model provide faithful self explanations thus sidestep typical accuracy interpretability trade specifically demonstrate several uci datasets maple least accurate random forest produce faithful local explanations lime popular interpretability system second maple provide example base local explanations detect global pattern allow diagnose limitations local explanations', 'propose novel class network model temporal dyadic interaction data objective capture important feature often observe social interactions sparsity degree heterogeneity community structure reciprocity use mutually excite hawk process model interactions direct pair individuals intensity process allow interactions arise responses opposite interactions reciprocity due share interest individuals community structure sparsity degree heterogeneity build non time dependent part intensity function compound random measure follow todeschini conduct experiment real world temporal interaction data show propose model outperform compete approach link prediction lead interpretable parameters', 'scale model capacity vital success deep learn typical network necessary compute resources train time grow dramatically model size conditional computation promise way increase number parameters relatively small increase resources propose train algorithm flexibly choose neural modules base data process decomposition modules learn end end contrast exist approach train rely regularization enforce diversity module use apply modular network image recognition language model task achieve superior performance compare several baselines introspection reveal modules specialize interpretable contexts', 'multivariate time series usually contain large number miss value hinder application advance analysis methods multivariate time series data conventional approach address challenge miss value include mean zero imputation case deletion matrix factorization base imputation incapable model temporal dependencies nature complex distribution multivariate time series paper treat problem miss value imputation data generation inspire success generative adversarial network gin image generation propose learn overall distribution multivariate time series dataset gin use generate miss value sample different image data time series data usually incomplete due nature data record process modify gate recurrent unit employ gin model temporal irregularity incomplete time series experiment two multivariate time series datasets show propose model outperform baselines term accuracy imputation experimental result also show simple model impute data achieve state art result prediction task demonstrate benefit model downstream applications', 'bayesian optimisation refer class methods global optimisation function accessible via point evaluations typically use settings expensive evaluate common use case machine learn model selection possible analytically model generalisation performance statistical model resort noisy expensive train validation procedures choose best model conventional methods focus euclidean categorical domains context model selection permit tune scalar hyper parameters machine learn algorithms however surge interest deep learn increase demand tune neural network architectures work develop nasbot gaussian process base framework neural architecture search accomplish develop distance metric space neural network architectures compute efficiently via optimal transport program distance might independent interest deep learn community may find applications outside demonstrate nasbot outperform alternatives architecture search several cross validation base model selection task multi layer perceptrons convolutional neural network', 'non local methods exploit self similarity natural signal well study example image analysis restoration exist approach however rely nearest neighbor knn match fix feature space main hurdle optimize feature space application performance non differentiability knn selection rule overcome propose continuous deterministic relaxation knn selection maintain differentiability pairwise distance retain original knn limit temperature parameter approach zero exploit relaxation propose neural nearest neighbor block block novel non local process layer leverage principle self similarity use build block modern neural network architectures show effectiveness set reason task correspondence classification well image restoration include image denoising single image super resolution outperform strong convolutional neural network cnn baselines recent non local model rely knn selection hand choose feature space', 'incremental gradient algorithm hybrid stochastic gradient descent hsgd enjoy merit stochastic full gradient methods finite sum minimization problem however exist rate convergence analysis hsgd make replacement sample wrs restrict convex problems clear whether hsgd still carry advantage common practice without replacement sample wors non convex problems paper affirmatively answer open question show wors convex non convex problems still possible hsgd constant step size match full gradient descent rate convergence maintain comparable sample size independent incremental first order oracle complexity stochastic gradient descent special class finite sum problems linear prediction model convergence result improve case extensive numerical result confirm theoretical affirmation demonstrate favorable efficiency wors base hsgd', 'nonlocal neural network propose show effective several computer vision task nonlocal operations directly capture long range dependencies feature space paper study nature diffusion damp effect nonlocal network spectrum analysis weight matrices well train network propose new formulation nonlocal block new block learn nonlocal interactions also stable dynamics thus allow deeper nonlocal structure moreover interpret formulation general nonlocal model perspective make connections propose nonlocal network nonlocal model nonlocal diffusion process markov jump process', 'many classic methods show non local self similarity natural image effective prior image restoration however remain unclear challenge make use intrinsic property via deep network paper propose non local recurrent network nlrn first attempt incorporate non local operations recurrent neural network rnn image restoration main contributions work unlike exist methods measure self similarity isolate manner propose non local module flexibly integrate exist deep network end end train capture deep feature correlation location neighborhood fully employ rnn structure parameter efficiency allow deep feature correlation propagate along adjacent recurrent state new design boost robustness inaccurate correlation estimation due severely degrade image show essential maintain confine neighborhood compute deep feature correlation give degrade image contrast exist practice deploy whole image extensive experiment image denoising super resolution task conduct thank recurrent non local operations correlation propagation propose nlrn achieve superior result state art methods many fewer parameters', 'problem estimate unknown discrete distribution sample fundamental tenet statistical learn past decade attract significant research effort solve variety divergence measure surprisingly equally important problem estimate unknown markov chain sample still far understand consider two problems relate min max risk expect loss estimate unknown state markov chain sequential sample predict conditional distribution next sample respect divergence estimate transition matrix respect natural loss induce general divergence measure', 'paper provide theoretical understand word embed dimensionality motivate unitary invariance word embed propose pairwise inner product pip loss novel metric dissimilarity word embeddings use techniques matrix perturbation theory reveal fundamental bias variance trade dimensionality selection word embeddings bias variance trade shed light many empirical observations previously unexplained example existence optimal dimensionality moreover new insights discoveries like word embeddings robust fit reveal optimize bias variance trade pip loss explicitly answer open question dimensionality selection word embed', 'consider problem maximize submodular function give access approximate version submodular function heavily study wide variety discipline since use model many real world phenomena amenable optimization however many case phenomena observe approximately submodular approximation guarantee cease hold describe technique call sample mean approximation yield strong guarantee maximization submodular function approximate surrogates cardinality intersection matroid constraints particular show tight guarantee maximization cardinality constraint approximation intersection matroids', 'image caption model become increasingly successful describe content image restrict domains however model function wild example assistants people impair vision much larger number variety visual concepts must understand address problem teach image caption model new visual concepts label image object detection datasets since image label object class interpret partial caption formulate problem learn partially specify sequence data propose novel algorithm train sequence model recurrent neural network partially specify sequence represent use finite state automata context image caption method lift restriction previously require image caption model train pair image sentence corpora otherwise require specialize model architectures take advantage alternative data modalities apply approach exist neural caption model achieve state art result novel object caption task use coco dataset show train caption model describe new visual concepts open image dataset maintain competitive coco evaluation score', 'increase need run convolutional neural network cnn model mobile devices limit compute power memory resource encourage study efficient model design number efficient architectures propose recent years example mobilenet shufflenet mobilenetv however model heavily dependent depthwise separable convolution lack efficient implementation deep learn frameworks study propose efficient architecture name peleenet build conventional convolution instead imagenet ilsvrc dataset propose peleenet achieve higher accuracy time faster speed mobilenet mobilenetv nvidia meanwhile peleenet model size mobilenet propose real time object detection system combine peleenet single shoot multibox detector ssd method optimize architecture fast speed propose detection system name pelee achieve map mean average precision pascal voc map coco dataset speed fps iphone fps nvidia result coco outperform yolov consideration higher precision time lower computational cost time smaller model size code model open source', 'present simple general framework feature learn point cloud key success cnns convolution operator capable leverage spatially local correlation data represent densely grids image however point cloud irregular unordered thus direct convolve kernels feature associate point result desert shape information variant order address problems propose learn transformation input point use simultaneously weight input feature associate point permute latent potentially canonical order element wise product sum operations typical convolution operator apply transform feature propose method generalization typical cnns learn feature point cloud thus call pointcnn experiment show pointcnn achieve par better performance state art methods multiple challenge benchmark datasets task', 'formulate private learn model study intrinsic tradeoff privacy query complexity sequential learn model involve learner aim determine scalar value sequentially query external database receive binary responses meantime adversary observe learner query though responses try infer value objective learner obtain accurate estimate use small number query simultaneously protect privacy make provably difficult learn adversary main result provide tight upper lower bound learner query complexity function desire level privacy estimation accuracy also construct explicit query strategies whose complexity optimal additive constant', 'computable stein discrepancies deploy variety applications range sampler selection posterior inference approximate bayesian inference goodness fit test exist convergence determine stein discrepancies admit strong theoretical guarantee suffer computational cost grow quadratically sample size linear time stein discrepancies propose goodness fit test exhibit avoidable degradations test power even power explicitly optimize address shortcomings introduce feature stein discrepancies sds new family quality measure cheaply approximate use importance sample show construct sds provably determine convergence sample target develop high accuracy approximations random sds sds computable near linear time experiment sampler selection approximate posterior inference goodness fit test sds perform well better quadratic time ksds order magnitude faster compute', 'normalization techniques play important role support efficient often effective train deep neural network conventional methods explicitly normalize activations suggest add loss term instead new loss term encourage variance activations stable vary one random mini batch next prove encourage activations distribute around distinct modes also show input mixture two gaussians new loss would either join two together separate optimally lda sense depend prior probabilities finally able link new regularization term batchnorm method provide regularization perspective experiment demonstrate improvement accuracy batchnorm technique cnns fully connect network', 'artificial intelligence model limit ability solve new task faster without forget previously acquire knowledge recently emerge paradigm continual learn aim solve issue model learn various task sequential fashion work novel approach continual learn propose search best neural architecture come task via sophisticatedly design reinforcement learn strategies name reinforce continual learn method good performance prevent catastrophic forget also fit new task well experiment sequential classification task variants mnist cifar datasets demonstrate propose approach outperform exist continual learn alternatives deep network', 'multiplicative noise include dropout widely use regularize deep neural network dnns show effective wide range architectures task information perspective consider inject multiplicative noise dnn train network solve task noisy information pathways lead observation multiplicative noise tend increase correlation feature increase signal noise ratio information pathways however high feature correlation undesirable increase redundancy representations work propose non correlate multiplicative noise ncmn exploit batch normalization remove correlation effect simple yet effective way show ncmn significantly improve performance standard multiplicative noise image classification task provide better alternative dropout batch normalize network additionally present unify view ncmn shake shake regularization explain performance gain latter', 'object detection performance measure canonical pascal voc dataset plateaued last years best perform methods complex ensemble systems typically combine multiple low level image feature high level context paper propose simple scalable detection algorithm improve mean average precision map relative previous best result voc achieve map approach combine two key insights one apply high capacity convolutional neural network cnns bottom region proposals order localize segment object label train data scarce supervise pre train auxiliary task follow domain specific fine tune yield significant performance boost since combine region proposals cnns call method cnn regions cnn feature also compare cnn overfeat recently propose slide window detector base similar cnn architecture find cnn outperform overfeat large margin class ilsvrc detection dataset source code complete system available http www berkeley edu rbg rcnn', 'singular value decomposition principal component analysis one widely use techniques dimensionality reduction successful efficiently computable nevertheless plague well know well document sensitivity outliers recent work consider set point arbitrarily corrupt components yet applications svd pca robust collaborative filter bioinformatics malicious agents defective genes simply corrupt contaminate experiment may effectively yield entire point completely corrupt present efficient convex optimization base algorithm call outlier pursuit mild assumptions uncorrupted point satisfy standard generative assumption pca problems recover exact optimal low dimensional subspace identify corrupt point identification corrupt point conform low dimensional approximation paramount interest bioinformatics financial applications beyond techniques involve matrix decomposition use nuclear norm minimization however result setup approach necessarily differ considerably exist line work matrix completion matrix decomposition since develop approach recover correct column space uncorrupted matrix rather exact matrix problem one seek recover structure rather exact initial matrices techniques develop thus far rely certificate optimality fail present important extension methods allow treatment problems', 'paper focus semantic scene completion task produce complete voxel representation volumetric occupancy semantic label scene single view depth map observation previous work consider scene completion semantic label depth map separately however observe two problems tightly intertwine leverage couple nature two task introduce semantic scene completion network sscnet end end convolutional network take single depth image input simultaneously output occupancy semantic label voxels camera view frustum network use dilation base context module efficiently expand receptive field enable context learn train network construct suncg manually create large scale dataset synthetic scenes dense volumetric annotations experiment demonstrate joint model outperform methods address task isolation outperform alternative approach semantic scene completion task', 'generalization performance central goal machine learn particularly learn representations large neural network common strategy improve generalization use regularizers typically norm constrain parameters regularize hide layer neural network architecture however straightforward effective layer wise suggestions without theoretical guarantee improve performance work theoretically empirically analyze one model call supervise auto encoder neural network predict input reconstruction error target jointly provide novel generalization result linear auto encoders prove uniform stability base inclusion reconstruction error particularly improvement simplistic regularization norms even advance regularizations use auxiliary task empirically demonstrate across array architectures different number hide units activation function supervise auto encoder compare correspond standard neural network never harm performance significantly improve generalization', 'beyond local convolution network explore harness various external human knowledge endow network capability semantic global reason rather use separate graphical model crf constraints model broader dependencies propose new symbolic graph reason sgr layer perform reason group symbolic nod whose output explicitly represent different properties semantic prior knowledge graph cooperate local convolutions sgr constitute three modules primal local semantic vote module feature symbolic nod generate vote local representations graph reason module propagate information knowledge graph achieve global semantic coherency dual semantic local map module learn new associations evolve symbolic nod local representations accordingly enhance local feature sgr layer inject convolution layer instantiate distinct prior graph extensive experiment show incorporate sgr significantly improve plain convnets three semantic segmentation task one image classification task analyse show sgr layer learn share symbolic representations domains datasets different label set give universal knowledge graph demonstrate superior generalization capability', 'ability transfer reinforcement learn key towards build agent general artificial intelligence paper consider problem learn simultaneously transfer across environments task probably importantly learn sparse environment task pair possible combinations propose novel compositional neural network architecture depict meta rule compose policies environment task embeddings notably one main challenge learn embeddings jointly meta rule propose new train methods disentangle embeddings make distinctive signatures environments task effective build block compose policies experiment gridworld thor agent take input egocentric view show approach give rise high success rat environment task pair learn', 'shoot learn become essential produce model generalize examples work identify metric scale metric task condition important improve performance shoot algorithms analysis reveal simple metric scale completely change nature shoot algorithm parameter update metric scale provide improvements accuracy certain metrics mini imagenet way shoot classification task propose simple effective way condition learner task sample set result learn task dependent metric space moreover propose empirically test practical end end optimization procedure base auxiliary task train learn task dependent metric space result shoot learn model base task dependent scale metric achieve state art mini imagenet confirm result another shoot dataset introduce paper base cifar', 'deep learn model often parameters observations still perform well sometimes describe paradox work show experimentally despite huge number parameters deep neural network compress data losslessly even take cost encode parameters account compression viewpoint originally motivate use variational methods neural network however show variational methods provide surprisingly poor compression bound despite explicitly build minimize bound might explain relatively poor practical performance variational methods deep learn better encode methods import minimum description length mdl toolbox yield much better compression value deep network', 'real world learn systems practical limitations quality quantity train datasets collect consider system choose subset possible train examples still allow learn accurate generalizable model help address question draw inspiration highly efficient practical learn system human child use head mount cameras eye gaze trackers model foveated vision collect first person egocentric image represent highly accurate approximation train data toddlers visual systems collect everyday naturalistic learn contexts use state art computer vision learn model convolutional neural network help characterize structure data find child data produce significantly better object model egocentric data experience adults exactly environment use cnns model tool investigate properties child data may enable rapid learn find child data exhibit unique combination quality diversity many similar large high quality object view also greater number diversity rare view novel methodology analyze visual train data use children may reveal insights improve machine learn also may suggest new experimental tool better understand infant learn developmental psychology', 'many image image translation problems ambiguous single input image may correspond multiple possible output work aim model emph distribution possible output conditional generative model set ambiguity map distil low dimensional latent vector randomly sample test time generator learn map give input combine latent code output explicitly encourage connection output latent code invertible help prevent many one map latent code output train also know problem mode collapse produce diverse result explore several variants approach employ different train objectives network architectures methods inject latent code propose method encourage bijective consistency latent encode output modes present systematic comparison method variants perceptual realism diversity', 'leverage temporal dimension key question video analysis recent work suggest efficient approach video feature learn factorize convolutions separate components respectively spatial temporal convolutions temporal convolution however come implicit assumption feature map across time step well align feature locations aggregate assumption may overly strong practical applications especially action recognition motion serve crucial cue work propose new cnn architecture trajectorynet incorporate trajectory convolution new operation integrate feature along temporal dimension replace exist temporal convolution operation explicitly take account change content cause deformation motion allow visual feature aggregate along motion paths trajectories two large scale action recognition datasets namely something something kinetics propose network architecture achieve notable improvement strong baselines', 'program translation important tool migrate legacy code one language ecosystem build different language work first employ deep neural network toward tackle problem observe program translation modular procedure sub tree source tree translate correspond target sub tree step capture intuition design tree tree neural network translate source tree target one meanwhile develop attention mechanism tree tree model decoder expand one non terminal target tree attention mechanism locate correspond sub tree source tree guide expansion decoder evaluate program translation capability tree tree model several state art approach compare neural translation model observe approach consistently better baselines margin point approach improve previous state art program translation approach margin point translation real world project', 'recent success human action recognition deep learn methods mostly adopt supervise learn paradigm require significant amount manually label data achieve good performance however label collection expensive time consume process work propose unsupervised learn framework exploit unlabeled data learn video representations different previous work video representation learn unsupervised learn task predict motion multiple target view use video representation source view learn extrapolate cross view motion representation capture view invariant motion dynamics discriminative action addition propose view adversarial train method enhance learn view invariant feature demonstrate effectiveness learn representations action recognition multiple datasets', 'one core problems modern statistics approximate difficult compute probability densities problem especially important bayesian statistics frame inference unknown quantities calculation involve posterior density paper review variational inference method machine learn approximate probability densities optimization use many applications tend faster classical methods markov chain monte carlo sample idea behind first posit family densities find member family close target closeness measure kullback leibler divergence review ideas behind mean field variational inference discuss special case apply exponential family model present full example bayesian mixture gaussians derive variant use stochastic optimization scale massive data discuss modern research highlight important open problems powerful yet well understand hope write paper catalyze statistical research class algorithms', 'introduce variability maintain coherence core task learn generate utterances conversation standard neural encoder decoder model extensions use conditional variational autoencoder often result either trivial digressive responses overcome explore novel approach inject variability neural encoder decoder via use external memory mixture model namely variational memory encoder decoder vmed associate memory read mode latent mixture distribution timestep model capture variability observe sequential data natural conversations empirically compare propose model recent approach various conversational datasets result show vmed consistently achieve significant improvement others metric base qualitative evaluations']\n",
            "\n",
            "Cluster  3\n",
            "['focus machine learn branch beyond train classifiers single task investigate previously acquire knowledge source domain leverage facilitate learn relate target domain know inductive transfer learn three active line research independently explore transfer learn use neural network weight transfer model train source domain use initialization point network train target domain deep metric learn source domain use construct embed capture class structure source target domains shoot learn focus generalize well target domain base limit number label examples compare state art methods three paradigms also explore hybrid adapt embed methods use limit target domain data fine tune embeddings construct source domain data conduct systematic comparison methods variety domains vary number label instance available target domain well number target domain class reach three principal conclusions deep embeddings far superior compare weight transfer start point inter domain transfer model use hybrid methods robustly outperform every shoot learn every deep metric learn method previously propose mean error reduction state art among loss function discover embeddings histogram loss ustinova lempitsky robust hope result motivate unification research weight transfer deep metric learn shoot learn', 'dominant sequence transduction model base complex recurrent convolutional neural network encoder decoder configuration best perform model also connect encoder decoder attention mechanism propose new simple network architecture transformer base solely attention mechanisms dispense recurrence convolutions entirely experiment two machine translation task show model superior quality parallelizable require significantly less time train model achieve bleu wmt english german translation task improve exist best result include ensembles bleu wmt english french translation task model establish new single model state art bleu score train days eight gpus small fraction train cost best model literature show transformer generalize well task apply successfully english constituency parse large limit train data', 'adversarial learn embed deep network learn disentangle transferable representations domain adaptation exist adversarial domain adaptation methods may struggle align different domains multimodal distributions native classification problems paper present conditional adversarial domain adaptation principled framework condition adversarial adaptation model discriminative information convey classifier predictions conditional domain adversarial network cdans design two novel condition strategies multilinear condition capture cross covariance feature representations classifier predictions improve discriminability entropy condition control uncertainty classifier predictions guarantee transferability experiment testify propose approach exceed state art result five benchmark datasets', 'replace output layer deep neural net typically softmax function novel interpolate function propose end end train test algorithms new architecture compare classical neural net softmax function output activation surrogate interpolate function output activation combine advantage deep manifold learn new framework demonstrate follow major advantage first better applicable case insufficient train data second significantly improve generalization accuracy wide variety network algorithm implement pytorch code available https github com baowangmath dnn datadependentactivation', 'boltzmann machine powerful distributions show effective prior binary latent variables variational autoencoders vaes however previous methods train discrete vaes use evidence lower bind tighter importance weight bind propose two approach relax boltzmann machine continuous distributions permit train importance weight bound relaxations base generalize overlap transformations gaussian integral trick experiment mnist omniglot datasets show relaxations outperform previous discrete vaes boltzmann priors implementation reproduce result available', 'introduce generative neural machine translation gnmt latent variable architecture design model semantics source target sentence modify encoder decoder translation model add latent variable language agnostic representation encourage learn mean sentence gnmt achieve competitive bleu score pure translation task superior miss word source sentence augment model facilitate multilingual translation semi supervise learn without add parameters framework significantly reduce overfitting limit pair data available effective translate pair languages see train', 'batch normalization milestone technique development deep learn enable various network train however normalize along batch dimension introduce problems error increase rapidly batch size become smaller cause inaccurate batch statistics estimation limit usage train larger model transfer feature computer vision task include detection segmentation video require small batch constrain memory consumption paper present group normalization simple alternative divide channel group compute within group mean variance normalization computation independent batch size accuracy stable wide range batch size resnet train imagenet lower error counterpart use batch size use typical batch size comparably good outperform normalization variants moreover naturally transfer pre train fine tune outperform base counterparts object detection segmentation coco video classification kinetics show effectively replace powerful variety task easily implement line code modern libraries', 'deep image translation methods recently show excellent result output high quality image cover multiple modes data distribution also increase interest disentangle internal representations learn deep methods improve performance achieve finer control paper bridge two objectives introduce concept cross domain disentanglement aim separate internal representation three part share part contain information domains exclusive part hand contain factor variation particular domain achieve bidirectional image translation base generative adversarial network cross domain autoencoders novel network component model offer multiple advantage output diverse sample cover multiple modes distributions domains perform domain specific image transfer interpolation cross domain retrieval without need label data pair image compare model state art multi modal image translation achieve better result translation challenge datasets well cross domain retrieval realistic datasets', 'work aim solve large collection task use single reinforcement learn agent single set parameters key challenge handle increase amount data extend train time develop new distribute agent impala importance weight actor learner architecture use resources efficiently single machine train also scale thousands machine without sacrifice data efficiency resource utilisation achieve stable learn high throughput combine decouple act learn novel policy correction method call trace demonstrate effectiveness impala multi task reinforcement learn dmlab set task deepmind lab environment beattie atari available atari game arcade learn environment bellemare result show impala able achieve better performance previous agents less data crucially exhibit positive transfer task result multi task approach', 'configure deep spike neural network snns excite research avenue low power spike event base computation however spike generation function non differentiable therefore directly compatible standard error backpropagation algorithm paper introduce new general backpropagation mechanism learn synaptic weight axonal delay overcome problem non differentiability spike function use temporal credit assignment policy backpropagating error precede layer describe release gpu accelerate software implementation method allow train fully connect convolutional neural network cnn architectures use software compare method exist snn base learn approach standard ann snn conversion techniques show method achieve state art performance snn mnist nmnist dvs gesture tidigits datasets', 'uncertainty sample popular active learn algorithm use reduce amount data require learn classifier observe practice converge different parameters depend initialization sometimes even better parameters standard train data work give theoretical explanation phenomenon show uncertainty sample convex logistic loss interpret perform precondition stochastic gradient step population zero one loss experiment synthetic real datasets support connection', 'adversarial learn base video prediction methods suffer image blur since commonly use adversarial regression loss pair work rather competitive way collaboration yield compromise blur effect meantime often rely single pass architecture predictor inadequate explicitly capture forthcoming uncertainty work involve two key insights video prediction approach stochastic process sample collection proposals conform possible frame distribution follow time stamp one select final prediction couple combine loss function dedicatedly design sub network encourage work collaborative way combine two insights propose two stage network call vpss textbf ideo textbf rediction via textbf elective textbf ampling specifically emph sample module produce collection high quality proposals facilitate multiple choice adversarial learn scheme yield diverse frame proposal set subsequently emph selection module select high possibility candidates proposals combine produce final prediction extensive experiment diverse challenge datasets demonstrate effectiveness propose video prediction approach yield diverse proposals accurate prediction result']\n",
            "\n",
            "Cluster  4\n",
            "['parallel implementations stochastic gradient descent sgd receive significant research attention thank excellent scalability properties algorithm efficiency context train deep neural network fundamental barrier parallelize large scale sgd fact cost communicate gradient update nod large consequently lossy compression heuristics propose nod communicate quantize gradients although effective practice heuristics always provably converge clear whether optimal paper propose quantize sgd qsgd family compression scheme allow compression gradient update node guarantee convergence standard assumptions qsgd allow user trade compression convergence time communicate sublinear number bits per iteration model dimension achieve asymptotically optimal communication cost complement theoretical result empirical data show qsgd significantly reduce communication cost competitive standard uncompress techniques variety real task particular experiment show gradient quantization apply train deep neural network image classification automate speech recognition lead significant reductions communication cost end end train time instance gpus able train resnet network imagenet faster full accuracy note show exist generic parameter settings know network architectures preserve slightly improve full accuracy use quantization', 'spatio temporal action detection videos typically address fully supervise setup manual annotation train videos require every frame since annotation extremely tedious prohibit scalability clear need minimize amount manual supervision work propose unify framework handle combine vary type less demand weak supervision model base discriminative cluster integrate different type supervision constraints optimization investigate applications model train setups alternative supervisory signal range video level class label temporal point sparse action bound box full per frame annotation action bound box experiment challenge ucf daly datasets demonstrate competitive performance method fraction supervision use previous methods flexibility model enable joint learn data different level annotation experimental result demonstrate significant gain add fully supervise examples otherwise weakly label videos', 'give sample probability distribution anomaly detection problem determine give point lie low density region paper concern calibrate anomaly detection practically relevant extension additionally wish produce confidence score point anomalous build classification framework anomaly detection show minimisation suitably modify proper loss produce density estimate anomalous instance show incorporate quantile control relate objective generalise version pinball loss finally show efficiently optimise objective kernelised scorer leverage recent result point process literature result objective capture close relative one class svm special case', 'relational reason central component generally intelligent behavior prove difficult neural network learn paper describe use relation network rns simple plug play module solve problems fundamentally hinge relational reason test augment network three task visual question answer use challenge dataset call clevr achieve state art super human performance text base question answer use babi suite task complex reason dynamic physical systems use curated dataset call sort clevr show powerful convolutional network general capacity solve relational question gain capacity augment rns work show deep learn architecture equip module implicitly discover learn reason entities relations', 'consider problem generate automatic code give sample input output pair train neural network map current state output program next statement neural network optimize multiple task concurrently next operation set high level command operands next statement variables drop memory use method able create program twice long exist state art solutions improve success rate comparable lengths cut run time two order magnitude code include implementation various literature baselines publicly available https github com amitz pccoder', 'introduce temper geodesic markov chain monte carlo mcmc algorithm initialize pose graph optimization problems arise various scenarios sfm structure motion slam simultaneous localization map mcmc first kind unite global non convex optimization spherical manifold quaternions posterior sample order provide reliable initial pose uncertainty estimate informative quality solutions devise theoretical convergence guarantee extensively evaluate method synthetic real benchmarks besides elegance formulation theory show method robust miss data noise estimate uncertainties capture intuitive properties data', 'stochastic gradient method minimize objective function compose large number differentiable function solve stochastic optimization problem moderate accuracy block coordinate descent update bcd method hand handle problems multiple block variables update one time block variables easier update individually together bcd lower per iteration cost paper introduce method combine feature bcd problems many components objective multiple block variables specifically block stochastic gradient bsg method propose solve convex nonconvex program iteration bsg approximate gradient differentiable part objective randomly sample small set data sample function sum term objective use sample update block variables either deterministic randomly shuffle order convergence convex nonconvex case establish different sense convex case propose method order convergence rate method nonconvex case convergence establish term expect violation first order optimality condition propose method numerically test problems include stochastic least square logistic regression convex well low rank tensor recovery bilinear logistic regression nonconvex', 'top visual attention mechanisms use extensively image caption visual question answer vqa enable deeper image understand fine grain analysis even multiple step reason work propose combine bottom top attention mechanism enable attention calculate level object salient image regions natural basis attention consider within approach bottom mechanism base faster cnn propose image regions associate feature vector top mechanism determine feature weight apply approach image caption result mscoco test server establish new state art task achieve cider spice bleu score respectively demonstrate broad applicability method apply approach vqa obtain first place vqa challenge', 'paper address mode collapse generative adversarial network gans view modes geometric structure data distribution metric space geometric lens embed subsamples dataset arbitrary metric space space preserve pairwise distance distribution metric embed determine dimensionality latent space automatically also enable construct mixture gaussians draw latent space random vectors use gaussian mixture model tandem simple augmentation objective function train gans every major step method support theoretical analysis experiment real synthetic data confirm generator able produce sample spread modes avoid unwanted sample outperform several recent gin variants number metrics offer new feature', 'connectionist temporal classification ctc objective function end end sequence learn adopt dynamic program algorithms directly learn map sequence ctc show promise result many sequence learn applications include speech recognition scene text recognition however ctc tend produce highly peaky overconfident distributions symptom overfitting remedy propose regularization method base maximum conditional entropy penalize peaky distributions encourage exploration also introduce entropy base prune method dramatically reduce number ctc feasible paths rule unreasonable alignments experiment scene text recognition show propose methods consistently improve ctc baseline without need adjust train settings code make publicly available https github com liuhu bigeye enctc crnn', 'present unsupervised visual feature learn algorithm drive context base pixel prediction analogy auto encoders propose context encoders convolutional neural network train generate content arbitrary image region condition surround order succeed task context encoders need understand content entire image well produce plausible hypothesis miss part train context encoders experiment standard pixel wise reconstruction loss well reconstruction plus adversarial loss latter produce much sharper result better handle multiple modes output find context encoder learn representation capture appearance also semantics visual structure quantitatively demonstrate effectiveness learn feature cnn pre train classification detection segmentation task furthermore context encoders use semantic inpainting task either stand alone initialization non parametric methods', 'despite efficacy variety computer vision task deep neural network dnns vulnerable adversarial attack limit applications security critical systems recent work show possibility generate imperceptibly perturb image input adversarial examples fool well train dnn classifiers make arbitrary predictions address problem propose train recipe name deep defense core idea integrate adversarial perturbation base regularizer classification objective obtain model learn resist potential attack directly precisely whole optimization problem solve like train recursive network experimental result demonstrate method outperform train adversarial parseval regularizations large margins various datasets include mnist cifar imagenet different dnn architectures code model reproduce result available https github com ziangyan deepdefense pytorch', 'various semantic attribute segmentation mask geometric feature keypoints materials encode per point probe function geometries give collection relate shape consider jointly analyze probe function different shape discover common latent structure use neural network even absence correspondence information network train point cloud representations shape geometry associate semantic function point cloud function express share semantic understand shape coordinate way example segmentation task function indicator function arbitrary set shape part particular combination involve know network network able produce small dictionary basis function shape dictionary whose span include semantic function provide shape even though shape independent discretizations functional correspondences provide network able generate latent base consistent order reflect share semantic structure among shape demonstrate effectiveness technique various segmentation keypoint selection applications', 'deeper neural network difficult train present residual learn framework ease train network substantially deeper use previously explicitly reformulate layer learn residual function reference layer input instead learn unreferenced function provide comprehensive empirical evidence show residual network easier optimize gain accuracy considerably increase depth imagenet dataset evaluate residual net depth layer deeper vgg net still lower complexity ensemble residual net achieve error imagenet test set result place ilsvrc classification task also present analysis cifar layer depth representations central importance many visual recognition task solely due extremely deep representations obtain relative improvement coco object detection dataset deep residual net foundations submissions ilsvrc coco competitions also place task imagenet detection imagenet localization coco detection coco segmentation', 'rapid growth image video data web hash extensively study image video search recent years benefit recent advance deep learn deep hash methods achieve promise result image retrieval however limitations previous deep hash methods semantic information fully exploit paper develop deep supervise discrete hash algorithm base assumption learn binary cod ideal classification pairwise label information classification information use learn hash cod within one stream framework constrain output last layer binary cod directly rarely investigate deep hash algorithm discrete nature hash cod alternate minimization method use optimize objective function experimental result show method outperform current state art methods benchmark datasets', 'recent work show convolutional network substantially deeper accurate efficient train contain shorter connections layer close input close output paper embrace observation introduce dense convolutional network densenet connect layer every layer fee forward fashion whereas traditional convolutional network layer connections one layer subsequent layer network direct connections layer feature map precede layer use input feature map use input subsequent layer densenets several compel advantage alleviate vanish gradient problem strengthen feature propagation encourage feature reuse substantially reduce number parameters evaluate propose architecture four highly competitive object recognition benchmark task cifar cifar svhn imagenet densenets obtain significant improvements state art whilst require less computation achieve high performance code pre train model available https github com liuzhuang densenet', 'paper present keypointnet end end geometric reason framework learn optimal set category specific keypoints along detectors predict keypoints single input image demonstrate framework pose estimation task propose differentiable pose objective seek optimal set keypoints recover relative pose two view object network automatically discover consistent set keypoints across viewpoints single object well across object instance give object class importantly find end end approach use grind truth keypoint annotations outperform fully supervise baseline use neural network architecture pose estimation task discover keypoints across car chair plane categories shapenet visualize https keypoints github', 'propose novel wasserstein method distillation mechanism yield joint learn word embeddings topics propose method base fact euclidean distance word embeddings may employ underlie distance wasserstein topic model word distributions topics optimal transport word distributions document embeddings word learn unify framework learn topic model leverage distil grind distance matrix update topic distributions smoothly calculate correspond optimal transport strategy provide update word embeddings robust guidance improve algorithm convergence application focus patient admission record propose method embed cod diseases procedures learn topics admissions obtain superior performance clinically meaningful disease network construction mortality prediction function admission cod procedure recommendation', 'propose dropmax stochastic version softmax classifier iteration drop non target class accord dropout probabilities adaptively decide instance specifically overlay binary mask variables class output probabilities input adaptively learn via variational inference stochastic regularization effect build ensemble classifier exponentially many classifiers different decision boundaries moreover learn dropout rat non target class instance allow classifier focus classification confuse class validate model multiple public datasets classification obtain significantly improve accuracy regular softmax classifier baselines analysis learn dropout probabilities show model indeed select confuse class often perform classification', 'consider minimization submodular function subject order constraints show potentially non convex optimization problem cast convex optimization problem space uni dimensional measure order constraints correspond first order stochastic dominance propose new discretization scheme lead simple efficient algorithms base zero first higher order oracles algorithms also lead improvements without isotonic constraints finally experiment show non convex loss function much robust outliers isotonic regression still solvable polynomial time', 'stochastic gradient hard thresholding methods recently show work favorably solve large scale empirical risk minimization problems sparsity rank constraint despite improve iteration complexity full gradient methods gradient evaluation hard thresholding complexity exist stochastic algorithms usually scale linearly data size could still expensive data huge hard thresholding step could expensive singular value decomposition rank constrain problems address deficiencies propose efficient hybrid stochastic gradient hard thresholding hsg method provably show sample size independent gradient evaluation hard thresholding complexity bound specifically prove stochastic gradient evaluation complexity hsg scale linearly inverse sub optimality hard thresholding complexity scale logarithmically apply heavy ball acceleration technique propose accelerate variant hsg show improve factor dependence restrict condition number numerical result confirm theoretical affirmation demonstrate computational efficiency propose methods', 'learn low dimensional embeddings knowledge graph powerful approach use predict unobserved miss edge entities however open challenge area develop techniques beyond simple edge prediction handle complex logical query might involve multiple unobserved edge entities variables instance give incomplete biological knowledge graph might want predict drug likely target proteins involve diseases query require reason possible proteins might interact diseases introduce framework efficiently make predictions conjunctive logical query flexible tractable subset first order logic incomplete knowledge graph approach embed graph nod low dimensional space represent logical operators learn geometric operations translation rotation embed space perform logical operations within low dimensional embed space approach achieve time complexity linear number query variables compare exponential complexity require naive enumeration base approach demonstrate utility framework two application study real world datasets millions relations predict logical relationships network drug gene disease interactions graph base representation social interactions derive popular web forum', 'deep reinforcement learn drl algorithms successfully apply range challenge control task however methods typically suffer three core difficulties temporal credit assignment sparse reward lack effective exploration brittle convergence properties extremely sensitive hyperparameters collectively challenge severely limit applicability approach real world problems evolutionary algorithms eas class black box optimization techniques inspire natural evolution well suit address three challenge however eas typically suffer high sample complexity struggle solve problems require optimization large number parameters paper introduce evolutionary reinforcement learn erl hybrid algorithm leverage population provide diversify data train agent reinserts agent population periodically inject gradient information erl inherit ability temporal credit assignment fitness metric effective exploration diverse set policies stability population base approach complement policy drl ability leverage gradients higher sample efficiency faster learn experiment range challenge continuous control benchmarks demonstrate erl significantly outperform prior drl methods', 'paper propose novel method provide contrastive explanations justify classification input black box classifier deep neural network give input find minimally sufficiently present viz important object pixels image justify classification analogously minimally necessarily emph absent viz certain background pixels argue explanations natural humans use commonly domains health care criminology minimally critically emph absent important part explanation best knowledge explicitly identify current explanation methods explain predictions neural network validate approach three real datasets obtain diverse domains namely handwritten digits dataset mnist large procurement fraud dataset brain activity strength dataset three case witness power approach generate precise explanations also easy human experts understand evaluate', 'basic principles design convolutional neural network cnn structure predict object different level image level region level pixel level diverge generally network structure design specifically image classification directly use default backbone structure task include detection segmentation seldom backbone structure design consideration unify advantage network design pixel level region level predict task may require deep feature high resolution towards goal design fish like network call fishnet fishnet information resolutions preserve refine final task besides observe exist work still emph directly propagate gradient information deep layer shallow layer design better handle problem extensive experiment conduct demonstrate remarkable performance fishnet particular imagenet accuracy fishnet able surpass performance densenet resnet fewer parameters fishnet apply one modules win entry coco detection challenge code available https github com kevin ssy fishnet', 'paper present novel framework video image segmentation localization cast single optimization problem integrate information low level appearance cue high level localization cue weakly supervise manner propose framework leverage two representations different level exploit spatial relationship bound box superpixels linear constraints simultaneously discriminate foreground background bound box superpixel level different previous approach mainly rely discriminative cluster incorporate foreground model minimize histogram difference object across image frame exploit geometric relation superpixels bound box enable transfer segmentation cue improve localization output vice versa inclusion foreground model generalize discriminative framework video data background tend similar thus discriminative demonstrate effectiveness unify framework youtube object video dataset internet object discovery dataset pascal voc', 'continuous word representation aka word embed basic build block many neural network base model use natural language process task although widely accept word similar semantics close embed space find word embeddings learn several task bias towards word frequency embeddings high frequency low frequency word lie different subregions embed space embed rare word popular word far even semantically similar make learn word embeddings ineffective especially rare word consequently limit performance neural network model order mitigate issue paper propose neat simple yet effective adversarial train method blur boundary embeddings high frequency word low frequency word conduct comprehensive study ten datasets across four natural language process task include word similarity language model machine translation text classification result show achieve higher performance baselines task', 'lot learn task require deal graph data contain rich relation information among elements model physics systems learn molecular fingerprint predict protein interface classify diseases demand model learn graph input domains learn non structural data like texts image reason extract structure like dependency tree sentence scene graph image important research topic also need graph reason model graph neural network gnns neural model capture dependence graph via message pass nod graph recent years variants gnns graph convolutional network gcn graph attention network gat graph recurrent network grn demonstrate grind break performances many deep learn task survey propose general design pipeline gnn model discuss variants component systematically categorize applications propose four open problems future research', 'paper propose generative multi column network image inpainting network synthesize different image components parallel manner within one stage better characterize global structure design confidence drive reconstruction loss implicit diversify mrf regularization adopt enhance local detail multi column network combine reconstruction mrf loss propagate local global information derive context target inpainting regions extensive experiment challenge street view face natural object scenes manifest method produce visual compel result even without previously common post process', 'paper describe infogan information theoretic extension generative adversarial network able learn disentangle representations completely unsupervised manner infogan generative adversarial network also maximize mutual information small subset latent variables observation derive lower bind mutual information objective optimize efficiently show train procedure interpret variation wake sleep algorithm specifically infogan successfully disentangle write style digit shape mnist dataset pose light render image background digits central digit svhn dataset also discover visual concepts include hair style presence absence eyeglasses emotions celeba face dataset experiment show infogan learn interpretable representations competitive representations learn exist fully supervise methods', 'two semimetrics probability distributions propose give sum differences expectations analytic function evaluate spatial frequency locations feature feature choose maximize distinguishability distributions optimize lower bind test power statistical test use feature result parsimonious interpretable indication two distributions differ locally empirical estimate test power criterion converge increase sample size ensure quality return feature real world benchmarks high dimensional text image data linear time test use propose semimetrics achieve comparable performance state art quadratic time maximum mean discrepancy test return human interpretable feature explain test result', 'decompose evidence lower bind show existence term measure total correlation latent variables use motivate beta tcvae total correlation variational autoencoder algorithm refinement plug replacement beta vae learn disentangle representations require additional hyperparameters train propose principled classifier free measure disentanglement call mutual information gap mig perform extensive quantitative qualitative experiment restrict non restrict settings show strong relation total correlation disentanglement model train use framework', 'suggest new loss learn deep embeddings key characteristics new loss absence tunable parameters good result obtain across range datasets problems loss compute estimate two distribution similarities positive match negative non match point pair compute probability positive pair lower similarity score negative pair base probability estimate show operations perform simple piecewise differentiable manner use histograms soft assignment operations make propose loss suitable learn deep embeddings use stochastic optimization experiment reveal favourable result compare recently propose loss function', 'progress deep learn spawn great successes many engineer applications prime example convolutional neural network type feedforward neural network approach sometimes even surpass human accuracy variety visual recognition task however show neural network recent extensions struggle recognition task dependent visual feature must detect long spatial range introduce visual challenge pathfinder describe novel recurrent neural network architecture call horizontal gate recurrent unit hgru learn intrinsic horizontal connections within across feature columns demonstrate single hgru layer match outperform test feedforward hierarchical baselines include state art architectures order magnitude parameters', 'machine learn become widely use practice need new methods build complex intelligent systems integrate learn exist software domain knowledge encode rule case study present system learn parse newtonian physics problems textbooks system nut bolt learn pipeline process incorporate exist code pre learn machine learn model human engineer rule jointly train entire pipeline prevent propagation errors use combination label unlabelled data approach achieve good performance parse task outperform simple pipeline variants finally also show nut bolt use achieve improvements relation extraction task end task answer newtonian physics problems', 'navigate unstructured environments basic capability intelligent creatures thus fundamental interest study development artificial intelligence long range navigation complex cognitive task rely develop internal representation space ground recognisable landmarks robust visual process simultaneously support continuous self localisation representation goal go build upon recent research apply deep reinforcement learn maze navigation problems present end end deep reinforcement learn approach apply city scale recognise successful navigation rely integration general policies locale specific knowledge propose dual pathway architecture allow locale specific feature encapsulate still enable transfer multiple cities key contribution paper interactive navigation environment use google street view photographic content worldwide coverage baselines demonstrate deep reinforcement learn agents learn navigate multiple cities traverse target destinations may kilometres away video summarize research show train agent diverse city environments well transfer task available https sit google com view learn navigate cities nip', 'fine grain visual classification fgvc important computer vision problem involve small diversity within different class often require expert annotators collect data utilize notion small visual diversity revisit maximum entropy learn context fine grain classification provide train routine maximize entropy output probability distribution train convolutional neural network fgvc task provide theoretical well empirical justification approach achieve state art performance across variety classification task fgvc potentially extend fine tune task method robust different hyperparameter value amount train data amount train label noise hence valuable tool many similar problems', 'paper address general problem blind echo retrieval give sensors measure discrete time domain mixtures delay attenuate copy unknown source signal echo location weight recover problem broad applications field sonars seismology ultrasounds room acoustics belong broader class blind channel identification problems intensively study signal process exist methods proceed two step blind estimation sparse discrete time filter echo information retrieval peak pick precision methods fundamentally limit rate signal sample estimate echo locations necessary grid since true locations never match sample grid weight estimation precision also strongly limit call basis mismatch problem compress sense propose radically different approach problem build top framework finite rate innovation sample approach operate directly parameter space echo locations weight enable near exact blind grid echo retrieval discrete time measurements show outperform conventional methods several order magnitudes precision', 'paper study generalization performance multi class classification obtain shaper data dependent generalization error bind fast convergence rate substantially improve state art bound exist data dependent generalization analysis theoretical analysis motivate devise two effective multi class kernel learn algorithms statistical guarantee experimental result show propose methods significantly outperform exist multi class classification methods', 'multi task learn multiple task solve jointly share inductive bias multi task learn inherently multi objective problem different task may conflict necessitate trade common compromise optimize proxy objective minimize weight linear combination per task losses however workaround valid task compete rarely case paper explicitly cast multi task learn multi objective optimization overall objective find pareto optimal solution end use algorithms develop gradient base multi objective optimization literature algorithms directly applicable large scale learn problems since scale poorly dimensionality gradients number task therefore propose upper bind multi objective loss show optimize efficiently prove optimize upper bind yield pareto optimal solution realistic assumptions apply method variety multi task deep learn problems include digit classification scene understand joint semantic segmentation instance segmentation depth estimation multi label classification method produce higher perform model recent multi task learn formulations per task train', 'numerous deep learn applications benefit multi task learn multiple regression classification objectives paper make observation performance systems strongly dependent relative weight task loss tune weight hand difficult expensive process make multi task learn prohibitive practice propose principled approach multi task deep learn weigh multiple loss function consider homoscedastic uncertainty task allow simultaneously learn various quantities different units scale classification regression settings demonstrate model learn per pixel depth regression semantic instance segmentation monocular input image perhaps surprisingly show model learn multi task weight outperform separate model train individually task', 'extend capabilities neural network couple external memory resources interact attentional process combine system analogous turing machine von neumann architecture differentiable end end allow efficiently train gradient descent preliminary result demonstrate neural turing machine infer simple algorithms copy sort associative recall input output examples', 'marry two powerful ideas deep representation learn visual recognition language understand symbolic program execution reason neural symbolic visual question answer vqa system first recover structural scene representation image program trace question execute program scene representation obtain answer incorporate symbolic structure prior knowledge offer three unique advantage first execute program symbolic space robust long program trace model solve complex reason task better achieve accuracy clevr dataset second model data memory efficient perform well learn small number train data also encode image compact representation require less storage exist methods offline question answer third symbolic program execution offer full transparency reason process thus able interpret diagnose execution step', 'study computational tractability pac reinforcement learn rich observations present new provably sample efficient algorithms environments deterministic hide state dynamics stochastic rich observations methods operate oracle model computation access policy value function class exclusively standard optimization primitives therefore represent computationally efficient alternatives prior algorithms require enumeration stochastic hide state dynamics prove know sample efficient algorithm olive implement oracle model also present several examples illustrate fundamental challenge tractable pac reinforcement learn general settings', 'propose study new model reinforcement learn rich observations generalize contextual bandits sequential decision make model require agent take action base observations feature goal achieve long term performance competitive large set policies avoid barriers sample efficient learn associate large observation space general pomdps focus problems summarize small number hide state long term reward predictable reactive function class set design analyze new reinforcement learn algorithm least square value elimination exploration prove algorithm learn near optimal behavior number episodes polynomial relevant parameters logarithmic number policies independent size observation space result provide theoretical justification reinforcement learn function approximation', 'existence evasion attack test phase machine learn algorithms represent significant challenge deployment understand attack carry add imperceptible perturbations input generate adversarial examples find effective defenses detectors prove difficult paper step away attack defense arm race seek understand limit learn presence evasion adversary particular extend probably approximately correct pac learn framework account presence adversary first define corrupt hypothesis class arise standard binary hypothesis class presence evasion adversary derive vapnik chervonenkis dimension denote adversarial dimension show sample complexity upper bound fundamental theorem statistical learn extend case evasion adversaries sample complexity control adversarial dimension explicitly derive adversarial dimension halfspace classifiers presence sample wise norm constrain adversary type commonly study evasion attack show standard dimension close open question finally prove adversarial dimension either larger smaller standard dimension depend hypothesis class adversary make interest object study right', 'propose parsimonious quantile regression framework learn dynamic tail behaviors financial asset return model capture well time vary characteristic asymmetrical heavy tail property financial time series combine merit popular sequential neural network model lstm novel parametric quantile function construct represent conditional distribution asset return model also capture individually serial dependences higher moments rather volatility across wide range asset class sample forecast conditional quantiles var model outperform garch family propose approach suffer issue quantile cross expose ill posedness compare parametric probability density function approach', 'introduce spike slab deep learn fully bayesian alternative dropout improve generalizability deep relu network new type regularization enable provable recovery smooth input output map unknown level smoothness indeed show posterior distribution concentrate near minimax rate alpha holder smooth map perform well know smoothness level alpha ahead time result shed light architecture design deep neural network namely choice depth width sparsity level network attribute typically depend unknown smoothness order optimal obviate constraint fully bay construction aside show overfit sense posterior concentrate smaller network fewer optimal number nod link result provide new theoretical justifications deep relu network bayesian point view', 'propose prototypical network problem shoot classification classifier must generalize new class see train set give small number examples new class prototypical network learn metric space classification perform compute distance prototype representations class compare recent approach shoot learn reflect simpler inductive bias beneficial limit data regime achieve excellent result provide analysis show simple design decisions yield substantial improvements recent approach involve complicate architectural choices meta learn extend prototypical network zero shoot learn achieve state art result bird dataset', 'study consistency properties machine learn methods base minimize convex surrogates extend recent framework osokin quantitative analysis consistency properties case inconsistent surrogates key technical contribution consist new lower bind calibration function quadratic surrogate non trivial always zero inconsistent case new bind allow quantify level inconsistency set show learn inconsistent surrogates guarantee sample complexity optimization difficulty apply theory two concrete case multi class classification tree structure loss rank mean average precision loss result show approximation computation trade off cause inconsistent surrogates potential benefit', 'despite impressive performance deep neural network dnns typically underperform gradient boost tree gbts many tabular dataset learn task propose apply different regularization coefficient weight might boost performance dnns allow make use relevant input however lead intractable number hyperparameters introduce regularization learn network rlns overcome challenge introduce efficient hyperparameter tune scheme minimize new counterfactual loss result show rlns significantly improve dnns tabular datasets achieve comparable result gbts best performance achieve ensemble combine gbts rlns rlns produce extremely sparse network eliminate network edge input feature thus provide interpretable model reveal importance network assign different input rlns could efficiently learn single network datasets comprise tabular unstructured data set medical image accompany electronic health record open source implementation rln find https github com irashavitt regularizationnetworks', 'convolutional network core state art computer vision solutions wide variety task since deep convolutional network start become mainstream yield substantial gain various benchmarks although increase model size computational cost tend translate immediate quality gain task long enough label data provide train computational efficiency low parameter count still enable factor various use case mobile vision big data scenarios explore ways scale network ways aim utilize add computation efficiently possible suitably factorize convolutions aggressive regularization benchmark methods ilsvrc classification challenge validation set demonstrate substantial gain state art top top error single frame evaluation use network computational cost billion multiply add per inference use less million parameters ensemble model multi crop evaluation report top error validation set error test set top error validation set', 'multi task learn mtl appeal deep learn regularization paper tackle specific mtl context denote primary mtl ultimate goal improve performance give primary task leverage several auxiliary task main methodological contribution introduce rock new generic multi modal fusion block deep learn tailor primary mtl context rock architecture base residual connection make forward prediction explicitly impact intermediate auxiliary representations auxiliary predictor architecture also specifically design primary mtl context incorporate intensive pool operators maximize complementarity intermediate representations extensive experiment nyuv dataset object detection scene classification depth prediction surface normal estimation auxiliary task validate relevance approach superiority flat mtl approach method outperform state art object detection model nyuv dataset large margin also able handle large scale heterogeneous input real synthetic image miss annotation modalities', 'model free reinforcement learn aim offer shelf solutions control dynamical systems without require model system dynamics introduce model free random search algorithm train static linear policies continuous control problems common evaluation methodology show method match state art sample efficiency benchmark mujoco locomotion task nonetheless rigorous evaluation reveal assessment performance benchmarks optimistic evaluate performance method hundreds random seed many different hyperparameter configurations benchmark task extensive evaluation possible small computational footprint method simulations highlight high variability performance benchmark task indicate commonly use estimations sample efficiency adequately evaluate performance algorithms result stress need new baselines benchmarks evaluation methodology algorithms', 'describe new software framework fast train generalize linear model framework name snap machine learn snap combine recent advance machine learn systems algorithms nest manner reflect hierarchical architecture modern compute systems prove theoretically hierarchical system accelerate train distribute environments intra node communication cheaper inter node communication additionally provide review implementation snap term gpu acceleration pipelining communication pattern software architecture highlight aspects critical achieve high performance evaluate performance snap single node multi node environments quantify benefit hierarchical scheme data stream functionality compare widely use machine learn software frameworks finally present logistic regression benchmark criteo terabyte click log dataset show snap achieve test loss order magnitude faster previously report result include obtain use tensorflow scikit learn', 'present splinenets practical novel approach use condition convolutional neural network cnns splinenets continuous generalizations neural decision graph dramatically reduce runtime complexity computation cost cnns maintain even increase accuracy function splinenets dynamic condition input hierarchical condition computational path splinenets employ unify loss function desire level smoothness network decision parameters allow sparse activation subset nod individual sample particular embed infinitely many function weight filter smooth low dimensional manifold parameterized compact splines index position parameter instead sample categorical distribution pick branch sample choose continuous position pick function weight show maximize mutual information spline position class label network optimally utilize specialize classification task experiment show approach significantly increase accuracy resnets negligible cost speed match precision level resnet level splinenet', 'study finite sum nonconvex optimization problems objective function average nonconvex function propose new stochastic gradient descent algorithm base nest variance reduction compare conventional stochastic variance reduce gradient svrg algorithm use two reference point construct semi stochastic gradient diminish variance iteration algorithm use nest reference point build semi stochastic gradient reduce variance iteration smooth nonconvex function propose algorithm converge epsilon approximate first order stationary point nabla mathbf leq epsilon within tilde land epsilon epsilon land epsilon number stochastic gradient evaluations improve best know gradient complexity svrg epsilon scsg land epsilon epsilon land epsilon gradient dominate function algorithm also achieve better gradient complexity state art algorithms thorough experimental result different nonconvex optimization problems back theory', 'study problem identify best action sequential decision make set reward distributions arm exhibit non trivial dependence structure govern underlie causal model domain agent deploy set play arm correspond intervene set variables set specific value paper show whenever underlie causal model take account decision make process standard strategies simultaneously intervene variables subsets variables may general lead suboptimal policies regardless number interventions perform agent environment formally acknowledge phenomenon investigate structural properties imply underlie causal model lead complete characterization relationships arm distributions leverage characterization build new algorithm take input causal structure find minimal sound complete set qualify arm agent play maximize expect reward empirically demonstrate new strategy learn optimal policy lead order magnitude faster convergence rat compare causal insensitive counterparts', 'several applications reinforcement learn suffer instability due high variance especially prevalent high dimensional domains regularization commonly use technique machine learn reduce variance cost introduce bias exist regularization techniques focus spatial perceptual regularization yet reinforcement learn due nature bellman equation opportunity also exploit temporal regularization base smoothness value estimate trajectories paper explore class methods temporal regularization formally characterize bias induce technique use markov chain concepts illustrate various characteristics temporal regularization via sequence simple discrete continuous mdps show technique provide improvement even high dimensional atari game', 'paper address problem manipulate image use natural language description task aim semantically modify visual attribute object image accord text describe new visual appearance although exist methods synthesize image new attribute fully preserve text irrelevant content original image paper propose text adaptive generative adversarial network tagan generate semantically manipulate image preserve text irrelevant content key method text adaptive discriminator create word level local discriminators accord input text classify fine grain attribute independently discriminator generator learn generate image regions correspond give text modify experimental result show method outperform exist methods cub oxford datasets result mostly prefer user study extensive analysis show method able effectively disentangle visual attribute produce please output', 'consider classification problem label unlabeled data available show linear classifiers define convex margin base surrogate losses decrease impossible construct emph semi supervise approach able guarantee improvement supervise classifier measure surrogate loss label unlabeled data convex margin base loss function also increase demonstrate safe improvements emph possible', 'long short term memory lstm network type recurrent neural network complex computational unit successfully apply variety sequence model task paper develop tree long short term memory treelstm neural network model base lstm design predict tree rather linear sequence treelstm define probability sentence estimate generation probability dependency tree time step node generate base representation generate sub tree enhance model power treelstm explicitly represent correlations leave right dependents application model msr sentence completion challenge achieve result beyond current state art also report result dependency parse reranking achieve competitive performance', 'model neural machine translation often discriminative family encoderdecoders learn conditional distribution target sentence give source sentence paper propose variational model learn conditional distribution neural machine translation variational encoderdecoder model train end end different vanilla encoder decoder model generate target translations hide representations source sentence alone variational model introduce continuous latent variable explicitly model underlie semantics source sentence guide generation target translations order perform efficient posterior inference large scale train build neural posterior approximator condition source target side equip reparameterization technique estimate variational lower bind experiment chinese english english german translation task show propose variational neural machine translation achieve significant improvements vanilla neural machine translation baselines', 'study problem video video synthesis whose goal learn map function input source video sequence semantic segmentation mask output photorealistic video precisely depict content source video image counterpart image image translation problem popular topic video video synthesis problem less explore literature without model temporal dynamics directly apply exist image synthesis approach input video often result temporally incoherent videos low visual quality paper propose video video synthesis approach generative adversarial learn framework carefully design generators discriminators couple spatio temporal adversarial objective achieve high resolution photorealistic temporally coherent video result diverse set input format include segmentation mask sketch pose experiment multiple benchmarks show advantage method compare strong baselines particular model capable synthesize resolution videos street scenes second long significantly advance state art video synthesis finally apply method future video prediction outperform several compete systems code model result available website https github com nvidia vid vid please use adobe reader see embed videos paper', 'recently learn discriminative feature improve recognition performances gradually become primary goal deep learn numerous remarkable work emerge paper propose novel yet extremely simple method virtual softmax enhance discriminative property learn feature inject dynamic virtual negative class original softmax inject virtual class aim enlarge inter class margin compress intra class distribution strengthen decision boundary constraint although seem weird optimize additional virtual class show method derive intuitive clear motivation indeed encourage feature compact separable paper empirically experimentally demonstrate superiority virtual softmax improve performances variety object classification face verification task', 'recent progress deep generative model lead tremendous breakthroughs image generation able synthesize photorealistic image exist model lack understand underlie world different previous work build datasets model present new generative model visual object network vons synthesize natural image object disentangle representation inspire classic graphics render pipelines unravel image formation process three conditionally independent factor shape viewpoint texture present end end adversarial learn framework jointly model shape texture model first learn synthesize shape indistinguishable real shape render object sketch silhouette depth map shape sample viewpoint finally learn add realistic textures sketch generate realistic image von generate image realistic state art image synthesis methods also enable many operations change viewpoint generate image shape texture edit linear interpolation texture shape space transfer appearance across different object viewpoints', 'propose wasserstein auto encoder wae new algorithm build generative model data distribution wae minimize penalize form wasserstein distance model distribution target distribution lead different regularizer one use variational auto encoder vae regularizer encourage encode train distribution match prior compare algorithm several techniques show generalization adversarial auto encoders aae experiment show wae share many properties vaes stable train encoder decoder architecture nice latent manifold structure generate sample better quality measure fid score', 'introduce new efficient principled backpropagation compatible algorithm learn probability distribution weight neural network call bay backprop regularise weight minimise compression cost know variational free energy expect lower bind marginal likelihood show principled kind regularisation yield comparable performance dropout mnist classification demonstrate learn uncertainty weight use improve generalisation non linear regression problems weight uncertainty use drive exploration exploitation trade reinforcement learn', 'object orient representations reinforcement learn show promise transfer learn previous research introduce propositional object orient framework provably efficient learn bound respect sample complexity however framework limitations term class task efficiently learn paper introduce novel deictic object orient framework provably efficient learn bound solve broader range task additionally show framework capable zero shoot transfer transition dynamics across task demonstrate empirically taxi sokoban domains']\n",
            "\n",
            "Cluster  5\n",
            "['introduce new convex optimization problem term quadratic decomposable submodular function minimization problem closely relate decomposable submodular function minimization arise many learn graph hypergraphs settings graph base semi supervise learn pagerank approach problem via new dual strategy describe objective may optimize via random coordinate descent rcd methods projections onto con also establish linear convergence rate rcd algorithm develop efficient projection algorithms provable performance guarantee numerical experiment semi supervise learn hypergraphs confirm efficiency propose algorithm demonstrate significant improvements prediction accuracy respect state art methods']\n",
            "\n",
            "Cluster  6\n",
            "['paper consider parallelization applications whose objective express maximize non monotone submodular function cardinality constraint main result algorithm whose approximation arbitrarily close log adaptive round size grind set exponential speedup parallel run time previously study algorithm constrain non monotone submodular maximization beyond provable guarantee algorithm perform well practice specifically experiment traffic monitor personalize data summarization applications show algorithm find solutions whose value competitive state art algorithms run exponentially fewer parallel iterations', 'study classic mean median cluster fundamental problems unsupervised learn set data partition across multiple sit allow discard small portion data label outliers propose simple approach base construct small summary original dataset propose method time communication efficient good approximation guarantee identify global outliers effectively best knowledge first practical algorithm theoretical guarantee distribute cluster outliers experiment real synthetic data demonstrate clear superiority algorithm baseline algorithms almost metrics', 'active search learn paradigm actively identify many members give class possible critical target scenario high throughput screen scientific discovery drug materials discovery settings specialize instrument often evaluate emph multiple point simultaneously however exist work active search focus sequential acquisition bridge gap address batch active search theoretical practical perspective first derive bayesian optimal policy problem prove lower bind performance gap sequential batch optimal policies cost parallelization also propose novel efficient batch policies inspire state art sequential policies develop aggressive prune technique dramatically speed computation conduct thorough experiment data three application domains citation network material science drug discovery test propose policies total wide range batch size result demonstrate empirical performance gap match theoretical bind nonmyopic policies usually significantly outperform myopic alternatives diversity important consideration batch policy design', 'similarity search fundamental problem compute science various applications attract significant research attention especially large scale search high dimension motivate evidence biological science work develop novel approach similarity search fundamentally different exist methods typically reduce dimension data lessen computational complexity speed search approach project data even higher dimensional space ensure sparsity data output space objective improve precision speed specifically approach two key step firstly compute optimal sparse lift give input sample increase dimension data approximately preserve pairwise similarity secondly seek optimal lift operator best map input sample optimal sparse lift computationally step model optimization problems efficiently effectively solve frank wolfe algorithm simple approach report significantly improve result empirical evaluations exhibit high potentials solve practical problems', 'distribute compute environment consider empirical risk minimization problem propose distribute communication efficient newton type optimization method every iteration worker locally find approximate newton ant direction send main driver main driver average ant directions receive workers form globally improve ant giant direction giant highly communication efficient naturally exploit trade off local computations global communications local computations result fewer overall round communications theoretically show giant enjoy improve convergence rate compare first order methods exist distribute newton type methods sharp contrast many exist distribute newton type methods well popular first order methods highly advantageous practical feature giant involve one tune parameter conduct large scale experiment computer cluster empirically demonstrate superior performance giant', 'exist deep convolutional neural network cnns classification global average first order pool gap become standard module summarize activations last convolution layer final representation prediction recent research show integration higher order pool hop methods clearly improve performance deep cnns however gap exist hop methods assume unimodal distributions fully capture statistics convolutional activations limit representation ability deep cnns especially sample complex content overcome limitation paper propose global gate mixture second order pool sop method improve representation ability deep cnns end introduce sparsity constrain gate mechanism propose novel parametric sop component mixture model give bank sop candidates method adaptively choose top candidates input sample sparsity constrain gate module perform weight sum output select candidates representation sample propose sop flexibly accommodate large number personalize sop candidates efficient way lead richer representations deep network sop end end train potential characterize complex multi modal distributions propose method evaluate two large scale image benchmarks downsampled imagenet place experimental result show sop superior counterparts achieve competitive performance source code available http www peihuali org sop', 'many structure data fit applications require solution optimization problem involve sum potentially large number measurements incremental gradient algorithms offer inexpensive iterations sample subset term sum methods make great progress initially often slow approach solution contrast full gradient methods achieve steady convergence expense evaluate full objective gradient iteration explore hybrid methods exhibit benefit approach rate convergence analysis show control sample size incremental gradient algorithm possible maintain steady convergence rat full gradient methods detail practical quasi newton implementation base approach numerical experiment illustrate potential benefit', 'goal reinforcement learn algorithms estimate optimise value function however unlike supervise learn teacher oracle available provide true value function instead majority reinforcement learn algorithms estimate optimise proxy value function proxy typically base sample bootstrapped approximation true value function know return particular choice return one chief components determine nature algorithm rate future reward discount value bootstrapped even nature reward well know decisions crucial overall success algorithms discuss gradient base meta learn algorithm able adapt nature return online whilst interact learn environment apply game atari environment million frame algorithm achieve new state art performance', 'occurrence multiple diseases among general population important problem patients risk complications represent large share health care expenditure learn predict time event probabilities patients challenge problem risk events correlate compete risk often patients experience individual events interest fraction actually observe data introduce paper survival model flexibility leverage common representation relate events design correct strong imbalance observe outcomes procedure sequential outcome specific survival distributions form components nonparametric multivariate estimators combine ensemble way ensure accurate predictions outcome type simultaneously algorithm general represent first boost like method time event data multiple outcomes demonstrate performance algorithm synthetic real data', 'identify fundamental source error learn form dynamic program function approximation delusional bias arise approximation architecture limit class expressible greedy policies since standard update make globally uncoordinated action choices respect expressible policy class inconsistent even conflict value estimate result lead pathological behaviour estimation instability even divergence solve problem introduce new notion policy consistency define local backup process ensure global consistency use information set set record constraints policies consistent back value prove model base model free algorithms use backup remove delusional bias yield first know algorithms guarantee optimal result general condition algorithms furthermore require polynomially many information set potentially exponential support finally suggest practical heuristics value iteration learn attempt reduce delusional bias', 'bayesian learn build assumption model space contain true reflection data generate mechanism assumption problematic particularly complex data environments present bayesian nonparametric approach learn make use statistical model assume model true approach provably better properties use parametric model admit monte carlo sample scheme afford massive scalability modern computer architectures model base aspect learn particularly attractive regularize nonparametric inference sample size small also correct approximate approach variational bay demonstrate approach number examples include classifiers bayesian random forest', 'capacity neural network absorb information limit number parameters conditional computation part network active per example basis propose theory way dramatically increase model capacity without proportional increase computation practice however significant algorithmic performance challenge work address challenge finally realize promise conditional computation achieve greater improvements model capacity minor losses computational efficiency modern gpu cluster introduce sparsely gate mixture experts layer moe consist thousands fee forward sub network trainable gate network determine sparse combination experts use example apply moe task language model machine translation model capacity critical absorb vast quantities knowledge available train corpora present model architectures moe billion parameters apply convolutionally stack lstm layer large language model machine translation benchmarks model achieve significantly better result state art lower computational cost', 'classical anomaly detection principally concern point base anomalies anomalies occur single point time yet many real world anomalies range base mean occur period time motivate observation present new mathematical model evaluate accuracy time series classification algorithms model expand well know precision recall metrics measure range simultaneously enable customization support domain specific preferences', 'paucity videos current action classification datasets ucf hmdb make difficult identify good video architectures methods obtain similar performance exist small scale benchmarks paper evaluate state art architectures light new kinetics human action video dataset kinetics two order magnitude data human action class clip per class collect realistic challenge youtube videos provide analysis current architectures fare task action classification dataset much performance improve smaller benchmark datasets pre train kinetics also introduce new two stream inflate convnet base convnet inflation filter pool kernels deep image classification convnets expand make possible learn seamless spatio temporal feature extractors video leverage successful imagenet architecture design even parameters show pre train kinetics model considerably improve upon state art action classification reach hmdb ucf', 'statistical leverage score emerge fundamental tool matrix sketch column sample applications low rank approximation regression random feature learn quadrature yet nature quantity barely understand borrow ideas orthogonal polynomial literature introduce regularize christoffel function associate positive definite kernel uncover variational formulation leverage score kernel methods allow elucidate relationships choose kernel well population density main result quantitatively describe decrease relation leverage score population density broad class kernels euclidean space numerical simulations support find', 'recently adversarial erase weakly supervise object attention deeply study due capability localize integral object regions however strategy raise one key problem attention regions gradually expand non object regions train iterations continue significantly decrease quality produce attention map tackle issue well promote quality object attention introduce simple yet effective self erase network seenet prohibit attentions spread unexpected background regions particular seenet leverage two self erase strategies encourage network use reliable object background cue learn attention way integral object regions effectively highlight without include much background regions test quality generate attention map employ mine object regions heuristic cue learn semantic segmentation model experiment pascal voc well demonstrate superiority seenet state art methods', 'role semantics zero shoot learn consider effectiveness previous approach analyze accord form supervision provide learn semantics independently others supervise semantic subspace explain train class thus former able constrain whole space lack ability model semantic correlations latter address issue leave part semantic space unsupervised complementarity exploit new convolutional neural network cnn framework propose use semantics constraints recognition although cnn train classification transfer ability encourage learn hide semantic layer together semantic code classification two form semantic constraints introduce first loss base regularizer introduce generalization constraint semantic predictor second codeword regularizer favor semantic class mappings consistent prior semantic knowledge allow learn data significant improvements state art achieve several datasets', 'study stochastic composite mirror descent class scalable algorithms able exploit geometry composite structure problem consider convex strongly convex objectives non smooth loss function establish high probability convergence rat optimal logarithmic factor apply derive computational error bound study generalization performance multi pass stochastic gradient descent sgd non parametric set high probability generalization bound enjoy logarithmical dependency number pass provide step size sequence square summable improve exist bound expectation polynomial dependency therefore give strong justification ability multi pass sgd overcome overfitting analysis remove boundedness assumptions subgradients often impose literature numerical result report support theoretical find', 'attention mechanism effective focus deep learn model relevant feature interpret however attentions may unreliable since network generate often train weakly supervise manner overcome limitation introduce notion input dependent uncertainty attention mechanism generate attention feature vary degrees noise base give input learn larger variance instance uncertain learn uncertainty aware attention mechanism use variational inference validate various risk prediction task electronic health record model significantly outperform exist attention model analysis learn attentions show model generate attentions comply clinicians interpretation provide richer interpretation via learn variance evaluation accuracy uncertainty calibration prediction performance know decision show yield network high reliability well']\n",
            "\n",
            "Cluster  7\n",
            "['convolutional long short term memory lstm network widely use action gesture recognition different attention mechanisms also embed lstm convolutional lstm convlstm network base previous gesture recognition architectures combine three dimensional convolution neural network dcnn convlstm paper explore effect attention mechanism convlstm several variants convlstm evaluate remove convolutional structure three gate convlstm apply attention mechanism input convlstm reconstruct input output gate respectively modify channel wise attention mechanism evaluation result demonstrate spatial convolutions three gate scarcely contribute spatiotemporal feature fusion attention mechanisms embed input output gate improve feature fusion word convlstm mainly contribute temporal fusion along recurrent step learn long term spatiotemporal feature take input spatial spatiotemporal feature basis new variant lstm derive convolutional structure embed input state transition lstm code lstm variants publicly available', 'channel prune one predominant approach deep model compression exist prune methods either train scratch sparsity constraints channel minimize reconstruction error pre train feature map compress ones strategies suffer limitations former kind computationally expensive difficult converge whilst latter kind optimize reconstruction error ignore discriminative power channel overcome drawbacks investigate simple yet effective method call discrimination aware channel prune choose channel really contribute discriminative power end introduce additional losses network increase discriminative power intermediate layer select discriminative channel layer consider additional loss reconstruction error last propose greedy algorithm conduct channel selection parameter optimization iterative way extensive experiment demonstrate effectiveness method example ilsvrc prune resnet reduction channel even outperform original model top accuracy', 'real time automatic speech recognition asr mobile embed devices great interest many years present real time speech recognition smartphones embed systems employ recurrent neural network rnn base acoustic model rnn base language model beam search decode acoustic model end end train connectionist temporal classification ctc loss rnn implementation embed devices suffer excessive dram access parameter size neural network usually exceed cache memory parameters use time step remedy problem employ multi time step parallelization approach compute multiple output sample time parameters fetch dram since number dram access reduce proportion number parallelization step achieve high process speed however conventional rnns long short term memory lstm gate recurrent unit gru permit multi time step parallelization construct acoustic model combine simple recurrent units srus depth wise dimensional convolution layer multi time step parallelization character word piece model develop acoustic model correspond rnn base language model use beam search decode achieve competitive wer wsj corpus use entire model size around achieve real time speed use single core arm without gpu special hardware', 'estimate individual treatment effect ite challenge problem causal inference due miss counterfactuals selection bias exist ite estimation methods mainly focus balance distributions control treat group ignore local similarity information helpful paper propose local similarity preserve individual treatment effect site estimation method base deep representation learn site preserve local similarity balance data distributions simultaneously focus several hard sample mini batch experimental result synthetic three real world datasets demonstrate advantage propose site method compare state art ite estimation methods']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za_CFSXOWF2V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vO94_SwxzIv",
        "outputId": "835512bb-beaa-456b-a866-391010cd6247"
      },
      "source": [
        "final_list = []\n",
        "for i in clustered_sentences:\n",
        "  if len(i) > 0:\n",
        "    final_list.append(i)\n",
        "for i in list2_a:\n",
        "  if len(i) > 0:\n",
        "    final_list.append(i)\n",
        "len(final_list)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgumKQJvWsnv",
        "outputId": "62642f38-3d8e-45e3-9e4f-552ca246820a"
      },
      "source": [
        "# Final list containing all the CLUSTERS\n",
        "\n",
        "final_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['great progress make recently automatic image manipulation limit object centric image like face structure scene datasets work take step towards general scene level image edit develop automatic interaction free object removal model model learn find remove object general scene image use image level label unpaired data generative adversarial network gin framework achieve two key contributions two stage editor architecture consist mask generator image painter operate remove object novel gin base prior mask generator allow flexibly incorporate knowledge object shape experimentally show two datasets method effectively remove wide variety object use weak supervision',\n",
              "  'exist methods interactive image retrieval demonstrate merit integrate user feedback improve retrieval result however current systems rely restrict form user feedback binary relevance responses feedback base fix set relative attribute limit impact paper introduce new approach interactive image search enable users provide feedback via natural language allow natural effective interaction formulate task dialog base interactive image retrieval reinforcement learn problem reward dialog system improve rank target image dialog turn mitigate cumbersome costly process collect human machine conversations dialog system learn train system user simulator train describe differences target candidate image efficacy approach demonstrate footwear retrieval application experiment simulate real world data show propose learn framework achieve better accuracy supervise reinforcement learn baselines user feedback base natural language rather pre specify attribute lead effective retrieval result natural expressive communication interface',\n",
              "  'graph match receive persistent attention decades formulate quadratic assignment problem qap show large family function define separable function approximate discrete graph match continuous domain asymptotically vary approximation control parameters also study properties global optimality devise convex concave preserve extensions widely use lawler qap form theoretical find show potential derive new algorithms techniques graph match deliver solvers base two specific instance separable function state art performance method verify popular benchmarks',\n",
              "  'convert input binary code hash algorithm widely use approximate nearest neighbor search large scale image set due computation storage efficiency deep hash improve retrieval quality combine hash cod deep neural network however major difficulty deep hash lie discrete constraints impose network output generally make optimization hard work adopt greedy principle tackle hard problem iteratively update network toward probable optimal discrete solution iteration hash cod layer design implement approach strictly use sign function forward propagation maintain discrete constraints back propagation gradients transmit intactly front layer avoid vanish gradients addition theoretical derivation provide new perspective visualize understand effectiveness efficiency algorithm experiment benchmark datasets show scheme outperform state art hash methods supervise unsupervised task',\n",
              "  'recent advance deep learn show excite promise fill large hole natural image semantically plausible context aware detail impact fundamental image manipulation task object removal learn base methods significantly effective capture high level feature prior techniques handle low resolution input due memory limitations difficulty train even slightly larger image inpainted regions would appear blurry unpleasant boundaries become visible propose multi scale neural patch synthesis approach base joint optimization image content texture constraints preserve contextual structure also produce high frequency detail match adapt patch similar mid layer feature correlations deep classification network evaluate method imagenet paris streetview datasets achieve state art inpainting accuracy show approach produce sharper coherent result prior methods especially high resolution image'],\n",
              " ['risk management dynamic decision problems primary concern many field include financial investment autonomous drive healthcare mean variance function one widely use objective function risk management due simplicity interpretability exist algorithms mean variance optimization base multi time scale stochastic approximation whose learn rate schedule often hard tune asymptotic convergence proof paper develop model free policy search framework mean variance optimization finite sample error bind analysis local optima start point reformulation original mean variance function fenchel dual propose stochastic block coordinate ascent policy search algorithm asymptotic convergence guarantee last iteration solution convergence rate randomly pick solution provide applicability demonstrate several benchmark domains',\n",
              "  'multiagent domains cop non stationary agents change behaviors time time challenge problem agent usually require able quickly detect agent policy online interaction adapt policy accordingly paper study efficient policy detect reuse techniques play non stationary agents markov game propose new deep bpr algorithm extend recent bpr algorithm neural network value function approximator detect policy accurately propose textit rectify belief model take advantage textit opponent model infer agent policy reward signal behaviors instead directly store individual policies bpr introduce textit distil policy network serve policy library bpr use policy distillation achieve efficient online policy learn reuse deep bpr inherit advantage bpr empirically show better performance term detection accuracy cumulative reward speed convergence compare exist algorithms complex markov game raw visual input',\n",
              "  'propose novel adaptive test goodness fit computational cost linear number sample learn test feature best indicate differences observe sample reference model minimize false negative rate feature construct via stein method mean necessary compute normalise constant model analyse asymptotic bahadur efficiency new test prove mean shift alternative test always greater relative efficiency previous linear time kernel test regardless choice parameters test experiment performance method exceed earlier linear time test match exceed power quadratic time kernel test high dimension model structure may exploit goodness fit test perform far better quadratic time two sample test base maximum mean discrepancy sample draw model',\n",
              "  'mainstream caption model often follow sequential structure generate cap tions lead issue introduction irrelevant semantics lack diversity generate caption inadequate generalization performance paper present alternative paradigm image caption factorize caption procedure two stag extract explicit semantic representation give image construct caption base recursive compositional procedure bottom manner compare conventional ones paradigm better preserve semantic content explicit factorization semantics syntax use compositional generation procedure caption construction follow recursive structure naturally fit properties human language moreover propose compositional procedure require less data train generalize better yield diverse caption',\n",
              "  'sequential prediction problems imitation learn future observations depend previous predictions action violate common assumptions make statistical learn lead poor performance theory often practice recent approach provide stronger guarantee set remain somewhat unsatisfactory train either non stationary stochastic policies require large number iterations paper propose new iterative algorithm train stationary deterministic policy see regret algorithm online learn set show regret algorithm combine additional reduction assumptions must find policy good performance distribution observations induce sequential settings demonstrate new approach outperform previous approach two challenge imitation learn problems benchmark sequence label problem',\n",
              "  'probabilistic generative model provide powerful framework represent data avoid expense manual annotation typically need discriminative approach model selection generative set challenge however particularly likelihoods easily accessible address issue introduce statistical test relative similarity use determine two model generate sample significantly closer real world reference dataset interest use test statistic difference maximum mean discrepancies mmds reference dataset model dataset derive powerful low variance test base joint asymptotic distribution mmds reference model pair experiment deep generative model include variational auto encoder generative moment match network test provide meaningful rank model performance function parameter train settings',\n",
              "  'present novel unify deep learn framework capable learn domain invariant representation data across multiple domains realize adversarial train additional ability exploit domain specific information propose network able perform continuous cross domain image translation manipulation produce desirable output image accordingly addition result feature representation exhibit superior performance unsupervised domain adaptation also verify effectiveness propose model learn disentangle feature describe cross domain data',\n",
              "  'learn capture long range relations fundamental image video recognition exist cnn model generally rely increase depth model relations highly inefficient work propose double attention block novel component aggregate propagate informative global feature entire spatio temporal space input image videos enable subsequent convolution layer access feature entire space efficiently component design double attention mechanism two step first step gather feature entire space compact set second order attention pool second step adaptively select distribute feature location via another attention propose double attention block easy adopt plug exist deep neural network conveniently conduct extensive ablation study experiment image video recognition task evaluate performance image recognition task resnet equip double attention block outperform much larger resnet architecture imagenet dataset less number parameters less flop action recognition task propose model achieve state art result kinetics ucf datasets significantly higher efficiency recent work',\n",
              "  'active learn task use label data select additional point label goal fit accurate model fix budget label point binary classification active learn know produce faster rat passive learn broad range settings however regression restrictive structure tailor methods previously need obtain theoretically superior performance paper propose intuitive tree base active learn algorithm non parametric regression provable improvement random sample implement mondrian tree algorithm tune parameter free consistent minimax optimal lipschitz function',\n",
              "  'despite achieve impressive performance state art classifiers remain highly vulnerable small imperceptible adversarial perturbations vulnerability prove empirically intricate address paper study phenomenon adversarial perturbations assumption data generate smooth generative model derive fundamental upper bound robustness perturbations classification function prove existence adversarial perturbations transfer well across different classifiers small risk analysis robustness also provide insights onto key properties generative model smoothness dimensionality latent space conclude numerical experimental result show bound provide informative baselines maximal achievable robustness several datasets',\n",
              "  'study generalization classic isotonic regression problem allow separable nonconvex objective function focus case estimators use robust regression simple dynamic program approach allow solve problem within accuracy global minimum time linear dimension combine techniques convex case branch bind ideas form new algorithm problem naturally exploit shape objective function algorithm achieve best bound general nonconvex convex case linear log perform much faster practice straightforward dynamic program approach especially desire accuracy increase',\n",
              "  'generative adversarial network gin powerful subclass generative model despite rich research activity lead numerous interest gin algorithms still hard assess algorithm perform better others conduct neutral multi faceted large scale empirical study state art model evaluation measure find model reach similar score enough hyperparameter optimization random restart suggest improvements arise higher computational budget tune fundamental algorithmic change overcome limitations current metrics also propose several data set precision recall compute experimental result suggest future gin research base systematic objective evaluation procedures finally find evidence test algorithms consistently outperform non saturate gin introduce cite goodfellow generative',\n",
              "  'residual network resnet standard deep neural net architecture state art performance across numerous applications main premise resnets allow train layer focus fit residual previous layer output target output thus expect train network worse obtain remove residual layer train shallower network instead however due non convexity optimization problem clear resnets indeed achieve behavior rather get stick arbitrarily poor local minimum paper rigorously prove arbitrarily deep nonlinear residual units indeed exhibit behavior sense optimization landscape contain local minima value obtain linear predictor namely layer network notably show minimal assumptions precise network architecture data distribution loss function use also provide quantitative analysis approximate stationary point problem finally show certain tweak architecture train network standard stochastic gradient descent achieve objective value close better linear predictor',\n",
              "  'perform efficient inference learn direct probabilistic model presence continuous latent variables intractable posterior distributions large datasets introduce stochastic variational inference learn algorithm scale large datasets mild differentiability condition even work intractable case contributions two fold first show reparameterization variational lower bind yield lower bind estimator straightforwardly optimize use standard stochastic gradient methods second show datasets continuous latent variables per datapoint posterior inference make especially efficient fit approximate inference model also call recognition model intractable posterior use propose lower bind estimator theoretical advantage reflect experimental result',\n",
              "  'train deep neural network complicate fact distribution layer input change train parameters previous layer change slow train require lower learn rat careful parameter initialization make notoriously hard train model saturate nonlinearities refer phenomenon internal covariate shift address problem normalize layer input method draw strength make normalization part model architecture perform normalization train mini batch batch normalization allow use much higher learn rat less careful initialization also act regularizer case eliminate need dropout apply state art image classification model batch normalization achieve accuracy time fewer train step beat original model significant margin use ensemble batch normalize network improve upon best publish result imagenet classification reach top validation error test error exceed accuracy human raters',\n",
              "  'real world image recognition often challenge variability visual style include object textures light condition filter effect etc although variations deem implicitly handle train data deeper network recent advance image style transfer suggest also possible explicitly manipulate style information extend idea general visual recognition problems present batch instance normalization bin explicitly normalize unnecessary style image consider certain style feature play essential role discriminative task bin learn selectively normalize disturb style preserve useful style propose normalization module easily incorporate exist network architectures residual network surprisingly improve recognition performance various scenarios furthermore experiment verify bin effectively adapt completely different task like object classification style transfer control trade preserve remove style variations bin implement line code use popular deep learn frameworks',\n",
              "  'base non local prior distributions propose bayesian model selection bms procedure boundary detection sequence data multiple systematic mean change bms method effectively suppress non boundary spike point large instantaneous change speed algorithm reduce multiple change point series single change point detection problems establish consistency estimate number locations change point various prior distributions extensive simulation study conduct compare bms exist methods approach illustrate application magnetic resonance image guide radiation therapy data',\n",
              "  'long live autonomous agent able respond online novel instance task familiar domain act online require fast responses term rapid convergence especially task instance short duration applications involve interactions humans requirements problematic many establish methods learn act domains agent know task instance draw family relate task albeit without access label give instance choose act process policy reuse library rather policy learn scratch policy reuse agent prior knowledge class task form library policies learn sample task instance offline train phase formalise problem policy reuse present algorithm efficiently respond novel task instance reuse policy library exist policies choice base observe signal correlate policy performance achieve pose problem bayesian choice problem correspond notion optimal response computation response many case intractable therefore reduce computation cost posterior follow bayesian optimisation approach define set policy selection function balance exploration policy library exploitation previously try policies together model expect performance policy library correspond task instance validate method several simulate domains interactive short duration episodic task show rapid convergence unknown task variations',\n",
              "  'propose data efficient gaussian process base bayesian approach semi supervise learn problem graph propose model show extremely competitive performance compare state art graph neural network semi supervise learn benchmark experiment outperform neural network active learn experiment label scarce furthermore model require validation data set early stop control fit model view instance empirical distribution regression weight locally network connectivity motivate intuitive construction model bayesian linear model interpretation node feature filter operator relate graph laplacian method easily implement adapt shelf scalable variational inference algorithms gaussian process',\n",
              "  'propose sparse low rank tensor regression model relate univariate outcome feature tensor unit rank tensor decomposition coefficient tensor assume sparse structure parsimonious highly interpretable imply outcome relate feature distinct pathways may involve subsets feature dimension take divide conquer strategy simplify task set sparse unit rank tensor regression problems make computation efficient scalable unit rank tensor regression propose stagewise estimation procedure efficiently trace entire solution path show step size go zero stagewise solution paths converge exactly correspond regularize regression superior performance approach demonstrate various real world synthetic examples',\n",
              "  'approximate probability density tractable manner central task bayesian statistics variational inference popular technique achieve tractability choose relatively simple variational approximation borrow ideas classic boost framework recent approach attempt emph boost replace selection single density iteratively construct mixture densities order guarantee convergence previous work impose stringent assumptions require significant effort practitioners specifically require custom implementation greedy step call lmo every probabilistic model respect unnatural variational family truncate distributions work fix issue novel theoretical algorithmic insights theoretical side show boost satisfy relax smoothness assumption sufficient convergence functional frank wolfe algorithm furthermore rephrase lmo problem propose maximize residual elbo relbo replace standard elbo optimization theoretical enhancements allow black box implementation boost subroutine finally present stop criterion draw duality gap classic analyse exhaustive experiment illustrate usefulness theoretical algorithmic contributions',\n",
              "  'present learn base approach compute solutions certain hard problems approach combine deep learn techniques useful algorithmic elements classic heuristics central component graph convolutional network train estimate likelihood vertex graph whether vertex part optimal solution network design train synthesize diverse set solutions enable rapid exploration solution space via tree search present approach evaluate four canonical hard problems five datasets include benchmark satisfiability problems real social network graph hundred thousand nod experimental result demonstrate present approach substantially outperform recent deep learn work perform par highly optimize state art heuristic solvers hard problems experiment indicate approach generalize across datasets scale graph order magnitude larger use train',\n",
              "  'new class dependent random measure call compound random measure propose use normalize versions random measure priors bayesian nonparametric mixture model consider tractability allow properties compound random measure normalize compound random measure derive particular show compound random measure construct gamma sigma stable generalize gamma process marginals also derive several form laplace exponent characterize dependence evy copula correlation function slice sampler augment olya urn scheme sampler describe posterior inference normalize compound random measure use mix measure nonparametric mixture model data example discuss',\n",
              "  'introduce principled approach unsupervised structure learn deep neural network propose new interpretation depth inter layer connectivity conditional independencies input distribution encode hierarchically network structure thus depth network determine inherently propose method cast problem neural network structure learn problem bayesian network structure learn instead directly learn discriminative structure learn generative graph construct stochastic inverse construct discriminative graph prove conditional dependency relations among latent variables generative graph preserve class conditional discriminative graph demonstrate image classification benchmarks deepest layer convolutional dense common network replace significantly smaller learn structure maintain classification accuracy state art test benchmarks structure learn algorithm require small computational cost run efficiently standard desktop cpu',\n",
              "  'goal precipitation nowcasting predict future rainfall intensity local region relatively short period time previous study examine crucial challenge weather forecast problem machine learn perspective paper formulate precipitation nowcasting spatiotemporal sequence forecast problem input prediction target spatiotemporal sequence extend fully connect lstm lstm convolutional structure input state state state transition propose convolutional lstm convlstm use build end end trainable model precipitation nowcasting problem experiment show convlstm network capture spatiotemporal correlations better consistently outperform lstm state art operational rover algorithm precipitation nowcasting',\n",
              "  'holistic indoor scene understand refer jointly recover object bound box room layout iii camera pose exist methods either ineffective tackle problem partially paper propose end end model simultaneously solve three task real time give single rgb image essence propose method improve prediction parametrizing target box instead directly estimate target cooperative train across different modules contrast train modules individually specifically parametrize object bound box predictions several modules camera pose object attribute propose method provide two major advantage parametrization help maintain consistency image world thus largely reduce prediction variances coordinate constraints impose parametrization train different modules simultaneously call constraints cooperative losses enable joint train inference employ three cooperative losses bound box projections physical constraints estimate geometrically consistent physically plausible scene experiment sun rgb dataset show propose method significantly outperform prior approach layout estimation object detection camera pose estimation holistic scene understand',\n",
              "  'visual attention derive cognitive neuroscience facilitate human perception pertinent subset sensory data recently significant efforts make exploit attention scheme advance computer vision systems visual track often challenge track target object undergo large appearance change attention map facilitate visual track selectively pay attention temporal robust feature exist track detection approach mainly use additional attention modules generate feature weight classifiers equip mechanisms paper propose reciprocative learn algorithm exploit visual attention train deep classifiers propose algorithm consist fee forward backward operations generate attention map serve regularization term couple original classification loss function train deep classifier learn attend regions target object robust appearance change extensive experiment large scale benchmark datasets show propose attentive track method perform favorably state art approach',\n",
              "  'deep learn see remarkable developments last years many inspire neuroscience however main learn mechanism behind advance error backpropagation appear odds neurobiology introduce multilayer neuronal network model simplify dendritic compartments error drive synaptic plasticity adapt network towards global desire output contrast previous work model require separate phase synaptic learn drive local dendritic prediction errors continuously time errors originate apical dendrites occur due mismatch predictive input lateral interneurons activity actual top feedback use simple dendritic compartments different cell type model represent error normal activity within pyramidal neuron demonstrate learn capabilities model regression classification task show analytically approximate error backpropagation algorithm moreover framework consistent recent observations learn brain areas architecture cortical microcircuits overall introduce novel view learn dendritic cortical circuit brain may solve long stand synaptic credit assignment problem',\n",
              "  'deep neural network dnns recently show state art performance semantic segmentation task however still suffer problems poor boundary localization spatial fragment predictions difficulties lie requirement make dense predictions long path model since detail hard keep data go deeper layer instead work decompose difficult task two relative simple sub task seed detection require predict initial predictions without need wholeness preciseness similarity estimation measure possibility two nod belong class without need know class use one branch network one sub task apply cascade random walk base hierarchical semantics approximate complex diffusion process propagate seed information whole image accord estimate similarities propose difnet consistently produce improvements baseline model depth equivalent number parameters also achieve promise performance pascal voc pascal context dataset ourdifnet train end end without complex loss function',\n",
              "  'simple way improve performance almost machine learn algorithm train many different model data average predictions unfortunately make predictions use whole ensemble model cumbersome may computationally expensive allow deployment large number users especially individual model large neural net caruana collaborators show possible compress knowledge ensemble single model much easier deploy develop approach use different compression technique achieve surprise result mnist show significantly improve acoustic model heavily use commercial system distil knowledge ensemble model single model also introduce new type ensemble compose one full model many specialist model learn distinguish fine grain class full model confuse unlike mixture experts specialist model train rapidly parallel',\n",
              "  'stochastic convex optimization algorithms popular way train machine learn model large scale data scale train process model crucial popular algorithm stochastic gradient descent sgd serial method surprisingly hard parallelize paper propose efficient distribute stochastic optimization method combine adaptivity variance reduction techniques analysis yield linear speedup number machine constant memory footprint logarithmic number communication round critically approach black box reduction parallelize serial online learn algorithm streamline prior analysis allow leverage significant progress make design adaptive algorithms particular achieve optimal convergence rat without prior knowledge smoothness parameters yield robust algorithm reduce need hyperparameter tune implement algorithm spark distribute framework exhibit dramatic performance gain large scale logistic regression problems',\n",
              "  'zero shoot learn zsl aim recognize unseen object class without train sample regard form transfer learn see class unseen ones make possible learn projection feature space semantic space attribute space key zsl thus learn projection function robust often large domain gap see unseen class paper propose novel zsl model term domain invariant projection learn dipl model two novel components domain invariant feature self reconstruction task introduce see unseen class data result simple linear formulation cast zsl min min optimization problem solve problem non trivial novel iterative algorithm formulate solver rigorous theoretic algorithm analysis provide align two domains via learn projection share semantic structure among see unseen class explore via form superclasses semantic space extensive experiment show model outperform state art alternatives significant margins',\n",
              "  'recent methods learn linear subspace data corrupt outliers base convex nuclear norm optimization require dimension subspace number outliers sufficiently small sharp contrast recently propose dual principal component pursuit dpcp method provably handle subspaces high dimension solve non convex optimization problem sphere however geometric analysis base quantities difficult interpret amenable statistical analysis paper provide refine geometric analysis new statistical analysis show dpcp tolerate many outliers square number inliers thus improve upon provably correct robust pca methods also propose scalable project sub gradient descent method dpcp psgd solve dpcp problem show admit linear convergence even though underlie optimization problem non convex non smooth experiment road plane detection point cloud data demonstrate dpcp psgd efficient traditional ransac algorithm one popular methods computer vision applications',\n",
              "  'train discrete latent variable model remain challenge pass gradient information discrete units difficult propose new class smooth transformations base mixture two overlap distributions show propose transformation use train binary latent model either direct undirected priors derive new variational bind efficiently train boltzmann machine priors use bind develop dvae generative model global discrete prior hierarchy convolutional continuous variables experiment several benchmarks show overlap transformations outperform recent continuous relaxations discrete latent variables include gumbel softmax maddison jang discrete variational autoencoders rolfe',\n",
              "  'deep convolutional neural network demonstrate powerfulness variety applications however storage computational requirements largely restrict extensions mobile devices recently prune unimportant parameters use network compression acceleration consider spatial redundancy within filter cnn propose frequency domain dynamic prune scheme exploit spatial correlations frequency domain coefficients prune dynamically iteration different frequency band prune discriminatively give different importance accuracy experimental result demonstrate propose scheme outperform previous spatial domain counterparts large margin specifically achieve compression ratio theoretical inference speed resnet accuracy even better reference model cifar',\n",
              "  'conditional density estimation cde model deal estimate conditional distributions condition impose distribution input model cde challenge task fundamental trade model complexity representational capacity overfitting work propose extend model input latent variables use gaussian process map augment input onto sample conditional distribution bayesian approach allow model small datasets also provide machinery apply big data use stochastic variational inference approach use model densities even sparse data regions allow share learn structure condition illustrate effectiveness wide reach applicability model variety real world problems spatio temporal density estimation taxi drop off non gaussian noise model shoot learn omniglot image',\n",
              "  'present generative framework generalize zero shoot learn train test class necessarily disjoint build upon variational autoencoder base architecture consist probabilistic encoder probabilistic conditional decoder model generate novel exemplars see unseen class give respective class attribute exemplars subsequently use train shelf classification model one key aspects encoder decoder architecture feedback drive mechanism discriminator multivariate regressor learn map generate exemplars correspond class attribute vectors lead improve generator model ability generate leverage examples unseen class train classification model naturally help mitigate bias towards predict see class generalize zero shoot learn settings comprehensive set experiment show model outperform several state art methods several benchmark datasets standard well generalize zero shoot learn',\n",
              "  'probability estimation one fundamental task statistics machine learn however standard methods probability estimation discrete object handle object structure satisfactory manner paper derive general bayesian network formulation probability estimation leaf label tree enable flexible approximations generalize beyond observations show efficient algorithms learn bayesian network easily extend probability estimation challenge structure space experiment synthetic real data show methods greatly outperform current practice use empirical distribution well previous effort probability estimation tree',\n",
              "  'responses generate neural conversational model tend lack informativeness diversity present adversarial information maximization aim adversarial learn framework address two relate distinct problems foster response diversity leverage adversarial train allow distributional match synthetic real responses improve informativeness framework explicitly optimize variational lower bind pairwise mutual information query response empirical result automatic human evaluations demonstrate methods significantly boost informativeness diversity',\n",
              "  'propose new framework estimate generative model via adversarial net simultaneously train two model generative model capture data distribution discriminative model estimate probability sample come train data rather train procedure maximize probability make mistake framework correspond minimax two player game space arbitrary function unique solution exist recover train data distribution equal everywhere case define multilayer perceptrons entire system train backpropagation need markov chain unroll approximate inference network either train generation sample experiment demonstrate potential framework qualitative quantitatively evaluation generate sample',\n",
              "  'recent deep learn base approach show promise result challenge task inpainting large miss regions image methods generate visually plausible image structure textures often create distort structure blurry textures inconsistent surround areas mainly due ineffectiveness convolutional neural network explicitly borrow copy information distant spatial locations hand traditional texture patch synthesis approach particularly suitable need borrow textures surround regions motivate observations propose new deep generative model base approach synthesize novel image structure also explicitly utilize surround image feature reference network train make better predictions model fee forward fully convolutional neural network process image multiple hole arbitrary locations variable size test time experiment multiple datasets include face celeba celeba textures dtd natural image imagenet place demonstrate propose approach generate higher quality inpainting result exist ones code demo model available https github com jiahuiyu generative inpainting',\n",
              "  'introduce genetic gate network simple neural network combine gate vector compose binary genetic genes hide layer network method take advantage gradient free optimization gradient base optimization methods former effective problems multiple local minima latter quickly find local minima addition multiple chromosomes define different model make easy construct multiple model effectively apply problems require multiple model show apply typical reinforcement learn algorithms achieve large improvement sample efficiency performance',\n",
              "  'monte carlo sample high dimensional low sample settings important many machine learn task improve current methods sample euclidean space avoid independence instead consider ways couple sample show fundamental connections optimal transport theory lead novel sample algorithms provide new theoretical ground exist strategies compare new strategies prior methods improve sample efficiency include qmc study discrepancy explore find empirically observe benefit sample scheme reinforcement learn generative model',\n",
              "  'propose new type generative model high dimensional data learn manifold geometry data rather density generate point evenly along manifold contrast exist generative model represent data density strongly affect noise artifacts data collection demonstrate approach correct sample bias artifacts thus improve several downstream data analysis task cluster classification finally demonstrate approach especially useful biology despite advent single cell technologies rare subpopulations gene interaction relationships affect bias sample show sugar generate hypothetical populations able reveal intrinsic pattern mutual information relationships genes single cell rna sequence dataset hematopoiesis',\n",
              "  'consider high dimensional linear regression problem goal efficiently recover unknown vector beta noisy linear observations beta instead adopt regularization base assume underlie vectors beta rational entries denominator call rationality assumption propose new polynomial time algorithm task base seminal lenstra lenstra lovasz lll lattice basis reduction algorithm establish rationality assumption algorithm recover exactly vector beta large class distributions iid entries non zero noise prove successful small noise even learner access one observation furthermore prove case gaussian white noise log sufficiently large algorithm tolerate nearly optimal information theoretic level noise',\n",
              "  'quantization promise technique reduce model size memory footprint massive computation operations recurrent neural network rnns embed devices limit resources although extreme low bite quantization achieve impressive success convolutional neural network still suffer huge accuracy degradation rnns low bite precision paper first investigate accuracy degradation rnn model different quantization scheme distribution tensor value full precision model observation reveal due difference distributions weight activations different quantization methods suitable different part model base observation propose hitnet hybrid ternary recurrent neural network bridge accuracy gap full precision model quantize model hitnet develop hybrid quantization method quantize weight activations moreover introduce slop factor motivate prior work boltzmann machine activation function close accuracy gap full precision model quantize model overall hitnet quantize rnn model ternary value outperform state art quantization methods rnn model significantly test typical rnn model long short term memory lstm gate recurrent units gru result outperform previous work significantly example improve perplexity per word ppw ternary lstm penn tree bank ptb corpus state art result best knowledge full precision model ternary gru full precision model',\n",
              "  'batch normalization batchnorm widely adopt technique enable faster stable train deep neural network dnns despite pervasiveness exact reason batchnorm effectiveness still poorly understand popular belief effectiveness stem control change layer input distributions train reduce call internal covariate shift work demonstrate distributional stability layer input little success batchnorm instead uncover fundamental impact batchnorm train process make optimization landscape significantly smoother smoothness induce predictive stable behavior gradients allow faster train',\n",
              "  'identify study two common failure modes early train deep relu net give rigorous proof occur avoid fully connect convolutional residual architectures show first failure mode explode vanish mean activation length avoid initialize weight symmetric distribution variance fan resnets correctly scale residual modules prove second failure mode exponentially large variance activation length never occur residual net first failure mode avoid contrast fully connect net prove failure mode happen avoid keep constant sum reciprocals layer widths demonstrate empirically effectiveness theoretical result predict network able start train particular note many popular initializations fail criteria whereas correct initialization architecture allow much deeper network train',\n",
              "  'dominant object detection approach treat recognition region separately overlook crucial semantic correlations object one scene paradigm lead substantial performance drop face heavy long tail problems sample available rare class plenty confuse categories exist exploit diverse human commonsense knowledge reason large scale object categories reach semantic coherency within one image particularly present hybrid knowledge rout modules hkrm incorporate reason rout two kinds knowledge form explicit knowledge module structure constraints summarize linguistic knowledge share attribute relationships concepts implicit knowledge module depict implicit constraints common spatial layouts function region region graph modules individualize adapt coordinate visual pattern image guide specific knowledge form hkrm light weight general purpose extensible easily incorporate multiple knowledge endow detection network ability global semantic reason experiment large scale object detection benchmarks show hkrm obtain around improvement visualgenome categories ade term map',\n",
              "  'generate long coherent report describe medical image pose challenge bridge visual pattern informative human linguistic descriptions propose novel hybrid retrieval generation reinforce agent hrgr agent reconcile traditional retrieval base approach populate human prior knowledge modern learn base approach achieve structure robust diverse report generation hrgr agent employ hierarchical decision make procedure sentence high level retrieval policy module choose either retrieve template sentence shelf template database invoke low level generation module generate new sentence hrgr agent update via reinforcement learn guide sentence level word level reward experiment show approach achieve state art result two medical report datasets generate well balance structure sentence robust coverage heterogeneous medical report content addition model achieve highest detection precision medical abnormality terminologies improve human evaluation performance',\n",
              "  'provide simple efficient way compute low variance gradients continuous random variables reparameterization trick become technique choice train variety latent variable model however applicable number important continuous distributions introduce alternative approach compute reparameterization gradients base implicit differentiation demonstrate broader applicability apply gamma beta dirichlet von mises distributions use classic reparameterization trick experiment show propose approach faster accurate exist gradient estimators distributions',\n",
              "  'generative adversarial network gans powerful generative model suffer train instability recently propose wasserstein gin wgan make progress toward stable train gans sometimes still generate low quality sample fail converge find problems often due use weight clip wgan enforce lipschitz constraint critic lead undesired behavior propose alternative clip weight penalize norm gradient critic respect input propose method perform better standard wgan enable stable train wide variety gin architectures almost hyperparameter tune include layer resnets language model discrete data also achieve high quality generations cifar lsun bedrooms',\n",
              "  'give two candidate model set target observations address problem measure relative goodness fit two model propose two new statistical test nonparametric computationally efficient runtime complexity linear sample size interpretable unique advantage test produce set examples informative feature indicate regions data domain one model fit significantly better real world problem compare gin model test power new test match state art test relative goodness fit one order magnitude faster',\n",
              "  'deep neural network highly expressive model recently achieve state art performance speech visual recognition task expressiveness reason succeed also cause learn uninterpretable solutions could counter intuitive properties paper report two properties first find distinction individual high level units random linear combinations high level units accord various methods unit analysis suggest space rather individual units contain semantic information high layer neural network second find deep neural network learn input output mappings fairly discontinuous significant extend cause network misclassify image apply certain imperceptible perturbation find maximize network prediction error addition specific nature perturbations random artifact learn perturbation cause different network train different subset dataset misclassify input',\n",
              "  'present novel introspective variational autoencoder introvae model synthesize high resolution photographic image introvae capable self evaluate quality generate sample improve accordingly inference generator model jointly train introspective way one hand generator require reconstruct input image noisy output inference model normal vaes hand inference model encourage classify generate real sample generator try fool gans two famous generative frameworks integrate simple yet efficient single stream architecture train single stage introvae preserve advantage vaes stable train nice latent manifold unlike hybrid model vaes gans introvae require extra discriminators inference model serve discriminator distinguish generate real sample experiment demonstrate method produce high resolution photo realistic image celeba image comparable better state art gans',\n",
              "  'consider problem active feature acquisition goal sequentially select subset feature order achieve maximum prediction performance cost effective way test time work formulate active feature acquisition jointly learn problem train classifier environment agent decide either collect new feature test time cost sensitive manner also introduce novel encode scheme represent acquire subsets feature propose order invariant set encode feature level also significantly reduce search space agent evaluate model carefully design synthetic dataset active feature acquisition well several medical datasets framework show meaningful feature acquisition process diagnosis comply human knowledge outperform baselines term prediction performance well feature acquisition cost',\n",
              "  'indispensable component batch normalization successfully improve train deep neural network dnns mini batch normalize distribution internal representation hide layer however effectiveness would diminish scenario micro batch less sample mini batch since estimate statistics mini batch reliable insufficient sample limit room train larger model segmentation detection video relate problems require small batch constrain memory consumption paper present novel normalization method call kalman normalization improve accelerate train dnns particularly context micro batch specifically unlike exist solutions treat hide layer isolate system treat layer network whole system estimate statistics certain layer consider distributions precede layer mimic merit kalman filter resnet train imagenet lower error counterpart use batch size even use typical batch size still maintain advantage variants suffer performance degradation moreover naturally generalize many exist normalization variants obtain gain equip group normalization group kalman normalization gkn outperform variants large scale object detection segmentation task coco',\n",
              "  'knowledge distillation aim train lightweight classifier suitable provide accurate inference constrain resources multi label learn instead directly consume feature label pair classifier train teacher high capacity model whose train may resource hungry accuracy classifier train way usually suboptimal difficult learn true data distribution teacher alternative method adversarially train classifier discriminator two player game akin generative adversarial network gin ensure classifier learn true data distribution equilibrium game however may take excessively long time two player game reach equilibrium due high variance gradient update address limitations propose three player game name kdgan consist classifier teacher discriminator classifier teacher learn via distillation losses adversarially train discriminator via adversarial losses simultaneously optimize distillation adversarial losses classifier learn true data distribution equilibrium approximate discrete distribution learn classifier teacher concrete distribution concrete distribution generate continuous sample obtain low variance gradient update speed train extensive experiment use real datasets confirm superiority kdgan accuracy train speed',\n",
              "  'present framework learn disentangle interpretable jointly continuous discrete representations unsupervised manner augment continuous latent distribution variational autoencoders relax discrete distribution control amount information encode latent unit show continuous categorical factor variation discover automatically data experiment show framework disentangle continuous discrete generative factor various datasets outperform current disentangle methods discrete generative factor prominent',\n",
              "  'address problem learn semantic representation question measure similarity pair continuous distance metric work naturally extend word mover distance wmd represent text document normal distributions instead bag embed word learn metric measure dissimilarity two question minimum amount distance intent hide representation one question need travel match intent another question first learn repeat reformulate question infer intents normal distributions deep generative model variational auto encoder semantic similarity pair learn discriminatively optimal transport distance metric wasserstein novel variational siamese framework among know model read sentence individually propose framework achieve competitive result quora duplicate question dataset work shed light deep generative model approximate distributions semantic representations effectively measure semantic similarity meaningful distance metrics information theory',\n",
              "  'human scene understand use variety visual non visual cue perform inference object type pose relations physics rich universal cue exploit enhance scene understand integrate physical cue stability learn process use reinforce approach couple physics engine apply problem produce bound box pose object scene first show apply physics supervision exist scene understand model increase performance produce stable predictions allow train equivalent performance level fewer annotate train examples present novel architecture scene parse name prim cnn learn predict bound box well size translation rotation physics supervision prim cnn outperform exist scene understand approach problem finally show apply physics supervision unlabeled real image improve real domain transfer model train synthetic data',\n",
              "  'single image humans able perceive full shape object exploit learn shape priors everyday life contemporary single image reconstruction algorithms aim solve task similar fashion often end priors highly bias train class present algorithm generalizable reconstruction genre design capture generic class agnostic shape priors achieve inference network train procedure combine representations visible surface depth silhouette spherical shape representations visible non visible surface voxel base representations principled manner exploit causal structure shape give rise image experiment demonstrate genre perform well single view shape reconstruction generalize diverse novel object categories see train',\n",
              "  'object relationships critical content image understand scene graph provide structure description capture properties image however reason relationships object challenge recent work attempt solve problem generate scene graph image paper present novel method improve scene graph generation explicitly model inter dependency among entire object instance design simple effective relational embed module enable model jointly represent connections among relate object rather focus object isolation novel method significantly benefit two main part scene graph generation task object classification relationship classification use top basic faster cnn model achieve state art result visual genome benchmark push performance introduce global context encode module geometrical layout encode module validate final model linknet extensive ablation study demonstrate efficacy scene graph generation',\n",
              "  'deep neural network suffer fit catastrophic forget train small data one natural remedy problem data augmentation recently show effective however previous work either assume intra class variances always generalize new class employ naive generation methods hallucinate finite examples without model latent distributions work propose covariance preserve adversarial augmentation network overcome exist limit low shoot learn specifically novel generative adversarial network design model latent distribution novel class give relate base counterparts since direct estimation novel class inductively bias explicitly preserve covariance information variability base examples generation process empirical result show model generate realistic yet diverse examples lead substantial improvements imagenet benchmark state art',\n",
              "  'learn examples remain key challenge machine learn despite recent advance important domains vision language standard supervise deep learn paradigm offer satisfactory solution learn new concepts rapidly little data work employ ideas metric learn base deep neural feature recent advance augment neural network external memories framework learn network map small label support set unlabelled example label obviate need fine tune adapt new class type define one shoot learn problems vision use omniglot imagenet language task algorithm improve one shoot accuracy imagenet omniglot compare compete approach also demonstrate usefulness model language model introduce one shoot task penn treebank',\n",
              "  'train model generalize new domains test time problem fundamental importance machine learn work encode notion domain generalization use novel regularization function pose problem find regularization function learn learn meta learn framework objective domain generalization explicitly model learn regularizer make model train one domain perform well another domain experimental validations computer vision natural language datasets indicate method learn regularizers achieve good cross domain generalization',\n",
              "  'consider problem sample constrain distributions pose significant challenge non asymptotic analysis algorithmic design propose unify framework inspire classical mirror descent derive novel first order sample scheme prove general target distribution strongly convex potential framework imply existence first order algorithm achieve epsilon convergence suggest state art epsilon vastly improve important latent dirichlet allocation lda application mind specialize algorithm sample dirichlet posteriors derive first non asymptotic epsilon rate first order sample extend framework mini batch set prove convergence rat stochastic gradients available finally report promise experimental result lda real datasets',\n",
              "  'complete data matrix become ubiquitous problem modern data science motivations recommender systems computer vision network inference name one typical assumption low rank general model assume column correspond one several low rank matrices paper generalize model call mixture matrix completion mmc case entry correspond one several low rank matrices mmc accurate model recommender systems bring flexibility completion cluster problems make four fundamental contributions new model first show mmc theoretically possible well pose second give precise information theoretic identifiability condition third derive sample complexity mmc finally give practical algorithm mmc performance comparable state art simpler relate problems synthetic real data',\n",
              "  'model interpretability increasingly important component practical machine learn common form interpretability systems example base local global explanations one main challenge interpretability design explanation systems capture aspects explanation type order develop thorough understand model address challenge novel model call maple use local linear model techniques along dual interpretation random forest supervise neighborhood approach feature selection method maple two fundamental advantage exist interpretability systems first effective black box explanation system maple highly accurate predictive model provide faithful self explanations thus sidestep typical accuracy interpretability trade specifically demonstrate several uci datasets maple least accurate random forest produce faithful local explanations lime popular interpretability system second maple provide example base local explanations detect global pattern allow diagnose limitations local explanations',\n",
              "  'propose novel class network model temporal dyadic interaction data objective capture important feature often observe social interactions sparsity degree heterogeneity community structure reciprocity use mutually excite hawk process model interactions direct pair individuals intensity process allow interactions arise responses opposite interactions reciprocity due share interest individuals community structure sparsity degree heterogeneity build non time dependent part intensity function compound random measure follow todeschini conduct experiment real world temporal interaction data show propose model outperform compete approach link prediction lead interpretable parameters',\n",
              "  'scale model capacity vital success deep learn typical network necessary compute resources train time grow dramatically model size conditional computation promise way increase number parameters relatively small increase resources propose train algorithm flexibly choose neural modules base data process decomposition modules learn end end contrast exist approach train rely regularization enforce diversity module use apply modular network image recognition language model task achieve superior performance compare several baselines introspection reveal modules specialize interpretable contexts',\n",
              "  'multivariate time series usually contain large number miss value hinder application advance analysis methods multivariate time series data conventional approach address challenge miss value include mean zero imputation case deletion matrix factorization base imputation incapable model temporal dependencies nature complex distribution multivariate time series paper treat problem miss value imputation data generation inspire success generative adversarial network gin image generation propose learn overall distribution multivariate time series dataset gin use generate miss value sample different image data time series data usually incomplete due nature data record process modify gate recurrent unit employ gin model temporal irregularity incomplete time series experiment two multivariate time series datasets show propose model outperform baselines term accuracy imputation experimental result also show simple model impute data achieve state art result prediction task demonstrate benefit model downstream applications',\n",
              "  'bayesian optimisation refer class methods global optimisation function accessible via point evaluations typically use settings expensive evaluate common use case machine learn model selection possible analytically model generalisation performance statistical model resort noisy expensive train validation procedures choose best model conventional methods focus euclidean categorical domains context model selection permit tune scalar hyper parameters machine learn algorithms however surge interest deep learn increase demand tune neural network architectures work develop nasbot gaussian process base framework neural architecture search accomplish develop distance metric space neural network architectures compute efficiently via optimal transport program distance might independent interest deep learn community may find applications outside demonstrate nasbot outperform alternatives architecture search several cross validation base model selection task multi layer perceptrons convolutional neural network',\n",
              "  'non local methods exploit self similarity natural signal well study example image analysis restoration exist approach however rely nearest neighbor knn match fix feature space main hurdle optimize feature space application performance non differentiability knn selection rule overcome propose continuous deterministic relaxation knn selection maintain differentiability pairwise distance retain original knn limit temperature parameter approach zero exploit relaxation propose neural nearest neighbor block block novel non local process layer leverage principle self similarity use build block modern neural network architectures show effectiveness set reason task correspondence classification well image restoration include image denoising single image super resolution outperform strong convolutional neural network cnn baselines recent non local model rely knn selection hand choose feature space',\n",
              "  'incremental gradient algorithm hybrid stochastic gradient descent hsgd enjoy merit stochastic full gradient methods finite sum minimization problem however exist rate convergence analysis hsgd make replacement sample wrs restrict convex problems clear whether hsgd still carry advantage common practice without replacement sample wors non convex problems paper affirmatively answer open question show wors convex non convex problems still possible hsgd constant step size match full gradient descent rate convergence maintain comparable sample size independent incremental first order oracle complexity stochastic gradient descent special class finite sum problems linear prediction model convergence result improve case extensive numerical result confirm theoretical affirmation demonstrate favorable efficiency wors base hsgd',\n",
              "  'nonlocal neural network propose show effective several computer vision task nonlocal operations directly capture long range dependencies feature space paper study nature diffusion damp effect nonlocal network spectrum analysis weight matrices well train network propose new formulation nonlocal block new block learn nonlocal interactions also stable dynamics thus allow deeper nonlocal structure moreover interpret formulation general nonlocal model perspective make connections propose nonlocal network nonlocal model nonlocal diffusion process markov jump process',\n",
              "  'many classic methods show non local self similarity natural image effective prior image restoration however remain unclear challenge make use intrinsic property via deep network paper propose non local recurrent network nlrn first attempt incorporate non local operations recurrent neural network rnn image restoration main contributions work unlike exist methods measure self similarity isolate manner propose non local module flexibly integrate exist deep network end end train capture deep feature correlation location neighborhood fully employ rnn structure parameter efficiency allow deep feature correlation propagate along adjacent recurrent state new design boost robustness inaccurate correlation estimation due severely degrade image show essential maintain confine neighborhood compute deep feature correlation give degrade image contrast exist practice deploy whole image extensive experiment image denoising super resolution task conduct thank recurrent non local operations correlation propagation propose nlrn achieve superior result state art methods many fewer parameters',\n",
              "  'problem estimate unknown discrete distribution sample fundamental tenet statistical learn past decade attract significant research effort solve variety divergence measure surprisingly equally important problem estimate unknown markov chain sample still far understand consider two problems relate min max risk expect loss estimate unknown state markov chain sequential sample predict conditional distribution next sample respect divergence estimate transition matrix respect natural loss induce general divergence measure',\n",
              "  'paper provide theoretical understand word embed dimensionality motivate unitary invariance word embed propose pairwise inner product pip loss novel metric dissimilarity word embeddings use techniques matrix perturbation theory reveal fundamental bias variance trade dimensionality selection word embeddings bias variance trade shed light many empirical observations previously unexplained example existence optimal dimensionality moreover new insights discoveries like word embeddings robust fit reveal optimize bias variance trade pip loss explicitly answer open question dimensionality selection word embed',\n",
              "  'consider problem maximize submodular function give access approximate version submodular function heavily study wide variety discipline since use model many real world phenomena amenable optimization however many case phenomena observe approximately submodular approximation guarantee cease hold describe technique call sample mean approximation yield strong guarantee maximization submodular function approximate surrogates cardinality intersection matroid constraints particular show tight guarantee maximization cardinality constraint approximation intersection matroids',\n",
              "  'image caption model become increasingly successful describe content image restrict domains however model function wild example assistants people impair vision much larger number variety visual concepts must understand address problem teach image caption model new visual concepts label image object detection datasets since image label object class interpret partial caption formulate problem learn partially specify sequence data propose novel algorithm train sequence model recurrent neural network partially specify sequence represent use finite state automata context image caption method lift restriction previously require image caption model train pair image sentence corpora otherwise require specialize model architectures take advantage alternative data modalities apply approach exist neural caption model achieve state art result novel object caption task use coco dataset show train caption model describe new visual concepts open image dataset maintain competitive coco evaluation score',\n",
              "  'increase need run convolutional neural network cnn model mobile devices limit compute power memory resource encourage study efficient model design number efficient architectures propose recent years example mobilenet shufflenet mobilenetv however model heavily dependent depthwise separable convolution lack efficient implementation deep learn frameworks study propose efficient architecture name peleenet build conventional convolution instead imagenet ilsvrc dataset propose peleenet achieve higher accuracy time faster speed mobilenet mobilenetv nvidia meanwhile peleenet model size mobilenet propose real time object detection system combine peleenet single shoot multibox detector ssd method optimize architecture fast speed propose detection system name pelee achieve map mean average precision pascal voc map coco dataset speed fps iphone fps nvidia result coco outperform yolov consideration higher precision time lower computational cost time smaller model size code model open source',\n",
              "  'present simple general framework feature learn point cloud key success cnns convolution operator capable leverage spatially local correlation data represent densely grids image however point cloud irregular unordered thus direct convolve kernels feature associate point result desert shape information variant order address problems propose learn transformation input point use simultaneously weight input feature associate point permute latent potentially canonical order element wise product sum operations typical convolution operator apply transform feature propose method generalization typical cnns learn feature point cloud thus call pointcnn experiment show pointcnn achieve par better performance state art methods multiple challenge benchmark datasets task',\n",
              "  'formulate private learn model study intrinsic tradeoff privacy query complexity sequential learn model involve learner aim determine scalar value sequentially query external database receive binary responses meantime adversary observe learner query though responses try infer value objective learner obtain accurate estimate use small number query simultaneously protect privacy make provably difficult learn adversary main result provide tight upper lower bound learner query complexity function desire level privacy estimation accuracy also construct explicit query strategies whose complexity optimal additive constant',\n",
              "  'computable stein discrepancies deploy variety applications range sampler selection posterior inference approximate bayesian inference goodness fit test exist convergence determine stein discrepancies admit strong theoretical guarantee suffer computational cost grow quadratically sample size linear time stein discrepancies propose goodness fit test exhibit avoidable degradations test power even power explicitly optimize address shortcomings introduce feature stein discrepancies sds new family quality measure cheaply approximate use importance sample show construct sds provably determine convergence sample target develop high accuracy approximations random sds sds computable near linear time experiment sampler selection approximate posterior inference goodness fit test sds perform well better quadratic time ksds order magnitude faster compute',\n",
              "  'normalization techniques play important role support efficient often effective train deep neural network conventional methods explicitly normalize activations suggest add loss term instead new loss term encourage variance activations stable vary one random mini batch next prove encourage activations distribute around distinct modes also show input mixture two gaussians new loss would either join two together separate optimally lda sense depend prior probabilities finally able link new regularization term batchnorm method provide regularization perspective experiment demonstrate improvement accuracy batchnorm technique cnns fully connect network',\n",
              "  'artificial intelligence model limit ability solve new task faster without forget previously acquire knowledge recently emerge paradigm continual learn aim solve issue model learn various task sequential fashion work novel approach continual learn propose search best neural architecture come task via sophisticatedly design reinforcement learn strategies name reinforce continual learn method good performance prevent catastrophic forget also fit new task well experiment sequential classification task variants mnist cifar datasets demonstrate propose approach outperform exist continual learn alternatives deep network',\n",
              "  'multiplicative noise include dropout widely use regularize deep neural network dnns show effective wide range architectures task information perspective consider inject multiplicative noise dnn train network solve task noisy information pathways lead observation multiplicative noise tend increase correlation feature increase signal noise ratio information pathways however high feature correlation undesirable increase redundancy representations work propose non correlate multiplicative noise ncmn exploit batch normalization remove correlation effect simple yet effective way show ncmn significantly improve performance standard multiplicative noise image classification task provide better alternative dropout batch normalize network additionally present unify view ncmn shake shake regularization explain performance gain latter',\n",
              "  'object detection performance measure canonical pascal voc dataset plateaued last years best perform methods complex ensemble systems typically combine multiple low level image feature high level context paper propose simple scalable detection algorithm improve mean average precision map relative previous best result voc achieve map approach combine two key insights one apply high capacity convolutional neural network cnns bottom region proposals order localize segment object label train data scarce supervise pre train auxiliary task follow domain specific fine tune yield significant performance boost since combine region proposals cnns call method cnn regions cnn feature also compare cnn overfeat recently propose slide window detector base similar cnn architecture find cnn outperform overfeat large margin class ilsvrc detection dataset source code complete system available http www berkeley edu rbg rcnn',\n",
              "  'singular value decomposition principal component analysis one widely use techniques dimensionality reduction successful efficiently computable nevertheless plague well know well document sensitivity outliers recent work consider set point arbitrarily corrupt components yet applications svd pca robust collaborative filter bioinformatics malicious agents defective genes simply corrupt contaminate experiment may effectively yield entire point completely corrupt present efficient convex optimization base algorithm call outlier pursuit mild assumptions uncorrupted point satisfy standard generative assumption pca problems recover exact optimal low dimensional subspace identify corrupt point identification corrupt point conform low dimensional approximation paramount interest bioinformatics financial applications beyond techniques involve matrix decomposition use nuclear norm minimization however result setup approach necessarily differ considerably exist line work matrix completion matrix decomposition since develop approach recover correct column space uncorrupted matrix rather exact matrix problem one seek recover structure rather exact initial matrices techniques develop thus far rely certificate optimality fail present important extension methods allow treatment problems',\n",
              "  'paper focus semantic scene completion task produce complete voxel representation volumetric occupancy semantic label scene single view depth map observation previous work consider scene completion semantic label depth map separately however observe two problems tightly intertwine leverage couple nature two task introduce semantic scene completion network sscnet end end convolutional network take single depth image input simultaneously output occupancy semantic label voxels camera view frustum network use dilation base context module efficiently expand receptive field enable context learn train network construct suncg manually create large scale dataset synthetic scenes dense volumetric annotations experiment demonstrate joint model outperform methods address task isolation outperform alternative approach semantic scene completion task',\n",
              "  'generalization performance central goal machine learn particularly learn representations large neural network common strategy improve generalization use regularizers typically norm constrain parameters regularize hide layer neural network architecture however straightforward effective layer wise suggestions without theoretical guarantee improve performance work theoretically empirically analyze one model call supervise auto encoder neural network predict input reconstruction error target jointly provide novel generalization result linear auto encoders prove uniform stability base inclusion reconstruction error particularly improvement simplistic regularization norms even advance regularizations use auxiliary task empirically demonstrate across array architectures different number hide units activation function supervise auto encoder compare correspond standard neural network never harm performance significantly improve generalization',\n",
              "  'beyond local convolution network explore harness various external human knowledge endow network capability semantic global reason rather use separate graphical model crf constraints model broader dependencies propose new symbolic graph reason sgr layer perform reason group symbolic nod whose output explicitly represent different properties semantic prior knowledge graph cooperate local convolutions sgr constitute three modules primal local semantic vote module feature symbolic nod generate vote local representations graph reason module propagate information knowledge graph achieve global semantic coherency dual semantic local map module learn new associations evolve symbolic nod local representations accordingly enhance local feature sgr layer inject convolution layer instantiate distinct prior graph extensive experiment show incorporate sgr significantly improve plain convnets three semantic segmentation task one image classification task analyse show sgr layer learn share symbolic representations domains datasets different label set give universal knowledge graph demonstrate superior generalization capability',\n",
              "  'ability transfer reinforcement learn key towards build agent general artificial intelligence paper consider problem learn simultaneously transfer across environments task probably importantly learn sparse environment task pair possible combinations propose novel compositional neural network architecture depict meta rule compose policies environment task embeddings notably one main challenge learn embeddings jointly meta rule propose new train methods disentangle embeddings make distinctive signatures environments task effective build block compose policies experiment gridworld thor agent take input egocentric view show approach give rise high success rat environment task pair learn',\n",
              "  'shoot learn become essential produce model generalize examples work identify metric scale metric task condition important improve performance shoot algorithms analysis reveal simple metric scale completely change nature shoot algorithm parameter update metric scale provide improvements accuracy certain metrics mini imagenet way shoot classification task propose simple effective way condition learner task sample set result learn task dependent metric space moreover propose empirically test practical end end optimization procedure base auxiliary task train learn task dependent metric space result shoot learn model base task dependent scale metric achieve state art mini imagenet confirm result another shoot dataset introduce paper base cifar',\n",
              "  'deep learn model often parameters observations still perform well sometimes describe paradox work show experimentally despite huge number parameters deep neural network compress data losslessly even take cost encode parameters account compression viewpoint originally motivate use variational methods neural network however show variational methods provide surprisingly poor compression bound despite explicitly build minimize bound might explain relatively poor practical performance variational methods deep learn better encode methods import minimum description length mdl toolbox yield much better compression value deep network',\n",
              "  'real world learn systems practical limitations quality quantity train datasets collect consider system choose subset possible train examples still allow learn accurate generalizable model help address question draw inspiration highly efficient practical learn system human child use head mount cameras eye gaze trackers model foveated vision collect first person egocentric image represent highly accurate approximation train data toddlers visual systems collect everyday naturalistic learn contexts use state art computer vision learn model convolutional neural network help characterize structure data find child data produce significantly better object model egocentric data experience adults exactly environment use cnns model tool investigate properties child data may enable rapid learn find child data exhibit unique combination quality diversity many similar large high quality object view also greater number diversity rare view novel methodology analyze visual train data use children may reveal insights improve machine learn also may suggest new experimental tool better understand infant learn developmental psychology',\n",
              "  'many image image translation problems ambiguous single input image may correspond multiple possible output work aim model emph distribution possible output conditional generative model set ambiguity map distil low dimensional latent vector randomly sample test time generator learn map give input combine latent code output explicitly encourage connection output latent code invertible help prevent many one map latent code output train also know problem mode collapse produce diverse result explore several variants approach employ different train objectives network architectures methods inject latent code propose method encourage bijective consistency latent encode output modes present systematic comparison method variants perceptual realism diversity',\n",
              "  'leverage temporal dimension key question video analysis recent work suggest efficient approach video feature learn factorize convolutions separate components respectively spatial temporal convolutions temporal convolution however come implicit assumption feature map across time step well align feature locations aggregate assumption may overly strong practical applications especially action recognition motion serve crucial cue work propose new cnn architecture trajectorynet incorporate trajectory convolution new operation integrate feature along temporal dimension replace exist temporal convolution operation explicitly take account change content cause deformation motion allow visual feature aggregate along motion paths trajectories two large scale action recognition datasets namely something something kinetics propose network architecture achieve notable improvement strong baselines',\n",
              "  'program translation important tool migrate legacy code one language ecosystem build different language work first employ deep neural network toward tackle problem observe program translation modular procedure sub tree source tree translate correspond target sub tree step capture intuition design tree tree neural network translate source tree target one meanwhile develop attention mechanism tree tree model decoder expand one non terminal target tree attention mechanism locate correspond sub tree source tree guide expansion decoder evaluate program translation capability tree tree model several state art approach compare neural translation model observe approach consistently better baselines margin point approach improve previous state art program translation approach margin point translation real world project',\n",
              "  'recent success human action recognition deep learn methods mostly adopt supervise learn paradigm require significant amount manually label data achieve good performance however label collection expensive time consume process work propose unsupervised learn framework exploit unlabeled data learn video representations different previous work video representation learn unsupervised learn task predict motion multiple target view use video representation source view learn extrapolate cross view motion representation capture view invariant motion dynamics discriminative action addition propose view adversarial train method enhance learn view invariant feature demonstrate effectiveness learn representations action recognition multiple datasets',\n",
              "  'one core problems modern statistics approximate difficult compute probability densities problem especially important bayesian statistics frame inference unknown quantities calculation involve posterior density paper review variational inference method machine learn approximate probability densities optimization use many applications tend faster classical methods markov chain monte carlo sample idea behind first posit family densities find member family close target closeness measure kullback leibler divergence review ideas behind mean field variational inference discuss special case apply exponential family model present full example bayesian mixture gaussians derive variant use stochastic optimization scale massive data discuss modern research highlight important open problems powerful yet well understand hope write paper catalyze statistical research class algorithms',\n",
              "  'introduce variability maintain coherence core task learn generate utterances conversation standard neural encoder decoder model extensions use conditional variational autoencoder often result either trivial digressive responses overcome explore novel approach inject variability neural encoder decoder via use external memory mixture model namely variational memory encoder decoder vmed associate memory read mode latent mixture distribution timestep model capture variability observe sequential data natural conversations empirically compare propose model recent approach various conversational datasets result show vmed consistently achieve significant improvement others metric base qualitative evaluations'],\n",
              " ['focus machine learn branch beyond train classifiers single task investigate previously acquire knowledge source domain leverage facilitate learn relate target domain know inductive transfer learn three active line research independently explore transfer learn use neural network weight transfer model train source domain use initialization point network train target domain deep metric learn source domain use construct embed capture class structure source target domains shoot learn focus generalize well target domain base limit number label examples compare state art methods three paradigms also explore hybrid adapt embed methods use limit target domain data fine tune embeddings construct source domain data conduct systematic comparison methods variety domains vary number label instance available target domain well number target domain class reach three principal conclusions deep embeddings far superior compare weight transfer start point inter domain transfer model use hybrid methods robustly outperform every shoot learn every deep metric learn method previously propose mean error reduction state art among loss function discover embeddings histogram loss ustinova lempitsky robust hope result motivate unification research weight transfer deep metric learn shoot learn',\n",
              "  'dominant sequence transduction model base complex recurrent convolutional neural network encoder decoder configuration best perform model also connect encoder decoder attention mechanism propose new simple network architecture transformer base solely attention mechanisms dispense recurrence convolutions entirely experiment two machine translation task show model superior quality parallelizable require significantly less time train model achieve bleu wmt english german translation task improve exist best result include ensembles bleu wmt english french translation task model establish new single model state art bleu score train days eight gpus small fraction train cost best model literature show transformer generalize well task apply successfully english constituency parse large limit train data',\n",
              "  'adversarial learn embed deep network learn disentangle transferable representations domain adaptation exist adversarial domain adaptation methods may struggle align different domains multimodal distributions native classification problems paper present conditional adversarial domain adaptation principled framework condition adversarial adaptation model discriminative information convey classifier predictions conditional domain adversarial network cdans design two novel condition strategies multilinear condition capture cross covariance feature representations classifier predictions improve discriminability entropy condition control uncertainty classifier predictions guarantee transferability experiment testify propose approach exceed state art result five benchmark datasets',\n",
              "  'replace output layer deep neural net typically softmax function novel interpolate function propose end end train test algorithms new architecture compare classical neural net softmax function output activation surrogate interpolate function output activation combine advantage deep manifold learn new framework demonstrate follow major advantage first better applicable case insufficient train data second significantly improve generalization accuracy wide variety network algorithm implement pytorch code available https github com baowangmath dnn datadependentactivation',\n",
              "  'boltzmann machine powerful distributions show effective prior binary latent variables variational autoencoders vaes however previous methods train discrete vaes use evidence lower bind tighter importance weight bind propose two approach relax boltzmann machine continuous distributions permit train importance weight bound relaxations base generalize overlap transformations gaussian integral trick experiment mnist omniglot datasets show relaxations outperform previous discrete vaes boltzmann priors implementation reproduce result available',\n",
              "  'introduce generative neural machine translation gnmt latent variable architecture design model semantics source target sentence modify encoder decoder translation model add latent variable language agnostic representation encourage learn mean sentence gnmt achieve competitive bleu score pure translation task superior miss word source sentence augment model facilitate multilingual translation semi supervise learn without add parameters framework significantly reduce overfitting limit pair data available effective translate pair languages see train',\n",
              "  'batch normalization milestone technique development deep learn enable various network train however normalize along batch dimension introduce problems error increase rapidly batch size become smaller cause inaccurate batch statistics estimation limit usage train larger model transfer feature computer vision task include detection segmentation video require small batch constrain memory consumption paper present group normalization simple alternative divide channel group compute within group mean variance normalization computation independent batch size accuracy stable wide range batch size resnet train imagenet lower error counterpart use batch size use typical batch size comparably good outperform normalization variants moreover naturally transfer pre train fine tune outperform base counterparts object detection segmentation coco video classification kinetics show effectively replace powerful variety task easily implement line code modern libraries',\n",
              "  'deep image translation methods recently show excellent result output high quality image cover multiple modes data distribution also increase interest disentangle internal representations learn deep methods improve performance achieve finer control paper bridge two objectives introduce concept cross domain disentanglement aim separate internal representation three part share part contain information domains exclusive part hand contain factor variation particular domain achieve bidirectional image translation base generative adversarial network cross domain autoencoders novel network component model offer multiple advantage output diverse sample cover multiple modes distributions domains perform domain specific image transfer interpolation cross domain retrieval without need label data pair image compare model state art multi modal image translation achieve better result translation challenge datasets well cross domain retrieval realistic datasets',\n",
              "  'work aim solve large collection task use single reinforcement learn agent single set parameters key challenge handle increase amount data extend train time develop new distribute agent impala importance weight actor learner architecture use resources efficiently single machine train also scale thousands machine without sacrifice data efficiency resource utilisation achieve stable learn high throughput combine decouple act learn novel policy correction method call trace demonstrate effectiveness impala multi task reinforcement learn dmlab set task deepmind lab environment beattie atari available atari game arcade learn environment bellemare result show impala able achieve better performance previous agents less data crucially exhibit positive transfer task result multi task approach',\n",
              "  'configure deep spike neural network snns excite research avenue low power spike event base computation however spike generation function non differentiable therefore directly compatible standard error backpropagation algorithm paper introduce new general backpropagation mechanism learn synaptic weight axonal delay overcome problem non differentiability spike function use temporal credit assignment policy backpropagating error precede layer describe release gpu accelerate software implementation method allow train fully connect convolutional neural network cnn architectures use software compare method exist snn base learn approach standard ann snn conversion techniques show method achieve state art performance snn mnist nmnist dvs gesture tidigits datasets',\n",
              "  'uncertainty sample popular active learn algorithm use reduce amount data require learn classifier observe practice converge different parameters depend initialization sometimes even better parameters standard train data work give theoretical explanation phenomenon show uncertainty sample convex logistic loss interpret perform precondition stochastic gradient step population zero one loss experiment synthetic real datasets support connection',\n",
              "  'adversarial learn base video prediction methods suffer image blur since commonly use adversarial regression loss pair work rather competitive way collaboration yield compromise blur effect meantime often rely single pass architecture predictor inadequate explicitly capture forthcoming uncertainty work involve two key insights video prediction approach stochastic process sample collection proposals conform possible frame distribution follow time stamp one select final prediction couple combine loss function dedicatedly design sub network encourage work collaborative way combine two insights propose two stage network call vpss textbf ideo textbf rediction via textbf elective textbf ampling specifically emph sample module produce collection high quality proposals facilitate multiple choice adversarial learn scheme yield diverse frame proposal set subsequently emph selection module select high possibility candidates proposals combine produce final prediction extensive experiment diverse challenge datasets demonstrate effectiveness propose video prediction approach yield diverse proposals accurate prediction result'],\n",
              " ['parallel implementations stochastic gradient descent sgd receive significant research attention thank excellent scalability properties algorithm efficiency context train deep neural network fundamental barrier parallelize large scale sgd fact cost communicate gradient update nod large consequently lossy compression heuristics propose nod communicate quantize gradients although effective practice heuristics always provably converge clear whether optimal paper propose quantize sgd qsgd family compression scheme allow compression gradient update node guarantee convergence standard assumptions qsgd allow user trade compression convergence time communicate sublinear number bits per iteration model dimension achieve asymptotically optimal communication cost complement theoretical result empirical data show qsgd significantly reduce communication cost competitive standard uncompress techniques variety real task particular experiment show gradient quantization apply train deep neural network image classification automate speech recognition lead significant reductions communication cost end end train time instance gpus able train resnet network imagenet faster full accuracy note show exist generic parameter settings know network architectures preserve slightly improve full accuracy use quantization',\n",
              "  'spatio temporal action detection videos typically address fully supervise setup manual annotation train videos require every frame since annotation extremely tedious prohibit scalability clear need minimize amount manual supervision work propose unify framework handle combine vary type less demand weak supervision model base discriminative cluster integrate different type supervision constraints optimization investigate applications model train setups alternative supervisory signal range video level class label temporal point sparse action bound box full per frame annotation action bound box experiment challenge ucf daly datasets demonstrate competitive performance method fraction supervision use previous methods flexibility model enable joint learn data different level annotation experimental result demonstrate significant gain add fully supervise examples otherwise weakly label videos',\n",
              "  'give sample probability distribution anomaly detection problem determine give point lie low density region paper concern calibrate anomaly detection practically relevant extension additionally wish produce confidence score point anomalous build classification framework anomaly detection show minimisation suitably modify proper loss produce density estimate anomalous instance show incorporate quantile control relate objective generalise version pinball loss finally show efficiently optimise objective kernelised scorer leverage recent result point process literature result objective capture close relative one class svm special case',\n",
              "  'relational reason central component generally intelligent behavior prove difficult neural network learn paper describe use relation network rns simple plug play module solve problems fundamentally hinge relational reason test augment network three task visual question answer use challenge dataset call clevr achieve state art super human performance text base question answer use babi suite task complex reason dynamic physical systems use curated dataset call sort clevr show powerful convolutional network general capacity solve relational question gain capacity augment rns work show deep learn architecture equip module implicitly discover learn reason entities relations',\n",
              "  'consider problem generate automatic code give sample input output pair train neural network map current state output program next statement neural network optimize multiple task concurrently next operation set high level command operands next statement variables drop memory use method able create program twice long exist state art solutions improve success rate comparable lengths cut run time two order magnitude code include implementation various literature baselines publicly available https github com amitz pccoder',\n",
              "  'introduce temper geodesic markov chain monte carlo mcmc algorithm initialize pose graph optimization problems arise various scenarios sfm structure motion slam simultaneous localization map mcmc first kind unite global non convex optimization spherical manifold quaternions posterior sample order provide reliable initial pose uncertainty estimate informative quality solutions devise theoretical convergence guarantee extensively evaluate method synthetic real benchmarks besides elegance formulation theory show method robust miss data noise estimate uncertainties capture intuitive properties data',\n",
              "  'stochastic gradient method minimize objective function compose large number differentiable function solve stochastic optimization problem moderate accuracy block coordinate descent update bcd method hand handle problems multiple block variables update one time block variables easier update individually together bcd lower per iteration cost paper introduce method combine feature bcd problems many components objective multiple block variables specifically block stochastic gradient bsg method propose solve convex nonconvex program iteration bsg approximate gradient differentiable part objective randomly sample small set data sample function sum term objective use sample update block variables either deterministic randomly shuffle order convergence convex nonconvex case establish different sense convex case propose method order convergence rate method nonconvex case convergence establish term expect violation first order optimality condition propose method numerically test problems include stochastic least square logistic regression convex well low rank tensor recovery bilinear logistic regression nonconvex',\n",
              "  'top visual attention mechanisms use extensively image caption visual question answer vqa enable deeper image understand fine grain analysis even multiple step reason work propose combine bottom top attention mechanism enable attention calculate level object salient image regions natural basis attention consider within approach bottom mechanism base faster cnn propose image regions associate feature vector top mechanism determine feature weight apply approach image caption result mscoco test server establish new state art task achieve cider spice bleu score respectively demonstrate broad applicability method apply approach vqa obtain first place vqa challenge',\n",
              "  'paper address mode collapse generative adversarial network gans view modes geometric structure data distribution metric space geometric lens embed subsamples dataset arbitrary metric space space preserve pairwise distance distribution metric embed determine dimensionality latent space automatically also enable construct mixture gaussians draw latent space random vectors use gaussian mixture model tandem simple augmentation objective function train gans every major step method support theoretical analysis experiment real synthetic data confirm generator able produce sample spread modes avoid unwanted sample outperform several recent gin variants number metrics offer new feature',\n",
              "  'connectionist temporal classification ctc objective function end end sequence learn adopt dynamic program algorithms directly learn map sequence ctc show promise result many sequence learn applications include speech recognition scene text recognition however ctc tend produce highly peaky overconfident distributions symptom overfitting remedy propose regularization method base maximum conditional entropy penalize peaky distributions encourage exploration also introduce entropy base prune method dramatically reduce number ctc feasible paths rule unreasonable alignments experiment scene text recognition show propose methods consistently improve ctc baseline without need adjust train settings code make publicly available https github com liuhu bigeye enctc crnn',\n",
              "  'present unsupervised visual feature learn algorithm drive context base pixel prediction analogy auto encoders propose context encoders convolutional neural network train generate content arbitrary image region condition surround order succeed task context encoders need understand content entire image well produce plausible hypothesis miss part train context encoders experiment standard pixel wise reconstruction loss well reconstruction plus adversarial loss latter produce much sharper result better handle multiple modes output find context encoder learn representation capture appearance also semantics visual structure quantitatively demonstrate effectiveness learn feature cnn pre train classification detection segmentation task furthermore context encoders use semantic inpainting task either stand alone initialization non parametric methods',\n",
              "  'despite efficacy variety computer vision task deep neural network dnns vulnerable adversarial attack limit applications security critical systems recent work show possibility generate imperceptibly perturb image input adversarial examples fool well train dnn classifiers make arbitrary predictions address problem propose train recipe name deep defense core idea integrate adversarial perturbation base regularizer classification objective obtain model learn resist potential attack directly precisely whole optimization problem solve like train recursive network experimental result demonstrate method outperform train adversarial parseval regularizations large margins various datasets include mnist cifar imagenet different dnn architectures code model reproduce result available https github com ziangyan deepdefense pytorch',\n",
              "  'various semantic attribute segmentation mask geometric feature keypoints materials encode per point probe function geometries give collection relate shape consider jointly analyze probe function different shape discover common latent structure use neural network even absence correspondence information network train point cloud representations shape geometry associate semantic function point cloud function express share semantic understand shape coordinate way example segmentation task function indicator function arbitrary set shape part particular combination involve know network network able produce small dictionary basis function shape dictionary whose span include semantic function provide shape even though shape independent discretizations functional correspondences provide network able generate latent base consistent order reflect share semantic structure among shape demonstrate effectiveness technique various segmentation keypoint selection applications',\n",
              "  'deeper neural network difficult train present residual learn framework ease train network substantially deeper use previously explicitly reformulate layer learn residual function reference layer input instead learn unreferenced function provide comprehensive empirical evidence show residual network easier optimize gain accuracy considerably increase depth imagenet dataset evaluate residual net depth layer deeper vgg net still lower complexity ensemble residual net achieve error imagenet test set result place ilsvrc classification task also present analysis cifar layer depth representations central importance many visual recognition task solely due extremely deep representations obtain relative improvement coco object detection dataset deep residual net foundations submissions ilsvrc coco competitions also place task imagenet detection imagenet localization coco detection coco segmentation',\n",
              "  'rapid growth image video data web hash extensively study image video search recent years benefit recent advance deep learn deep hash methods achieve promise result image retrieval however limitations previous deep hash methods semantic information fully exploit paper develop deep supervise discrete hash algorithm base assumption learn binary cod ideal classification pairwise label information classification information use learn hash cod within one stream framework constrain output last layer binary cod directly rarely investigate deep hash algorithm discrete nature hash cod alternate minimization method use optimize objective function experimental result show method outperform current state art methods benchmark datasets',\n",
              "  'recent work show convolutional network substantially deeper accurate efficient train contain shorter connections layer close input close output paper embrace observation introduce dense convolutional network densenet connect layer every layer fee forward fashion whereas traditional convolutional network layer connections one layer subsequent layer network direct connections layer feature map precede layer use input feature map use input subsequent layer densenets several compel advantage alleviate vanish gradient problem strengthen feature propagation encourage feature reuse substantially reduce number parameters evaluate propose architecture four highly competitive object recognition benchmark task cifar cifar svhn imagenet densenets obtain significant improvements state art whilst require less computation achieve high performance code pre train model available https github com liuzhuang densenet',\n",
              "  'paper present keypointnet end end geometric reason framework learn optimal set category specific keypoints along detectors predict keypoints single input image demonstrate framework pose estimation task propose differentiable pose objective seek optimal set keypoints recover relative pose two view object network automatically discover consistent set keypoints across viewpoints single object well across object instance give object class importantly find end end approach use grind truth keypoint annotations outperform fully supervise baseline use neural network architecture pose estimation task discover keypoints across car chair plane categories shapenet visualize https keypoints github',\n",
              "  'propose novel wasserstein method distillation mechanism yield joint learn word embeddings topics propose method base fact euclidean distance word embeddings may employ underlie distance wasserstein topic model word distributions topics optimal transport word distributions document embeddings word learn unify framework learn topic model leverage distil grind distance matrix update topic distributions smoothly calculate correspond optimal transport strategy provide update word embeddings robust guidance improve algorithm convergence application focus patient admission record propose method embed cod diseases procedures learn topics admissions obtain superior performance clinically meaningful disease network construction mortality prediction function admission cod procedure recommendation',\n",
              "  'propose dropmax stochastic version softmax classifier iteration drop non target class accord dropout probabilities adaptively decide instance specifically overlay binary mask variables class output probabilities input adaptively learn via variational inference stochastic regularization effect build ensemble classifier exponentially many classifiers different decision boundaries moreover learn dropout rat non target class instance allow classifier focus classification confuse class validate model multiple public datasets classification obtain significantly improve accuracy regular softmax classifier baselines analysis learn dropout probabilities show model indeed select confuse class often perform classification',\n",
              "  'consider minimization submodular function subject order constraints show potentially non convex optimization problem cast convex optimization problem space uni dimensional measure order constraints correspond first order stochastic dominance propose new discretization scheme lead simple efficient algorithms base zero first higher order oracles algorithms also lead improvements without isotonic constraints finally experiment show non convex loss function much robust outliers isotonic regression still solvable polynomial time',\n",
              "  'stochastic gradient hard thresholding methods recently show work favorably solve large scale empirical risk minimization problems sparsity rank constraint despite improve iteration complexity full gradient methods gradient evaluation hard thresholding complexity exist stochastic algorithms usually scale linearly data size could still expensive data huge hard thresholding step could expensive singular value decomposition rank constrain problems address deficiencies propose efficient hybrid stochastic gradient hard thresholding hsg method provably show sample size independent gradient evaluation hard thresholding complexity bound specifically prove stochastic gradient evaluation complexity hsg scale linearly inverse sub optimality hard thresholding complexity scale logarithmically apply heavy ball acceleration technique propose accelerate variant hsg show improve factor dependence restrict condition number numerical result confirm theoretical affirmation demonstrate computational efficiency propose methods',\n",
              "  'learn low dimensional embeddings knowledge graph powerful approach use predict unobserved miss edge entities however open challenge area develop techniques beyond simple edge prediction handle complex logical query might involve multiple unobserved edge entities variables instance give incomplete biological knowledge graph might want predict drug likely target proteins involve diseases query require reason possible proteins might interact diseases introduce framework efficiently make predictions conjunctive logical query flexible tractable subset first order logic incomplete knowledge graph approach embed graph nod low dimensional space represent logical operators learn geometric operations translation rotation embed space perform logical operations within low dimensional embed space approach achieve time complexity linear number query variables compare exponential complexity require naive enumeration base approach demonstrate utility framework two application study real world datasets millions relations predict logical relationships network drug gene disease interactions graph base representation social interactions derive popular web forum',\n",
              "  'deep reinforcement learn drl algorithms successfully apply range challenge control task however methods typically suffer three core difficulties temporal credit assignment sparse reward lack effective exploration brittle convergence properties extremely sensitive hyperparameters collectively challenge severely limit applicability approach real world problems evolutionary algorithms eas class black box optimization techniques inspire natural evolution well suit address three challenge however eas typically suffer high sample complexity struggle solve problems require optimization large number parameters paper introduce evolutionary reinforcement learn erl hybrid algorithm leverage population provide diversify data train agent reinserts agent population periodically inject gradient information erl inherit ability temporal credit assignment fitness metric effective exploration diverse set policies stability population base approach complement policy drl ability leverage gradients higher sample efficiency faster learn experiment range challenge continuous control benchmarks demonstrate erl significantly outperform prior drl methods',\n",
              "  'paper propose novel method provide contrastive explanations justify classification input black box classifier deep neural network give input find minimally sufficiently present viz important object pixels image justify classification analogously minimally necessarily emph absent viz certain background pixels argue explanations natural humans use commonly domains health care criminology minimally critically emph absent important part explanation best knowledge explicitly identify current explanation methods explain predictions neural network validate approach three real datasets obtain diverse domains namely handwritten digits dataset mnist large procurement fraud dataset brain activity strength dataset three case witness power approach generate precise explanations also easy human experts understand evaluate',\n",
              "  'basic principles design convolutional neural network cnn structure predict object different level image level region level pixel level diverge generally network structure design specifically image classification directly use default backbone structure task include detection segmentation seldom backbone structure design consideration unify advantage network design pixel level region level predict task may require deep feature high resolution towards goal design fish like network call fishnet fishnet information resolutions preserve refine final task besides observe exist work still emph directly propagate gradient information deep layer shallow layer design better handle problem extensive experiment conduct demonstrate remarkable performance fishnet particular imagenet accuracy fishnet able surpass performance densenet resnet fewer parameters fishnet apply one modules win entry coco detection challenge code available https github com kevin ssy fishnet',\n",
              "  'paper present novel framework video image segmentation localization cast single optimization problem integrate information low level appearance cue high level localization cue weakly supervise manner propose framework leverage two representations different level exploit spatial relationship bound box superpixels linear constraints simultaneously discriminate foreground background bound box superpixel level different previous approach mainly rely discriminative cluster incorporate foreground model minimize histogram difference object across image frame exploit geometric relation superpixels bound box enable transfer segmentation cue improve localization output vice versa inclusion foreground model generalize discriminative framework video data background tend similar thus discriminative demonstrate effectiveness unify framework youtube object video dataset internet object discovery dataset pascal voc',\n",
              "  'continuous word representation aka word embed basic build block many neural network base model use natural language process task although widely accept word similar semantics close embed space find word embeddings learn several task bias towards word frequency embeddings high frequency low frequency word lie different subregions embed space embed rare word popular word far even semantically similar make learn word embeddings ineffective especially rare word consequently limit performance neural network model order mitigate issue paper propose neat simple yet effective adversarial train method blur boundary embeddings high frequency word low frequency word conduct comprehensive study ten datasets across four natural language process task include word similarity language model machine translation text classification result show achieve higher performance baselines task',\n",
              "  'lot learn task require deal graph data contain rich relation information among elements model physics systems learn molecular fingerprint predict protein interface classify diseases demand model learn graph input domains learn non structural data like texts image reason extract structure like dependency tree sentence scene graph image important research topic also need graph reason model graph neural network gnns neural model capture dependence graph via message pass nod graph recent years variants gnns graph convolutional network gcn graph attention network gat graph recurrent network grn demonstrate grind break performances many deep learn task survey propose general design pipeline gnn model discuss variants component systematically categorize applications propose four open problems future research',\n",
              "  'paper propose generative multi column network image inpainting network synthesize different image components parallel manner within one stage better characterize global structure design confidence drive reconstruction loss implicit diversify mrf regularization adopt enhance local detail multi column network combine reconstruction mrf loss propagate local global information derive context target inpainting regions extensive experiment challenge street view face natural object scenes manifest method produce visual compel result even without previously common post process',\n",
              "  'paper describe infogan information theoretic extension generative adversarial network able learn disentangle representations completely unsupervised manner infogan generative adversarial network also maximize mutual information small subset latent variables observation derive lower bind mutual information objective optimize efficiently show train procedure interpret variation wake sleep algorithm specifically infogan successfully disentangle write style digit shape mnist dataset pose light render image background digits central digit svhn dataset also discover visual concepts include hair style presence absence eyeglasses emotions celeba face dataset experiment show infogan learn interpretable representations competitive representations learn exist fully supervise methods',\n",
              "  'two semimetrics probability distributions propose give sum differences expectations analytic function evaluate spatial frequency locations feature feature choose maximize distinguishability distributions optimize lower bind test power statistical test use feature result parsimonious interpretable indication two distributions differ locally empirical estimate test power criterion converge increase sample size ensure quality return feature real world benchmarks high dimensional text image data linear time test use propose semimetrics achieve comparable performance state art quadratic time maximum mean discrepancy test return human interpretable feature explain test result',\n",
              "  'decompose evidence lower bind show existence term measure total correlation latent variables use motivate beta tcvae total correlation variational autoencoder algorithm refinement plug replacement beta vae learn disentangle representations require additional hyperparameters train propose principled classifier free measure disentanglement call mutual information gap mig perform extensive quantitative qualitative experiment restrict non restrict settings show strong relation total correlation disentanglement model train use framework',\n",
              "  'suggest new loss learn deep embeddings key characteristics new loss absence tunable parameters good result obtain across range datasets problems loss compute estimate two distribution similarities positive match negative non match point pair compute probability positive pair lower similarity score negative pair base probability estimate show operations perform simple piecewise differentiable manner use histograms soft assignment operations make propose loss suitable learn deep embeddings use stochastic optimization experiment reveal favourable result compare recently propose loss function',\n",
              "  'progress deep learn spawn great successes many engineer applications prime example convolutional neural network type feedforward neural network approach sometimes even surpass human accuracy variety visual recognition task however show neural network recent extensions struggle recognition task dependent visual feature must detect long spatial range introduce visual challenge pathfinder describe novel recurrent neural network architecture call horizontal gate recurrent unit hgru learn intrinsic horizontal connections within across feature columns demonstrate single hgru layer match outperform test feedforward hierarchical baselines include state art architectures order magnitude parameters',\n",
              "  'machine learn become widely use practice need new methods build complex intelligent systems integrate learn exist software domain knowledge encode rule case study present system learn parse newtonian physics problems textbooks system nut bolt learn pipeline process incorporate exist code pre learn machine learn model human engineer rule jointly train entire pipeline prevent propagation errors use combination label unlabelled data approach achieve good performance parse task outperform simple pipeline variants finally also show nut bolt use achieve improvements relation extraction task end task answer newtonian physics problems',\n",
              "  'navigate unstructured environments basic capability intelligent creatures thus fundamental interest study development artificial intelligence long range navigation complex cognitive task rely develop internal representation space ground recognisable landmarks robust visual process simultaneously support continuous self localisation representation goal go build upon recent research apply deep reinforcement learn maze navigation problems present end end deep reinforcement learn approach apply city scale recognise successful navigation rely integration general policies locale specific knowledge propose dual pathway architecture allow locale specific feature encapsulate still enable transfer multiple cities key contribution paper interactive navigation environment use google street view photographic content worldwide coverage baselines demonstrate deep reinforcement learn agents learn navigate multiple cities traverse target destinations may kilometres away video summarize research show train agent diverse city environments well transfer task available https sit google com view learn navigate cities nip',\n",
              "  'fine grain visual classification fgvc important computer vision problem involve small diversity within different class often require expert annotators collect data utilize notion small visual diversity revisit maximum entropy learn context fine grain classification provide train routine maximize entropy output probability distribution train convolutional neural network fgvc task provide theoretical well empirical justification approach achieve state art performance across variety classification task fgvc potentially extend fine tune task method robust different hyperparameter value amount train data amount train label noise hence valuable tool many similar problems',\n",
              "  'paper address general problem blind echo retrieval give sensors measure discrete time domain mixtures delay attenuate copy unknown source signal echo location weight recover problem broad applications field sonars seismology ultrasounds room acoustics belong broader class blind channel identification problems intensively study signal process exist methods proceed two step blind estimation sparse discrete time filter echo information retrieval peak pick precision methods fundamentally limit rate signal sample estimate echo locations necessary grid since true locations never match sample grid weight estimation precision also strongly limit call basis mismatch problem compress sense propose radically different approach problem build top framework finite rate innovation sample approach operate directly parameter space echo locations weight enable near exact blind grid echo retrieval discrete time measurements show outperform conventional methods several order magnitudes precision',\n",
              "  'paper study generalization performance multi class classification obtain shaper data dependent generalization error bind fast convergence rate substantially improve state art bound exist data dependent generalization analysis theoretical analysis motivate devise two effective multi class kernel learn algorithms statistical guarantee experimental result show propose methods significantly outperform exist multi class classification methods',\n",
              "  'multi task learn multiple task solve jointly share inductive bias multi task learn inherently multi objective problem different task may conflict necessitate trade common compromise optimize proxy objective minimize weight linear combination per task losses however workaround valid task compete rarely case paper explicitly cast multi task learn multi objective optimization overall objective find pareto optimal solution end use algorithms develop gradient base multi objective optimization literature algorithms directly applicable large scale learn problems since scale poorly dimensionality gradients number task therefore propose upper bind multi objective loss show optimize efficiently prove optimize upper bind yield pareto optimal solution realistic assumptions apply method variety multi task deep learn problems include digit classification scene understand joint semantic segmentation instance segmentation depth estimation multi label classification method produce higher perform model recent multi task learn formulations per task train',\n",
              "  'numerous deep learn applications benefit multi task learn multiple regression classification objectives paper make observation performance systems strongly dependent relative weight task loss tune weight hand difficult expensive process make multi task learn prohibitive practice propose principled approach multi task deep learn weigh multiple loss function consider homoscedastic uncertainty task allow simultaneously learn various quantities different units scale classification regression settings demonstrate model learn per pixel depth regression semantic instance segmentation monocular input image perhaps surprisingly show model learn multi task weight outperform separate model train individually task',\n",
              "  'extend capabilities neural network couple external memory resources interact attentional process combine system analogous turing machine von neumann architecture differentiable end end allow efficiently train gradient descent preliminary result demonstrate neural turing machine infer simple algorithms copy sort associative recall input output examples',\n",
              "  'marry two powerful ideas deep representation learn visual recognition language understand symbolic program execution reason neural symbolic visual question answer vqa system first recover structural scene representation image program trace question execute program scene representation obtain answer incorporate symbolic structure prior knowledge offer three unique advantage first execute program symbolic space robust long program trace model solve complex reason task better achieve accuracy clevr dataset second model data memory efficient perform well learn small number train data also encode image compact representation require less storage exist methods offline question answer third symbolic program execution offer full transparency reason process thus able interpret diagnose execution step',\n",
              "  'study computational tractability pac reinforcement learn rich observations present new provably sample efficient algorithms environments deterministic hide state dynamics stochastic rich observations methods operate oracle model computation access policy value function class exclusively standard optimization primitives therefore represent computationally efficient alternatives prior algorithms require enumeration stochastic hide state dynamics prove know sample efficient algorithm olive implement oracle model also present several examples illustrate fundamental challenge tractable pac reinforcement learn general settings',\n",
              "  'propose study new model reinforcement learn rich observations generalize contextual bandits sequential decision make model require agent take action base observations feature goal achieve long term performance competitive large set policies avoid barriers sample efficient learn associate large observation space general pomdps focus problems summarize small number hide state long term reward predictable reactive function class set design analyze new reinforcement learn algorithm least square value elimination exploration prove algorithm learn near optimal behavior number episodes polynomial relevant parameters logarithmic number policies independent size observation space result provide theoretical justification reinforcement learn function approximation',\n",
              "  'existence evasion attack test phase machine learn algorithms represent significant challenge deployment understand attack carry add imperceptible perturbations input generate adversarial examples find effective defenses detectors prove difficult paper step away attack defense arm race seek understand limit learn presence evasion adversary particular extend probably approximately correct pac learn framework account presence adversary first define corrupt hypothesis class arise standard binary hypothesis class presence evasion adversary derive vapnik chervonenkis dimension denote adversarial dimension show sample complexity upper bound fundamental theorem statistical learn extend case evasion adversaries sample complexity control adversarial dimension explicitly derive adversarial dimension halfspace classifiers presence sample wise norm constrain adversary type commonly study evasion attack show standard dimension close open question finally prove adversarial dimension either larger smaller standard dimension depend hypothesis class adversary make interest object study right',\n",
              "  'propose parsimonious quantile regression framework learn dynamic tail behaviors financial asset return model capture well time vary characteristic asymmetrical heavy tail property financial time series combine merit popular sequential neural network model lstm novel parametric quantile function construct represent conditional distribution asset return model also capture individually serial dependences higher moments rather volatility across wide range asset class sample forecast conditional quantiles var model outperform garch family propose approach suffer issue quantile cross expose ill posedness compare parametric probability density function approach',\n",
              "  'introduce spike slab deep learn fully bayesian alternative dropout improve generalizability deep relu network new type regularization enable provable recovery smooth input output map unknown level smoothness indeed show posterior distribution concentrate near minimax rate alpha holder smooth map perform well know smoothness level alpha ahead time result shed light architecture design deep neural network namely choice depth width sparsity level network attribute typically depend unknown smoothness order optimal obviate constraint fully bay construction aside show overfit sense posterior concentrate smaller network fewer optimal number nod link result provide new theoretical justifications deep relu network bayesian point view',\n",
              "  'propose prototypical network problem shoot classification classifier must generalize new class see train set give small number examples new class prototypical network learn metric space classification perform compute distance prototype representations class compare recent approach shoot learn reflect simpler inductive bias beneficial limit data regime achieve excellent result provide analysis show simple design decisions yield substantial improvements recent approach involve complicate architectural choices meta learn extend prototypical network zero shoot learn achieve state art result bird dataset',\n",
              "  'study consistency properties machine learn methods base minimize convex surrogates extend recent framework osokin quantitative analysis consistency properties case inconsistent surrogates key technical contribution consist new lower bind calibration function quadratic surrogate non trivial always zero inconsistent case new bind allow quantify level inconsistency set show learn inconsistent surrogates guarantee sample complexity optimization difficulty apply theory two concrete case multi class classification tree structure loss rank mean average precision loss result show approximation computation trade off cause inconsistent surrogates potential benefit',\n",
              "  'despite impressive performance deep neural network dnns typically underperform gradient boost tree gbts many tabular dataset learn task propose apply different regularization coefficient weight might boost performance dnns allow make use relevant input however lead intractable number hyperparameters introduce regularization learn network rlns overcome challenge introduce efficient hyperparameter tune scheme minimize new counterfactual loss result show rlns significantly improve dnns tabular datasets achieve comparable result gbts best performance achieve ensemble combine gbts rlns rlns produce extremely sparse network eliminate network edge input feature thus provide interpretable model reveal importance network assign different input rlns could efficiently learn single network datasets comprise tabular unstructured data set medical image accompany electronic health record open source implementation rln find https github com irashavitt regularizationnetworks',\n",
              "  'convolutional network core state art computer vision solutions wide variety task since deep convolutional network start become mainstream yield substantial gain various benchmarks although increase model size computational cost tend translate immediate quality gain task long enough label data provide train computational efficiency low parameter count still enable factor various use case mobile vision big data scenarios explore ways scale network ways aim utilize add computation efficiently possible suitably factorize convolutions aggressive regularization benchmark methods ilsvrc classification challenge validation set demonstrate substantial gain state art top top error single frame evaluation use network computational cost billion multiply add per inference use less million parameters ensemble model multi crop evaluation report top error validation set error test set top error validation set',\n",
              "  'multi task learn mtl appeal deep learn regularization paper tackle specific mtl context denote primary mtl ultimate goal improve performance give primary task leverage several auxiliary task main methodological contribution introduce rock new generic multi modal fusion block deep learn tailor primary mtl context rock architecture base residual connection make forward prediction explicitly impact intermediate auxiliary representations auxiliary predictor architecture also specifically design primary mtl context incorporate intensive pool operators maximize complementarity intermediate representations extensive experiment nyuv dataset object detection scene classification depth prediction surface normal estimation auxiliary task validate relevance approach superiority flat mtl approach method outperform state art object detection model nyuv dataset large margin also able handle large scale heterogeneous input real synthetic image miss annotation modalities',\n",
              "  'model free reinforcement learn aim offer shelf solutions control dynamical systems without require model system dynamics introduce model free random search algorithm train static linear policies continuous control problems common evaluation methodology show method match state art sample efficiency benchmark mujoco locomotion task nonetheless rigorous evaluation reveal assessment performance benchmarks optimistic evaluate performance method hundreds random seed many different hyperparameter configurations benchmark task extensive evaluation possible small computational footprint method simulations highlight high variability performance benchmark task indicate commonly use estimations sample efficiency adequately evaluate performance algorithms result stress need new baselines benchmarks evaluation methodology algorithms',\n",
              "  'describe new software framework fast train generalize linear model framework name snap machine learn snap combine recent advance machine learn systems algorithms nest manner reflect hierarchical architecture modern compute systems prove theoretically hierarchical system accelerate train distribute environments intra node communication cheaper inter node communication additionally provide review implementation snap term gpu acceleration pipelining communication pattern software architecture highlight aspects critical achieve high performance evaluate performance snap single node multi node environments quantify benefit hierarchical scheme data stream functionality compare widely use machine learn software frameworks finally present logistic regression benchmark criteo terabyte click log dataset show snap achieve test loss order magnitude faster previously report result include obtain use tensorflow scikit learn',\n",
              "  'present splinenets practical novel approach use condition convolutional neural network cnns splinenets continuous generalizations neural decision graph dramatically reduce runtime complexity computation cost cnns maintain even increase accuracy function splinenets dynamic condition input hierarchical condition computational path splinenets employ unify loss function desire level smoothness network decision parameters allow sparse activation subset nod individual sample particular embed infinitely many function weight filter smooth low dimensional manifold parameterized compact splines index position parameter instead sample categorical distribution pick branch sample choose continuous position pick function weight show maximize mutual information spline position class label network optimally utilize specialize classification task experiment show approach significantly increase accuracy resnets negligible cost speed match precision level resnet level splinenet',\n",
              "  'study finite sum nonconvex optimization problems objective function average nonconvex function propose new stochastic gradient descent algorithm base nest variance reduction compare conventional stochastic variance reduce gradient svrg algorithm use two reference point construct semi stochastic gradient diminish variance iteration algorithm use nest reference point build semi stochastic gradient reduce variance iteration smooth nonconvex function propose algorithm converge epsilon approximate first order stationary point nabla mathbf leq epsilon within tilde land epsilon epsilon land epsilon number stochastic gradient evaluations improve best know gradient complexity svrg epsilon scsg land epsilon epsilon land epsilon gradient dominate function algorithm also achieve better gradient complexity state art algorithms thorough experimental result different nonconvex optimization problems back theory',\n",
              "  'study problem identify best action sequential decision make set reward distributions arm exhibit non trivial dependence structure govern underlie causal model domain agent deploy set play arm correspond intervene set variables set specific value paper show whenever underlie causal model take account decision make process standard strategies simultaneously intervene variables subsets variables may general lead suboptimal policies regardless number interventions perform agent environment formally acknowledge phenomenon investigate structural properties imply underlie causal model lead complete characterization relationships arm distributions leverage characterization build new algorithm take input causal structure find minimal sound complete set qualify arm agent play maximize expect reward empirically demonstrate new strategy learn optimal policy lead order magnitude faster convergence rat compare causal insensitive counterparts',\n",
              "  'several applications reinforcement learn suffer instability due high variance especially prevalent high dimensional domains regularization commonly use technique machine learn reduce variance cost introduce bias exist regularization techniques focus spatial perceptual regularization yet reinforcement learn due nature bellman equation opportunity also exploit temporal regularization base smoothness value estimate trajectories paper explore class methods temporal regularization formally characterize bias induce technique use markov chain concepts illustrate various characteristics temporal regularization via sequence simple discrete continuous mdps show technique provide improvement even high dimensional atari game',\n",
              "  'paper address problem manipulate image use natural language description task aim semantically modify visual attribute object image accord text describe new visual appearance although exist methods synthesize image new attribute fully preserve text irrelevant content original image paper propose text adaptive generative adversarial network tagan generate semantically manipulate image preserve text irrelevant content key method text adaptive discriminator create word level local discriminators accord input text classify fine grain attribute independently discriminator generator learn generate image regions correspond give text modify experimental result show method outperform exist methods cub oxford datasets result mostly prefer user study extensive analysis show method able effectively disentangle visual attribute produce please output',\n",
              "  'consider classification problem label unlabeled data available show linear classifiers define convex margin base surrogate losses decrease impossible construct emph semi supervise approach able guarantee improvement supervise classifier measure surrogate loss label unlabeled data convex margin base loss function also increase demonstrate safe improvements emph possible',\n",
              "  'long short term memory lstm network type recurrent neural network complex computational unit successfully apply variety sequence model task paper develop tree long short term memory treelstm neural network model base lstm design predict tree rather linear sequence treelstm define probability sentence estimate generation probability dependency tree time step node generate base representation generate sub tree enhance model power treelstm explicitly represent correlations leave right dependents application model msr sentence completion challenge achieve result beyond current state art also report result dependency parse reranking achieve competitive performance',\n",
              "  'model neural machine translation often discriminative family encoderdecoders learn conditional distribution target sentence give source sentence paper propose variational model learn conditional distribution neural machine translation variational encoderdecoder model train end end different vanilla encoder decoder model generate target translations hide representations source sentence alone variational model introduce continuous latent variable explicitly model underlie semantics source sentence guide generation target translations order perform efficient posterior inference large scale train build neural posterior approximator condition source target side equip reparameterization technique estimate variational lower bind experiment chinese english english german translation task show propose variational neural machine translation achieve significant improvements vanilla neural machine translation baselines',\n",
              "  'study problem video video synthesis whose goal learn map function input source video sequence semantic segmentation mask output photorealistic video precisely depict content source video image counterpart image image translation problem popular topic video video synthesis problem less explore literature without model temporal dynamics directly apply exist image synthesis approach input video often result temporally incoherent videos low visual quality paper propose video video synthesis approach generative adversarial learn framework carefully design generators discriminators couple spatio temporal adversarial objective achieve high resolution photorealistic temporally coherent video result diverse set input format include segmentation mask sketch pose experiment multiple benchmarks show advantage method compare strong baselines particular model capable synthesize resolution videos street scenes second long significantly advance state art video synthesis finally apply method future video prediction outperform several compete systems code model result available website https github com nvidia vid vid please use adobe reader see embed videos paper',\n",
              "  'recently learn discriminative feature improve recognition performances gradually become primary goal deep learn numerous remarkable work emerge paper propose novel yet extremely simple method virtual softmax enhance discriminative property learn feature inject dynamic virtual negative class original softmax inject virtual class aim enlarge inter class margin compress intra class distribution strengthen decision boundary constraint although seem weird optimize additional virtual class show method derive intuitive clear motivation indeed encourage feature compact separable paper empirically experimentally demonstrate superiority virtual softmax improve performances variety object classification face verification task',\n",
              "  'recent progress deep generative model lead tremendous breakthroughs image generation able synthesize photorealistic image exist model lack understand underlie world different previous work build datasets model present new generative model visual object network vons synthesize natural image object disentangle representation inspire classic graphics render pipelines unravel image formation process three conditionally independent factor shape viewpoint texture present end end adversarial learn framework jointly model shape texture model first learn synthesize shape indistinguishable real shape render object sketch silhouette depth map shape sample viewpoint finally learn add realistic textures sketch generate realistic image von generate image realistic state art image synthesis methods also enable many operations change viewpoint generate image shape texture edit linear interpolation texture shape space transfer appearance across different object viewpoints',\n",
              "  'propose wasserstein auto encoder wae new algorithm build generative model data distribution wae minimize penalize form wasserstein distance model distribution target distribution lead different regularizer one use variational auto encoder vae regularizer encourage encode train distribution match prior compare algorithm several techniques show generalization adversarial auto encoders aae experiment show wae share many properties vaes stable train encoder decoder architecture nice latent manifold structure generate sample better quality measure fid score',\n",
              "  'introduce new efficient principled backpropagation compatible algorithm learn probability distribution weight neural network call bay backprop regularise weight minimise compression cost know variational free energy expect lower bind marginal likelihood show principled kind regularisation yield comparable performance dropout mnist classification demonstrate learn uncertainty weight use improve generalisation non linear regression problems weight uncertainty use drive exploration exploitation trade reinforcement learn',\n",
              "  'object orient representations reinforcement learn show promise transfer learn previous research introduce propositional object orient framework provably efficient learn bound respect sample complexity however framework limitations term class task efficiently learn paper introduce novel deictic object orient framework provably efficient learn bound solve broader range task additionally show framework capable zero shoot transfer transition dynamics across task demonstrate empirically taxi sokoban domains'],\n",
              " ['introduce new convex optimization problem term quadratic decomposable submodular function minimization problem closely relate decomposable submodular function minimization arise many learn graph hypergraphs settings graph base semi supervise learn pagerank approach problem via new dual strategy describe objective may optimize via random coordinate descent rcd methods projections onto con also establish linear convergence rate rcd algorithm develop efficient projection algorithms provable performance guarantee numerical experiment semi supervise learn hypergraphs confirm efficiency propose algorithm demonstrate significant improvements prediction accuracy respect state art methods'],\n",
              " ['paper consider parallelization applications whose objective express maximize non monotone submodular function cardinality constraint main result algorithm whose approximation arbitrarily close log adaptive round size grind set exponential speedup parallel run time previously study algorithm constrain non monotone submodular maximization beyond provable guarantee algorithm perform well practice specifically experiment traffic monitor personalize data summarization applications show algorithm find solutions whose value competitive state art algorithms run exponentially fewer parallel iterations',\n",
              "  'study classic mean median cluster fundamental problems unsupervised learn set data partition across multiple sit allow discard small portion data label outliers propose simple approach base construct small summary original dataset propose method time communication efficient good approximation guarantee identify global outliers effectively best knowledge first practical algorithm theoretical guarantee distribute cluster outliers experiment real synthetic data demonstrate clear superiority algorithm baseline algorithms almost metrics',\n",
              "  'active search learn paradigm actively identify many members give class possible critical target scenario high throughput screen scientific discovery drug materials discovery settings specialize instrument often evaluate emph multiple point simultaneously however exist work active search focus sequential acquisition bridge gap address batch active search theoretical practical perspective first derive bayesian optimal policy problem prove lower bind performance gap sequential batch optimal policies cost parallelization also propose novel efficient batch policies inspire state art sequential policies develop aggressive prune technique dramatically speed computation conduct thorough experiment data three application domains citation network material science drug discovery test propose policies total wide range batch size result demonstrate empirical performance gap match theoretical bind nonmyopic policies usually significantly outperform myopic alternatives diversity important consideration batch policy design',\n",
              "  'similarity search fundamental problem compute science various applications attract significant research attention especially large scale search high dimension motivate evidence biological science work develop novel approach similarity search fundamentally different exist methods typically reduce dimension data lessen computational complexity speed search approach project data even higher dimensional space ensure sparsity data output space objective improve precision speed specifically approach two key step firstly compute optimal sparse lift give input sample increase dimension data approximately preserve pairwise similarity secondly seek optimal lift operator best map input sample optimal sparse lift computationally step model optimization problems efficiently effectively solve frank wolfe algorithm simple approach report significantly improve result empirical evaluations exhibit high potentials solve practical problems',\n",
              "  'distribute compute environment consider empirical risk minimization problem propose distribute communication efficient newton type optimization method every iteration worker locally find approximate newton ant direction send main driver main driver average ant directions receive workers form globally improve ant giant direction giant highly communication efficient naturally exploit trade off local computations global communications local computations result fewer overall round communications theoretically show giant enjoy improve convergence rate compare first order methods exist distribute newton type methods sharp contrast many exist distribute newton type methods well popular first order methods highly advantageous practical feature giant involve one tune parameter conduct large scale experiment computer cluster empirically demonstrate superior performance giant',\n",
              "  'exist deep convolutional neural network cnns classification global average first order pool gap become standard module summarize activations last convolution layer final representation prediction recent research show integration higher order pool hop methods clearly improve performance deep cnns however gap exist hop methods assume unimodal distributions fully capture statistics convolutional activations limit representation ability deep cnns especially sample complex content overcome limitation paper propose global gate mixture second order pool sop method improve representation ability deep cnns end introduce sparsity constrain gate mechanism propose novel parametric sop component mixture model give bank sop candidates method adaptively choose top candidates input sample sparsity constrain gate module perform weight sum output select candidates representation sample propose sop flexibly accommodate large number personalize sop candidates efficient way lead richer representations deep network sop end end train potential characterize complex multi modal distributions propose method evaluate two large scale image benchmarks downsampled imagenet place experimental result show sop superior counterparts achieve competitive performance source code available http www peihuali org sop',\n",
              "  'many structure data fit applications require solution optimization problem involve sum potentially large number measurements incremental gradient algorithms offer inexpensive iterations sample subset term sum methods make great progress initially often slow approach solution contrast full gradient methods achieve steady convergence expense evaluate full objective gradient iteration explore hybrid methods exhibit benefit approach rate convergence analysis show control sample size incremental gradient algorithm possible maintain steady convergence rat full gradient methods detail practical quasi newton implementation base approach numerical experiment illustrate potential benefit',\n",
              "  'goal reinforcement learn algorithms estimate optimise value function however unlike supervise learn teacher oracle available provide true value function instead majority reinforcement learn algorithms estimate optimise proxy value function proxy typically base sample bootstrapped approximation true value function know return particular choice return one chief components determine nature algorithm rate future reward discount value bootstrapped even nature reward well know decisions crucial overall success algorithms discuss gradient base meta learn algorithm able adapt nature return online whilst interact learn environment apply game atari environment million frame algorithm achieve new state art performance',\n",
              "  'occurrence multiple diseases among general population important problem patients risk complications represent large share health care expenditure learn predict time event probabilities patients challenge problem risk events correlate compete risk often patients experience individual events interest fraction actually observe data introduce paper survival model flexibility leverage common representation relate events design correct strong imbalance observe outcomes procedure sequential outcome specific survival distributions form components nonparametric multivariate estimators combine ensemble way ensure accurate predictions outcome type simultaneously algorithm general represent first boost like method time event data multiple outcomes demonstrate performance algorithm synthetic real data',\n",
              "  'identify fundamental source error learn form dynamic program function approximation delusional bias arise approximation architecture limit class expressible greedy policies since standard update make globally uncoordinated action choices respect expressible policy class inconsistent even conflict value estimate result lead pathological behaviour estimation instability even divergence solve problem introduce new notion policy consistency define local backup process ensure global consistency use information set set record constraints policies consistent back value prove model base model free algorithms use backup remove delusional bias yield first know algorithms guarantee optimal result general condition algorithms furthermore require polynomially many information set potentially exponential support finally suggest practical heuristics value iteration learn attempt reduce delusional bias',\n",
              "  'bayesian learn build assumption model space contain true reflection data generate mechanism assumption problematic particularly complex data environments present bayesian nonparametric approach learn make use statistical model assume model true approach provably better properties use parametric model admit monte carlo sample scheme afford massive scalability modern computer architectures model base aspect learn particularly attractive regularize nonparametric inference sample size small also correct approximate approach variational bay demonstrate approach number examples include classifiers bayesian random forest',\n",
              "  'capacity neural network absorb information limit number parameters conditional computation part network active per example basis propose theory way dramatically increase model capacity without proportional increase computation practice however significant algorithmic performance challenge work address challenge finally realize promise conditional computation achieve greater improvements model capacity minor losses computational efficiency modern gpu cluster introduce sparsely gate mixture experts layer moe consist thousands fee forward sub network trainable gate network determine sparse combination experts use example apply moe task language model machine translation model capacity critical absorb vast quantities knowledge available train corpora present model architectures moe billion parameters apply convolutionally stack lstm layer large language model machine translation benchmarks model achieve significantly better result state art lower computational cost',\n",
              "  'classical anomaly detection principally concern point base anomalies anomalies occur single point time yet many real world anomalies range base mean occur period time motivate observation present new mathematical model evaluate accuracy time series classification algorithms model expand well know precision recall metrics measure range simultaneously enable customization support domain specific preferences',\n",
              "  'paucity videos current action classification datasets ucf hmdb make difficult identify good video architectures methods obtain similar performance exist small scale benchmarks paper evaluate state art architectures light new kinetics human action video dataset kinetics two order magnitude data human action class clip per class collect realistic challenge youtube videos provide analysis current architectures fare task action classification dataset much performance improve smaller benchmark datasets pre train kinetics also introduce new two stream inflate convnet base convnet inflation filter pool kernels deep image classification convnets expand make possible learn seamless spatio temporal feature extractors video leverage successful imagenet architecture design even parameters show pre train kinetics model considerably improve upon state art action classification reach hmdb ucf',\n",
              "  'statistical leverage score emerge fundamental tool matrix sketch column sample applications low rank approximation regression random feature learn quadrature yet nature quantity barely understand borrow ideas orthogonal polynomial literature introduce regularize christoffel function associate positive definite kernel uncover variational formulation leverage score kernel methods allow elucidate relationships choose kernel well population density main result quantitatively describe decrease relation leverage score population density broad class kernels euclidean space numerical simulations support find',\n",
              "  'recently adversarial erase weakly supervise object attention deeply study due capability localize integral object regions however strategy raise one key problem attention regions gradually expand non object regions train iterations continue significantly decrease quality produce attention map tackle issue well promote quality object attention introduce simple yet effective self erase network seenet prohibit attentions spread unexpected background regions particular seenet leverage two self erase strategies encourage network use reliable object background cue learn attention way integral object regions effectively highlight without include much background regions test quality generate attention map employ mine object regions heuristic cue learn semantic segmentation model experiment pascal voc well demonstrate superiority seenet state art methods',\n",
              "  'role semantics zero shoot learn consider effectiveness previous approach analyze accord form supervision provide learn semantics independently others supervise semantic subspace explain train class thus former able constrain whole space lack ability model semantic correlations latter address issue leave part semantic space unsupervised complementarity exploit new convolutional neural network cnn framework propose use semantics constraints recognition although cnn train classification transfer ability encourage learn hide semantic layer together semantic code classification two form semantic constraints introduce first loss base regularizer introduce generalization constraint semantic predictor second codeword regularizer favor semantic class mappings consistent prior semantic knowledge allow learn data significant improvements state art achieve several datasets',\n",
              "  'study stochastic composite mirror descent class scalable algorithms able exploit geometry composite structure problem consider convex strongly convex objectives non smooth loss function establish high probability convergence rat optimal logarithmic factor apply derive computational error bound study generalization performance multi pass stochastic gradient descent sgd non parametric set high probability generalization bound enjoy logarithmical dependency number pass provide step size sequence square summable improve exist bound expectation polynomial dependency therefore give strong justification ability multi pass sgd overcome overfitting analysis remove boundedness assumptions subgradients often impose literature numerical result report support theoretical find',\n",
              "  'attention mechanism effective focus deep learn model relevant feature interpret however attentions may unreliable since network generate often train weakly supervise manner overcome limitation introduce notion input dependent uncertainty attention mechanism generate attention feature vary degrees noise base give input learn larger variance instance uncertain learn uncertainty aware attention mechanism use variational inference validate various risk prediction task electronic health record model significantly outperform exist attention model analysis learn attentions show model generate attentions comply clinicians interpretation provide richer interpretation via learn variance evaluation accuracy uncertainty calibration prediction performance know decision show yield network high reliability well'],\n",
              " ['convolutional long short term memory lstm network widely use action gesture recognition different attention mechanisms also embed lstm convolutional lstm convlstm network base previous gesture recognition architectures combine three dimensional convolution neural network dcnn convlstm paper explore effect attention mechanism convlstm several variants convlstm evaluate remove convolutional structure three gate convlstm apply attention mechanism input convlstm reconstruct input output gate respectively modify channel wise attention mechanism evaluation result demonstrate spatial convolutions three gate scarcely contribute spatiotemporal feature fusion attention mechanisms embed input output gate improve feature fusion word convlstm mainly contribute temporal fusion along recurrent step learn long term spatiotemporal feature take input spatial spatiotemporal feature basis new variant lstm derive convolutional structure embed input state transition lstm code lstm variants publicly available',\n",
              "  'channel prune one predominant approach deep model compression exist prune methods either train scratch sparsity constraints channel minimize reconstruction error pre train feature map compress ones strategies suffer limitations former kind computationally expensive difficult converge whilst latter kind optimize reconstruction error ignore discriminative power channel overcome drawbacks investigate simple yet effective method call discrimination aware channel prune choose channel really contribute discriminative power end introduce additional losses network increase discriminative power intermediate layer select discriminative channel layer consider additional loss reconstruction error last propose greedy algorithm conduct channel selection parameter optimization iterative way extensive experiment demonstrate effectiveness method example ilsvrc prune resnet reduction channel even outperform original model top accuracy',\n",
              "  'real time automatic speech recognition asr mobile embed devices great interest many years present real time speech recognition smartphones embed systems employ recurrent neural network rnn base acoustic model rnn base language model beam search decode acoustic model end end train connectionist temporal classification ctc loss rnn implementation embed devices suffer excessive dram access parameter size neural network usually exceed cache memory parameters use time step remedy problem employ multi time step parallelization approach compute multiple output sample time parameters fetch dram since number dram access reduce proportion number parallelization step achieve high process speed however conventional rnns long short term memory lstm gate recurrent unit gru permit multi time step parallelization construct acoustic model combine simple recurrent units srus depth wise dimensional convolution layer multi time step parallelization character word piece model develop acoustic model correspond rnn base language model use beam search decode achieve competitive wer wsj corpus use entire model size around achieve real time speed use single core arm without gpu special hardware',\n",
              "  'estimate individual treatment effect ite challenge problem causal inference due miss counterfactuals selection bias exist ite estimation methods mainly focus balance distributions control treat group ignore local similarity information helpful paper propose local similarity preserve individual treatment effect site estimation method base deep representation learn site preserve local similarity balance data distributions simultaneously focus several hard sample mini batch experimental result synthetic three real world datasets demonstrate advantage propose site method compare state art ite estimation methods'],\n",
              " ['hypergraph partition important problem machine learn computer vision network analytics widely use method hypergraph partition rely minimize normalize sum cost partition hyperedges across cluster algorithmic solutions base approach assume different partition hyperedge incur cost however assumption fail leverage fact different subsets vertices within hyperedge may different structural importance hence propose new hypergraph cluster technique term inhomogeneous hypergraph partition assign different cost different hyperedge cut prove inhomogeneous partition produce quadratic approximation optimal solution inhomogeneous cost satisfy submodularity constraints moreover demonstrate inhomogenous partition offer significant performance improvements applications structure learn rank subspace segmentation motif cluster',\n",
              "  'introduce new approach decomposable submodular function minimization dsfm exploit incidence relations incidence relations describe variables effectively influence component function properly utilize allow improve convergence rat dsfm solvers main result include precise parametrization dsfm problem base incidence relations development new scalable alternative projections parallel coordinate descent methods accompany rigorous analysis convergence rat',\n",
              "  'propose structure adaptive variant state art stochastic variance reduce gradient algorithm katyusha regularize empirical risk minimization propose method able exploit intrinsic low dimensional structure solution sparsity low rank enforce non smooth regularization achieve even faster convergence rate provable algorithmic improvement do restart katyusha algorithm accord restrict strong convexity constants demonstrate effectiveness approach via numerical experiment',\n",
              "  'give rigorous analysis statistical behavior gradients randomly initialize fully connect network relu activations result show empirical variance square entries input output jacobian exponential simple architecture dependent constant beta give sum reciprocals hide layer widths beta large gradients compute initialization vary wildly approach complement mean field theory analysis random network point view rigorously compute finite width corrections statistics gradients edge chaos',\n",
              "  'present first accelerate randomize algorithm solve linear systems euclidean space one essential problem type matrix inversion problem particular algorithm specialize invert positive definite matrices way iterate approximate solutions generate algorithm positive definite matrices open way many applications field optimization machine learn application general theory develop first accelerate deterministic stochastic quasi newton update update lead provably aggressive approximations inverse hessian lead speed up classical non accelerate rule numerical experiment experiment empirical risk minimization show rule accelerate train machine learn model',\n",
              "  'paper address graph match problem follow recent work cite zaslavskiy path vestner analyze generalize idea concave relaxations introduce concepts emph conditionally concave emph probably conditionally concave energies polytopes show encapsulate many instance graph match problem include match euclidean graph graph surface prove local minima probably conditionally concave energies general match polytopes doubly stochastic high probability extreme point match polytope permutations',\n",
              "  'work introduce interactive structure learn framework unify many different interactive learn task present generalization query committee active learn algorithm set study consistency rate convergence theoretically empirically without noise',\n",
              "  'convolutional neural network cnns recently achieve great success single image super resolution sisr however methods tend produce smooth output miss textural detail solve problems propose super resolution cliquenet srcliquenet reconstruct high resolution image better textural detail wavelet domain propose srcliquenet firstly extract set feature map low resolution image clique block group send set feature map clique sample module reconstruct image clique sample module consist four sub net predict high resolution wavelet coefficients four sub band since consider edge feature properties four sub band four sub net connect others learn coefficients four sub band jointly finally apply inverse discrete wavelet transform idwt output four sub net end clique sample module increase resolution reconstruct image extensive quantitative qualitative experiment benchmark datasets show method achieve superior performance state art methods'],\n",
              " ['large scale network model use neurons static nonlinearities produce analog output despite fact information process brain predominantly carry dynamic neurons produce discrete pulse call spike research spike base computation impede lack efficient supervise learn algorithm spike neural network present gradient descent method optimize spike network model introduce differentiable formulation spike dynamics derive exact gradient calculation demonstration train recurrent spike network two dynamic task one require optimize fast millisecond spike base interactions efficient encode information delay memory task extend duration second result show gradient descent approach indeed optimize network dynamics time scale individual spike well behavioral time scale conclusion method yield general purpose supervise learn algorithm spike neural network facilitate investigations spike base computations',\n",
              "  'ideas enjoy large impact deep learn convolution problem involve pixels spatial representations common intuition hold convolutional neural network may appropriate paper show strike counterexample intuition via seemingly trivial coordinate transform problem simply require learn map coordinate cartesian space coordinate one hot pixel space although convolutional network would seem appropriate task show fail spectacularly demonstrate carefully analyze failure first toy problem point simple fix become obvious call solution coordconv work give convolution access input coordinate use extra coordinate channel without sacrifice computational parametric efficiency ordinary convolution coordconv allow network learn either complete translation invariance vary degrees translation dependence require end task coordconv solve coordinate transform problem perfect generalization time faster time fewer parameters convolution stark contrast raise question extent inability convolution persist insidiously inside task subtly hamper performance within complete answer question require investigation show preliminary evidence swap convolution coordconv improve model diverse set task use coordconv gin produce less mode collapse transform high level spatial latents pixels become easier learn faster cnn detection model train mnist detection show better iou use coordconv reinforcement learn domain agents play atari game benefit significantly use coordconv layer',\n",
              "  'accurate exposure key capture high quality photos computational photography especially mobile phone limit size camera modules inspire luminosity mask usually apply professional photographers paper develop novel algorithm learn local exposures deep reinforcement adversarial learn specific segment image sub image reflect variations dynamic range exposures accord raw low level feature base sub image local exposure sub image automatically learn virtue policy network sequentially reward learn globally design strike balance overall exposures aesthetic evaluation function approximate discriminator generative adversarial network reinforcement learn adversarial learn train collaboratively asynchronous deterministic policy gradient generative loss approximation simply algorithmic architecture also prove feasibility leverage discriminator value function employ local exposure retouch raw input image respectively thus deliver multiple retouch image different exposures fuse exposure blend extensive experiment verify algorithms superior state art methods term quantitative accuracy visual illustration',\n",
              "  'typical way network data record measure interactions involve specify set core nod produce graph contain core together potentially larger set fringe nod link core interactions nod fringe however present result graph data example phone service provider may record call least one participants customer include call customer non customer pair non customers knowledge nod belong core crucial interpret dataset metadata unavailable many case either lose due difficulties data provenance network consist find data obtain settings counter surveillance lead algorithmic problem recover core set since core vertex cover essentially plant vertex cover problem arbitrary underlie graph develop framework analyze plant vertex cover problem base theory fix parameter tractability together algorithms recover core algorithms fast simple implement perform several baselines base core periphery structure various real world datasets',\n",
              "  'despite remarkable advance image synthesis research exist work often fail manipulate image context large geometric transformations synthesize person image condition arbitrary pose one representative examples generation quality largely rely capability identify model arbitrary transformations different body part current generative model often build local convolutions overlook key challenge heavy occlusions different view dramatic appearance change distinct geometric change happen part cause arbitrary pose manipulations paper aim resolve challenge induce geometric variability spatial displacements via new soft gate warp generative adversarial network warp gin compose two stag first synthesize target part segmentation map give target pose depict region level spatial layouts guide image synthesis higher level structure constraints warp gin equip soft gate warp block learn feature level map render textures original image generate segmentation map warp gin capable control different transformation degrees give distinct target pose moreover propose warp block light weight flexible enough inject network human perceptual study quantitative evaluations demonstrate superiority warp gin significantly outperform exist methods two large datasets',\n",
              "  'tremendous recent progress equilibrium find algorithms zero sum imperfect information extensive form game puzzle gap theory practice first order methods significantly better theoretical convergence rat counterfactual regret minimization cfr variant despite cfr variants favor practice experiment first order methods conduct small medium size game methods complicate implement set cfr variants enhance extensively decade perform well practice paper show particular first order method state art variant excessive gap technique instantiate dilate entropy distance function efficiently solve large real world problems competitively cfr variants show large endgames encounter libratus poker recently beat top human poker specialist professionals limit texas hold show experimental result variant excessive gap technique well prior version introduce numerically friendly implementation smooth best response computation associate first order methods extensive form game solve present knowledge first gpu implementation first order method extensive form game present comparisons several excessive gap technique cfr variants',\n",
              "  'convolutional neural network cnns inherently subject invariable filter aggregate local input topological structure cause cnns allow manage data euclidean grid like structure image ones non euclidean graph structure traffic network broaden reach cnns develop structure aware convolution eliminate invariance yield unify mechanism deal euclidean non euclidean structure data technically filter structure aware convolution generalize univariate function capable aggregate local input diverse topological structure since infinite parameters require determine univariate function parameterize filter number learnable parameters context function approximation theory replace classical convolution cnns structure aware convolution structure aware convolutional neural network sacnns readily establish extensive experiment eleven datasets strongly evidence sacnns outperform current model various machine learn task include image classification cluster text categorization skeleton base action recognition molecular activity detection taxi flow prediction',\n",
              "  'recent years supervise learn convolutional network cnns see huge adoption computer vision applications comparatively unsupervised learn cnns receive less attention work hope help bridge gap success cnns supervise learn unsupervised learn introduce class cnns call deep convolutional generative adversarial network dcgans certain architectural constraints demonstrate strong candidate unsupervised learn train various image datasets show convince evidence deep convolutional adversarial pair learn hierarchy representations object part scenes generator discriminator additionally use learn feature novel task demonstrate applicability general image representations',\n",
              "  'recurrent neural network powerful tool understand model computation representation populations neurons continuous variable rate model network analyze apply extensively purpose however neurons fire action potentials discrete nature spike important feature neural circuit dynamics despite significant advance train recurrently connect spike neural network remain challenge present procedure train recurrently connect spike network generate dynamical pattern autonomously produce complex temporal output base integrate network input model physiological data procedure make use continuous variable network identify target train input spike model neurons surprisingly able construct spike network duplicate task perform continuous variable network relatively minor expansion number neurons approach provide novel view significance appropriate use fire rate model useful approach build model spike network use address important question representation computation neural systems'],\n",
              " ['paper propose stochastic recursive gradient algorithm sarah well practical variant sarah novel approach finite sum minimization problems different vanilla sgd modern stochastic methods svrg sag saga sarah admit simple recursive framework update stochastic gradient estimate compare sag saga sarah require storage past gradients linear convergence rate sarah prove strong convexity assumption also prove linear convergence rate strongly convex case inner loop sarah property svrg possess numerical experiment demonstrate efficiency algorithm',\n",
              "  'paper introduce wasserstein variational inference new form approximate bayesian inference base optimal transport theory wasserstein variational inference use new family divergences include divergences wasserstein distance special case gradients wasserstein variational loss obtain backpropagating sinkhorn iterations technique result stable likelihood free train method use implicit distributions probabilistic program use wasserstein variational inference framework introduce several new form autoencoders test robustness performance exist variational autoencoding techniques',\n",
              "  'introduce game theoretic approach study recommendation systems strategic content providers systems fair stable show traditional approach fail satisfy requirements propose shapley mediator show shapley mediator satisfy fairness stability requirements run linear time economically efficient mechanism satisfy properties',\n",
              "  'propose structure adaptive variant state art stochastic variance reduce gradient algorithm katyusha regularize empirical risk minimization propose method able exploit intrinsic low dimensional structure solution sparsity low rank enforce non smooth regularization achieve even faster convergence rate provable algorithmic improvement do restart katyusha algorithm accord restrict strong convexity constants demonstrate effectiveness approach via numerical experiment',\n",
              "  'introduce new approach decomposable submodular function minimization dsfm exploit incidence relations incidence relations describe variables effectively influence component function properly utilize allow improve convergence rat dsfm solvers main result include precise parametrization dsfm problem base incidence relations development new scalable alternative projections parallel coordinate descent methods accompany rigorous analysis convergence rat'],\n",
              " ['unsupervised learn generative adversarial network gans prove hugely successful regular gans hypothesize discriminator classifier sigmoid cross entropy loss function however find loss function may lead vanish gradients problem learn process overcome problem propose paper least square generative adversarial network lsgans adopt least square loss function discriminator show minimize objective function lsgan yield minimize pearson chi divergence two benefit lsgans regular gans first lsgans able generate higher quality image regular gans second lsgans perform stable learn process evaluate lsgans five scene datasets experimental result show image generate lsgans better quality ones generate regular gans also conduct two comparison experiment lsgans regular gans illustrate stability lsgans',\n",
              "  'stochastic gradient descent sgd popular algorithm achieve state art performance variety machine learn task several researchers recently propose scheme parallelize sgd require performance destroy memory lock synchronization work aim show use novel theoretical analysis algorithms implementation sgd implement without lock present update scheme call hogwild allow processors access share memory possibility overwrite work show associate optimization problem sparse mean gradient update modify small part decision variable hogwild achieve nearly optimal rate convergence demonstrate experimentally hogwild outperform alternative scheme use lock order magnitude',\n",
              "  'paper introduce versatile filter construct efficient convolutional neural network consider demand efficient deep learn techniques run cost effective hardware number methods develop learn compact neural network work aim slim filter different ways investigate small sparse binarized filter contrast treat filter additive perspective series secondary filter derive primary filter secondary filter inherit primary filter without occupy storage unfold computation could significantly enhance capability filter integrate information extract different receptive field besides spatial versatile filter additionally investigate versatile filter channel perspective new techniques general upgrade filter exist cnns experimental result benchmark datasets neural network demonstrate cnns construct versatile filter able achieve comparable accuracy original filter require less memory flop'],\n",
              " ['policy gradient methods widely use control reinforcement learn particularly continuous action set host theoretically sound algorithms propose policy set due existence policy gradient theorem provide simplify form gradient policy learn however behaviour policy necessarily attempt learn follow optimal policy give task existence theorem elusive work solve open problem provide first policy policy gradient theorem key derivation use emphatic weight develop new actor critic algorithm call actor critic emphatic weight ace approximate simplify gradients provide theorem demonstrate simple counterexample previous policy policy gradient methods particularly offpac dpg converge wrong solution whereas ace find optimal solution',\n",
              "  'consider problem online learn linear contextual bandits set also strong individual fairness constraints govern unknown similarity metric constraints demand select similar action individuals approximately equal probability dhprz may odds optimize reward thus model settings profit social policy tension assume learn unknown mahalanobis similarity metric weak feedback identify fairness violations quantify extent intend represent interventions regulator know unfairness see nevertheless enunciate quantitative fairness metric individuals main result algorithm adversarial context set number fairness violations depend logarithmically obtain optimal sqrt regret bind best fair policy',\n",
              "  'humans routinely retrace path novel environment forward backwards despite uncertainty motion paper present approach give demonstration path first network generate path equip second network observe world decide act order retrace path noisy actuation change environment two network optimize end end train time evaluate method two realistic simulators perform path follow forward backwards experiment show approach outperform classical approach solve task well number baselines',\n",
              "  'humans make repeat choices among options imperfectly know reward outcomes important problem psychology neuroscience often study use multi arm bandits also frequently study machine learn present data human stationary bandit experiment vary average abundance variability reward availability mean variance reward rate distributions surprisingly find subject significantly underestimate prior mean reward rat base self report end game reward expectation non choose arm previously human learn bandit task find well capture bayesian ideal learn model dynamic belief model dbm albeit incorrect generative assumption temporal structure humans assume reward rat change time even though actually fix find pessimism bias bandit task well capture prior mean dbm fit human choices poorly capture prior mean fix belief model fbm alternative bayesian model correctly assume reward rat constants pessimism bias also incompletely capture simple reinforcement learn model commonly use neuroscience psychology term fit initial value seem sub optimal thus mysterious humans underestimate prior reward expectation simulations show underestimate prior mean help maximize long term gain observer assume volatility reward rat stable utilize softmax decision policy instead optimal one obtainable dynamic program raise intrigue possibility brain underestimate reward rat compensate incorrect non stationarity assumption generative model simplify decision policy'],\n",
              " ['algorithmically construct multi output gaussian process priors satisfy linear differential equations approach attempt parametrize solutions equations use bner base successful push forward gaussian process along paramerization desire prior consider several examples physics geomathmatics control among full inhomogeneous system maxwell equations bring together stochastic learn computeralgebra novel way combine noisy observations precise algebraic computations',\n",
              "  'present shapenet richly annotate large scale repository shape represent cad model object shapenet contain model multitude semantic categories organize wordnet taxonomy collection datasets provide many semantic annotations model consistent rigid alignments part bilateral symmetry plan physical size keywords well plan annotations annotations make available public web base interface enable data visualization object attribute promote data drive geometric analysis provide large scale quantitative benchmark research computer graphics vision time technical report shapenet index model model classify categories wordnet synsets report describe shapenet effort whole provide detail currently available datasets summarize future plan',\n",
              "  'wide adoption dnns give birth unrelenting compute requirements force datacenter operators adopt domain specific accelerators train accelerators typically employ densely pack full precision float point arithmetic maximize performance per area ongoing research efforts seek increase performance density replace float point fix point arithmetic however significant roadblock attempt fix point narrow dynamic range insufficient dnn train convergence identify block float point bfp promise alternative representation since exhibit wide dynamic range enable majority dnn operations perform fix point logic unfortunately bfp alone introduce several limitations preclude direct applicability work introduce hbfp hybrid bfp approach perform dot products bfp operations float point hbfp deliver best worlds high accuracy float point superior hardware density fix point wide variety model show hbfp match float point accuracy enable hardware implementations deliver higher throughput'],\n",
              " ['machine learn model vulnerable adversarial examples small change image cause computer vision model make mistake identify school bus ostrich however still open question whether humans prone similar mistake address question leverage recent techniques transfer adversarial examples computer vision model know parameters architecture model unknown parameters architecture match initial process human visual system find adversarial examples strongly transfer across computer vision model influence classifications make time limit human observers'],\n",
              " ['paper address graph match problem follow recent work cite zaslavskiy path vestner analyze generalize idea concave relaxations introduce concepts emph conditionally concave emph probably conditionally concave energies polytopes show encapsulate many instance graph match problem include match euclidean graph graph surface prove local minima probably conditionally concave energies general match polytopes doubly stochastic high probability extreme point match polytope permutations',\n",
              "  'ensembles randomize decision tree usually refer random forest widely use classification regression task machine learn statistics random forest achieve competitive predictive performance computationally efficient train test make excellent candidates real world prediction task popular random forest variants breiman random forest extremely randomize tree operate batch train data online methods greater demand exist online random forest however require train data batch counterpart achieve comparable predictive performance work use mondrian process roy teh construct ensembles random decision tree call mondrian forest mondrian forest grow incremental online fashion remarkably distribution online mondrian forest batch mondrian forest mondrian forest achieve competitive predictive performance comparable exist online random forest periodically train batch random forest order magnitude faster thus represent better computation accuracy tradeoff'],\n",
              " ['paper try organize machine teach coherent set ideas idea present vary along dimension collection dimension form problem space machine teach exist teach problems characterize space hope organization allow gain deeper understand individual teach problems discover connections among identify gap field'],\n",
              " ['goal orient dialog give attention due numerous applications artificial intelligence goal orient dialogue task occur questioner ask action orient question answerer respond intent let questioner know correct action take ask adequate question deep learn reinforcement learn recently apply however approach struggle find competent recurrent neural questioner owe complexity learn series sentence motivate theory mind propose answerer questioner mind aqm novel information theoretic algorithm goal orient dialog aqm questioner ask infer base approximate probabilistic model answerer questioner figure answerer intention via select plausible question explicitly calculate information gain candidate intentions possible answer question test framework two goal orient visual dialog task mnist count dialog guesswhat experiment aqm outperform comparative algorithms large margin'],\n",
              " ['bilinear model show achieve impressive performance wide range visual task semantic segmentation fine grain recognition face recognition however bilinear feature high dimensional typically order hundreds thousands million make impractical subsequent analysis propose two compact bilinear representations discriminative power full bilinear representation thousand dimension compact representations allow back propagation classification errors enable end end optimization visual recognition system compact bilinear representations derive novel kernelized analysis bilinear pool provide insights discriminative power bilinear pool platform research compact pool methods experimentation illustrate utility propose representations image classification shoot learn across several datasets'],\n",
              " ['tremendous recent progress equilibrium find algorithms zero sum imperfect information extensive form game puzzle gap theory practice first order methods significantly better theoretical convergence rat counterfactual regret minimization cfr variant despite cfr variants favor practice experiment first order methods conduct small medium size game methods complicate implement set cfr variants enhance extensively decade perform well practice paper show particular first order method state art variant excessive gap technique instantiate dilate entropy distance function efficiently solve large real world problems competitively cfr variants show large endgames encounter libratus poker recently beat top human poker specialist professionals limit texas hold show experimental result variant excessive gap technique well prior version introduce numerically friendly implementation smooth best response computation associate first order methods extensive form game solve present knowledge first gpu implementation first order method extensive form game present comparisons several excessive gap technique cfr variants',\n",
              "  'aim obtain interpretable expressive disentangle scene representation contain comprehensive structural textural information object previous scene representations learn neural network often uninterpretable limit single object lack knowledge work propose scene render network sdn address issue integrate disentangle representations semantics geometry appearance deep generative model scene encoder perform inverse graphics translate scene structure object wise representation decoder two components differentiable shape renderer neural texture generator disentanglement semantics geometry appearance support aware scene manipulation rotate move object freely keep consistent shape texture change object appearance without affect shape experiment demonstrate edit scheme base sdn superior counterpart',\n",
              "  'accurately answer question give image require combine observations general knowledge effortless humans reason general knowledge remain algorithmic challenge advance research direction novel reason correct answer jointly consider entities show challenge fvqa dataset lead improvement accuracy around compare state art',\n",
              "  'reason play essential role visual question answer vqa multi step dynamic reason often necessary answer complex question example question place next bus right picture talk compound object bus right generate relation'],\n",
              " ['generative adversarial network gans technique learn generative model complex data distributions sample despite remarkable advance generate realistic image major shortcoming gans fact tend produce sample little diversity even train diverse datasets phenomenon know mode collapse focus much recent work study principled approach handle mode collapse call pack main idea modify discriminator make decisions base multiple sample class either real artificially generate draw analysis tool binary hypothesis test particular seminal result blackwell prove fundamental connection pack mode collapse show pack naturally penalize generators mode collapse thereby favor generator distributions less mode collapse train process numerical experiment benchmark datasets suggest pack provide significant improvements',\n",
              "  'accurate exposure key capture high quality photos computational photography especially mobile phone limit size camera modules inspire luminosity mask usually apply professional photographers paper develop novel algorithm learn local exposures deep reinforcement adversarial learn specific segment image sub image reflect variations dynamic range exposures accord raw low level feature base sub image local exposure sub image automatically learn virtue policy network sequentially reward learn globally design strike balance overall exposures aesthetic evaluation function approximate discriminator generative adversarial network reinforcement learn adversarial learn train collaboratively asynchronous deterministic policy gradient generative loss approximation simply algorithmic architecture also prove feasibility leverage discriminator value function employ local exposure retouch raw input image respectively thus deliver multiple retouch image different exposures fuse exposure blend extensive experiment verify algorithms superior state art methods term quantitative accuracy visual illustration',\n",
              "  'hash one popular powerful approximate nearest neighbor search techniques large scale image retrieval traditional hash methods first represent image shelf visual feature produce hash cod separate stage however shelf visual feature may optimally compatible hash code learn procedure may result sub optimal hash cod recently deep hash methods propose simultaneously learn image feature hash cod use deep neural network show superior performance traditional hash methods deep hash methods give supervise information form pairwise label triplet label current state art deep hash method dpsh cite feature base pairwise label perform image feature learn hash code learn simultaneously maximize likelihood pairwise similarities inspire dpsh cite feature propose triplet label base deep hash method aim maximize likelihood give triplet label experimental result show method outperform baselines cifar nus wide datasets include state art method dpsh cite feature previous triplet label base deep hash methods'],\n",
              " ['learn decision tree data difficult optimization problem widespread algorithm practice date base greedy growth tree structure recursively split nod possibly prune back final tree parameters decision function internal node approximately estimate minimize impurity measure give algorithm give input tree structure parameter value nod produce new tree smaller structure new parameter value provably lower leave unchanged misclassification error apply axis align oblique tree experiment show consistently outperform various algorithms highly scalable large datasets tree algorithm handle sparsity penalty learn sparse oblique tree structure subset original tree nonzero parameters combine best axis align oblique tree flexibility model correlate data low generalization error fast inference interpretable nod involve feature decision',\n",
              "  'paper compare different type recurrent units recurrent neural network rnns especially focus sophisticate units implement gate mechanism long short term memory lstm unit recently propose gate recurrent unit gru evaluate recurrent units task polyphonic music model speech signal model experiment reveal advance recurrent units indeed better traditional recurrent units tanh units also find gru comparable lstm'],\n",
              " ['contextual bandit literature traditionally focus algorithms address exploration exploitation tradeoff particular greedy algorithms exploit current estimate without exploration may sub optimal general however exploration free greedy algorithms desirable practical settings exploration may costly unethical clinical trials surprisingly find simple greedy algorithm rate optimal achieve asymptotically optimal regret sufficient randomness observe contexts covariates prove always case two arm bandit general class context distributions satisfy condition term covariate diversity furthermore even absent condition show greedy algorithm rate optimal positive probability thus standard bandit algorithms may unnecessarily explore motivate result introduce greedy first new algorithm use observe contexts reward determine whether follow greedy algorithm explore prove algorithm rate optimal without additional assumptions context distribution number arm extensive simulations demonstrate greedy first successfully reduce exploration outperform exist exploration base contextual bandit algorithms thompson sample upper confidence bind ucb',\n",
              "  'note consider normalize gradient descent ngd natural modification classical gradient descent optimization problems serious shortcoming non convex problems may take arbitrarily long escape neighborhood saddle point issue make convergence arbitrarily slow particularly high dimensional non convex problems relative number saddle point often large paper focus continuous time descent show contrary standard ngd escape saddle point quickly particular show ngd almost never converge saddle point time require ngd escape ball radius saddle point sqrt kappa kappa condition number hessian application result global convergence time bind establish ngd mild assumptions',\n",
              "  'societies often rely human experts take wide variety decisions affect members jail release decisions take judge stop frisk decisions take police officer accept reject decisions take academics context decision take expert typically choose uniformly random pool experts however decisions may imperfect due limit experience implicit bias faulty probabilistic reason improve accuracy fairness overall decision make process optimize assignment experts decisions'],\n",
              " ['draw attention important yet largely overlook aspect evaluate fairness automate decision make systems namely risk welfare considerations propose family measure correspond long establish formulations cardinal social welfare economics justify rawlsian conception fairness behind veil ignorance convex formulation welfare base measure fairness allow integrate constraint convex loss minimization pipeline empirical analysis reveal interest trade off proposal prediction accuracy group discrimination dwork notion individual fairness furthermore perhaps importantly work provide heuristic justification empirical evidence suggest lower bind measure often lead bound inequality algorithmic outcomes hence present first computationally feasible mechanism bound individual level inequality'],\n",
              " ['technical challenge deep learn recognize target class without see data zero shoot learn leverage semantic representations attribute class prototypes bridge source target class exist standard zero shoot learn methods may prone overfitting see data source class blind semantic representations target class paper study generalize zero shoot learn assume accessible target class unseen data train prediction unseen data make search source target class propose novel deep calibration network dcn approach towards generalize zero shoot learn paradigm enable simultaneous calibration deep network confidence source class uncertainty target class approach map visual feature image semantic representations class prototypes common embed space compatibility see data source target class maximize show superior accuracy approach state art benchmark datasets generalize zero shoot learn include awa cub sun apy',\n",
              "  'paper propose conceptually simple general framework call metagan shoot learn problems state art shoot classification model integrate metagan principled straightforward way introduce adversarial generator condition task augment vanilla shoot classification model ability discriminate real fake data argue gin base approach help shoot classifiers learn sharper decision boundary could generalize better show metagan framework extend supervise shoot learn model naturally cope unsupervised data different previous work semi supervise shoot learn algorithms deal semi supervision sample level task level give theoretical justifications strength metagan validate effectiveness metagan challenge shoot image classification benchmarks'],\n",
              " ['adapt ideas underlie success deep learn continuous action domain present actor critic model free algorithm base deterministic policy gradient operate continuous action space use learn algorithm network architecture hyper parameters algorithm robustly solve simulate physics task include classic problems cartpole swing dexterous manipulation legged locomotion car drive algorithm able find policies whose performance competitive find plan algorithm full access dynamics domain derivatives demonstrate many task algorithm learn policies end end directly raw pixel input',\n",
              "  'consider problem online learn linear contextual bandits set also strong individual fairness constraints govern unknown similarity metric constraints demand select similar action individuals approximately equal probability dhprz may odds optimize reward thus model settings profit social policy tension assume learn unknown mahalanobis similarity metric weak feedback identify fairness violations quantify extent intend represent interventions regulator know unfairness see nevertheless enunciate quantitative fairness metric individuals main result algorithm adversarial context set number fairness violations depend logarithmically obtain optimal sqrt regret bind best fair policy',\n",
              "  'recurrent network spike neurons rsnns underlie astound compute learn capabilities brain compute learn capabilities rsnn model remain poor least comparison anns address two possible reason one rsnns brain randomly connect design accord simple rule start learn tabula rasa network rather rsnns brain optimize task evolution development prior experience detail optimization process largely unknown functional contribution approximate powerful optimization methods backpropagation time bptt'],\n",
              " ['describe iterative procedure optimize policies guarantee monotonic improvement make several approximations theoretically justify procedure develop practical algorithm call trust region policy optimization trpo algorithm similar natural policy gradient methods effective optimize large nonlinear policies neural network experiment demonstrate robust performance wide variety task learn simulate robotic swim hop walk gaits play atari game use image screen input despite approximations deviate theory trpo tend give monotonic improvement little tune hyperparameters',\n",
              "  'suppose design matrix linear regression problem give response point hide unless explicitly request goal sample small number responses produce weight vector whose sum square loss point epsilon time minimum small jointly sample diverse subsets point crucial one method call volume sample unique desirable property weight vector produce unbiased estimate optimum therefore natural ask method offer optimal unbiased estimate term number responses need achieve epsilon loss approximation',\n",
              "  'consider problem learn optimal reserve price repeat auction non myopic bidders may bid strategically order gain future round even single round auction truthful previous algorithms empirical price provide non trivial regret round set general introduce algorithms obtain small regret non myopic bidders either market large bidder appear constant fraction round bidders impatient discount future utility factor mildly bound away one approach carefully control information reveal bidder build techniques differentially private online learn well recent line work jointly differentially private algorithms',\n",
              "  'duplicate removal critical step accomplish reasonable amount predictions prevalent proposal base object detection frameworks albeit simple effective previous algorithms utilize greedy process without make sufficient use properties input data work design new two stage framework effectively select appropriate proposal candidate object first stage suppress easy negative object proposals second stage select true positives reduce proposal set two stag share network structure encoder decoder form recurrent neural network rnn global attention context gate encoder scan proposal candidates sequential manner capture global context information feed decoder extract optimal proposals extensive experiment propose method outperform alternatives large margin'],\n",
              " ['humans routinely retrace path novel environment forward backwards despite uncertainty motion paper present approach give demonstration path first network generate path equip second network observe world decide act order retrace path noisy actuation change environment two network optimize end end train time evaluate method two realistic simulators perform path follow forward backwards experiment show approach outperform classical approach solve task well number baselines',\n",
              "  'significant interest able predict crimes happen example aid efficient task police protective measure aim model temporal spatial dependencies often exhibit violent crimes order make predictions temporal variation crimes typically follow pattern familiar time series analysis spatial pattern irregular vary smoothly across area instead find spatially disjoint regions exhibit correlate crime pattern indeterminate inter region correlation structure along low count discrete nature count serious crimes motivate propose forecast tool particular propose model crime count region use integer value first order autoregressive process take bayesian nonparametric approach flexibly discover cluster region specific time series describe account covariates within framework approach adjust seasonality demonstrate approach analysis weekly report violent crimes washington forecast outperform standard methods additionally provide useful tool prediction intervals',\n",
              "  'generative recurrent neural network quickly train unsupervised manner model popular reinforcement learn environments compress spatio temporal representations world model extract feature feed compact simple policies train evolution achieve state art result various environments also train agent entirely inside environment generate internal world model transfer policy back actual environment interactive version paper available https worldmodels github',\n",
              "  'decision tree random forest well establish model offer good predictive performance also provide rich feature importance information practitioners often employ variable importance methods rely impurity base information methods remain poorly characterize theoretical perspective provide novel insights performance methods derive finite sample performance guarantee high dimensional set various model assumptions demonstrate effectiveness impurity base methods via extensive set simulations',\n",
              "  'several large scale deployments differential privacy use collect statistical information users however deployments periodically recollect data recompute statistics use algorithms design single use result systems provide meaningful privacy guarantee long time scale moreover exist techniques mitigate effect apply local model differential privacy systems use'],\n",
              " ['propose novel flexible anchor mechanism name metaanchor object detection frameworks unlike many previous detectors model anchor via predefined manner metaanchor anchor function could dynamically generate arbitrary customize prior box take advantage weight prediction metaanchor able work anchor base object detection systems retinanet compare predefined anchor scheme empirically find metaanchor robust anchor settings bound box distributions addition also show potential transfer task experiment coco detection task show metaanchor consistently outperform counterparts various scenarios'],\n",
              " ['accurately answer question give image require combine observations general knowledge effortless humans reason general knowledge remain algorithmic challenge advance research direction novel reason correct answer jointly consider entities show challenge fvqa dataset lead improvement accuracy around compare state art'],\n",
              " ['convolutional neural network cnns recently achieve great success single image super resolution sisr however methods tend produce smooth output miss textural detail solve problems propose super resolution cliquenet srcliquenet reconstruct high resolution image better textural detail wavelet domain propose srcliquenet firstly extract set feature map low resolution image clique block group send set feature map clique sample module reconstruct image clique sample module consist four sub net predict high resolution wavelet coefficients four sub band since consider edge feature properties four sub band four sub net connect others learn coefficients four sub band jointly finally apply inverse discrete wavelet transform idwt output four sub net end clique sample module increase resolution reconstruct image extensive quantitative qualitative experiment benchmark datasets show method achieve superior performance state art methods',\n",
              "  'recurrent neural network powerful tool understand model computation representation populations neurons continuous variable rate model network analyze apply extensively purpose however neurons fire action potentials discrete nature spike important feature neural circuit dynamics despite significant advance train recurrently connect spike neural network remain challenge present procedure train recurrently connect spike network generate dynamical pattern autonomously produce complex temporal output base integrate network input model physiological data procedure make use continuous variable network identify target train input spike model neurons surprisingly able construct spike network duplicate task perform continuous variable network relatively minor expansion number neurons approach provide novel view significance appropriate use fire rate model useful approach build model spike network use address important question representation computation neural systems',\n",
              "  'recent years supervise learn convolutional network cnns see huge adoption computer vision applications comparatively unsupervised learn cnns receive less attention work hope help bridge gap success cnns supervise learn unsupervised learn introduce class cnns call deep convolutional generative adversarial network dcgans certain architectural constraints demonstrate strong candidate unsupervised learn train various image datasets show convince evidence deep convolutional adversarial pair learn hierarchy representations object part scenes generator discriminator additionally use learn feature novel task demonstrate applicability general image representations',\n",
              "  'despite remarkable advance image synthesis research exist work often fail manipulate image context large geometric transformations synthesize person image condition arbitrary pose one representative examples generation quality largely rely capability identify model arbitrary transformations different body part current generative model often build local convolutions overlook key challenge heavy occlusions different view dramatic appearance change distinct geometric change happen part cause arbitrary pose manipulations paper aim resolve challenge induce geometric variability spatial displacements via new soft gate warp generative adversarial network warp gin compose two stag first synthesize target part segmentation map give target pose depict region level spatial layouts guide image synthesis higher level structure constraints warp gin equip soft gate warp block learn feature level map render textures original image generate segmentation map warp gin capable control different transformation degrees give distinct target pose moreover propose warp block light weight flexible enough inject network human perceptual study quantitative evaluations demonstrate superiority warp gin significantly outperform exist methods two large datasets',\n",
              "  'people belong multiple communities word belong multiple topics book cover multiple genres overlap cluster commonplace many exist overlap cluster methods model person word book non negative weight combination exemplars belong solely one community small noise geometrically person point cone whose corner exemplars basic form encompass widely use mix membership stochastic blockmodel network degree correct variants well topic model lda show simple one class svm yield provably consistent parameter inference model scale large datasets experimental result several simulate real datasets show algorithm call svm cone accurate scalable'],\n",
              " ['convolutional neural network cnns inherently subject invariable filter aggregate local input topological structure cause cnns allow manage data euclidean grid like structure image ones non euclidean graph structure traffic network broaden reach cnns develop structure aware convolution eliminate invariance yield unify mechanism deal euclidean non euclidean structure data technically filter structure aware convolution generalize univariate function capable aggregate local input diverse topological structure since infinite parameters require determine univariate function parameterize filter number learnable parameters context function approximation theory replace classical convolution cnns structure aware convolution structure aware convolutional neural network sacnns readily establish extensive experiment eleven datasets strongly evidence sacnns outperform current model various machine learn task include image classification cluster text categorization skeleton base action recognition molecular activity detection taxi flow prediction',\n",
              "  'paper introduce versatile filter construct efficient convolutional neural network consider demand efficient deep learn techniques run cost effective hardware number methods develop learn compact neural network work aim slim filter different ways investigate small sparse binarized filter contrast treat filter additive perspective series secondary filter derive primary filter secondary filter inherit primary filter without occupy storage unfold computation could significantly enhance capability filter integrate information extract different receptive field besides spatial versatile filter additionally investigate versatile filter channel perspective new techniques general upgrade filter exist cnns experimental result benchmark datasets neural network demonstrate cnns construct versatile filter able achieve comparable accuracy original filter require less memory flop',\n",
              "  'learn solve complex sequence task leverage transfer avoid catastrophic forget remain key obstacle achieve human level intelligence progressive network approach represent step forward direction immune forget leverage prior knowledge via lateral connections previously learn feature evaluate architecture extensively wide variety reinforcement learn task atari maze game show outperform common baselines base pretraining finetuning use novel sensitivity measure demonstrate transfer occur low level sensory high level control layer learn policy',\n",
              "  'introduce approach convert mono audio record video camera spatial audio representation distribution sound full view sphere spatial audio important component immersive video view spatial audio microphones still rare current video production system consist end end trainable neural network separate individual sound source localize view sphere condition multi modal analysis audio video frame introduce several datasets include one film one collect wild youtube consist videos upload spatial audio train grind truth spatial audio serve self supervision mix mono track form input network use approach show possible infer spatial localization sound base synchronize video mono audio track'],\n",
              " ['propose simple yet effective approach spatiotemporal feature learn use deep dimensional convolutional network convnets train large scale supervise video dataset find three fold convnets suitable spatiotemporal feature learn compare convnets homogeneous architecture small convolution kernels layer among best perform architectures convnets learn feature namely convolutional simple linear classifier outperform state art methods different benchmarks comparable current best methods benchmarks addition feature compact achieve accuracy ucf dataset dimension also efficient compute due fast inference convnets finally conceptually simple easy train use',\n",
              "  'softmax output activation function model categorical probability distributions many applications deep learn however recent study reveal softmax bottleneck representational capacity neural network language model softmax bottleneck paper propose output activation function break softmax bottleneck without additional parameters analyze softmax bottleneck perspective output set log softmax identify cause softmax bottleneck basis analysis propose sigsoftmax compose multiplication exponential function sigmoid function sigsoftmax break softmax bottleneck experiment language model demonstrate sigsoftmax mixture sigsoftmax outperform softmax mixture softmax respectively'],\n",
              " ['real world applications education effective teacher adaptively choose next example teach base learner current state however exist work algorithmic machine teach focus batch set adaptivity play role paper study case teach consistent version space learners interactive set time step teacher provide example learner perform update teacher observe learner new state highlight adaptivity speed teach process consider exist model version space learners worst case model learner pick next hypothesis randomly version space preference base model learner pick hypothesis accord global preference inspire human teach propose new model learner pick hypotheses accord local preference define current hypothesis show model exhibit several desirable properties adaptivity play key role learner transition hypotheses smooth interpretable develop adaptive teach algorithms demonstrate result via simulation user study']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7bUllVJDnoC"
      },
      "source": [
        "# Just appending all the sentences of a cluster into 1 string\n",
        "\n",
        "appended_final_list = []\n",
        "for i in final_list:\n",
        "    result= ''\n",
        "    for element in i:\n",
        "        result += str(element)\n",
        "    appended_final_list.append(result)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC_uEWBQXMg1"
      },
      "source": [
        "appended_final_list = ['although various machine learn base predictors develop estimate protein protein interactions performances vary dataset species affect two primary aspects choice learn algorithm representation protein pair improve performance predict protein protein interactions exploit synergy multiple learn algorithms utilize expressiveness different protein pair feature develop stack generalization scheme integrate five learn algorithms also design three type protein pair feature base physicochemical properties amino acids gene ontology annotations interaction network topologies test publish datasets collect eight species propose approach achieve significantly higher comparable overall performance compare seven competitive predictors introduce ensemble learn approach ppi prediction integrate multiple learn algorithms different protein pair representations extensive comparisons state art prediction tool demonstrate feasibility superiority propose methodbackground increase amount structure data publish use principles link data mean likely find resources web data describe real life concepts however discover resources relate give resource still open research area thesis study recommender systems use link data source generate recommendations exploit large amount available resources relationships among aim main objective study propose recommendation tech nique resources consider semantic relationships concepts link data specific objectives define semantic relationships derive resources take account knowledge find link data datasets determine semantic similarity measure base semantic relationships derive resources iii propose algorithm dynami cally generate automatic rank resources accord define similarity measure methodology base recommendations project management institute integral model engineer professionals universidad del cauca first one manage project second one develop experimental prototype accordingly main phase conceptual base generation identify main problems objectives project scope systematic literature review conduct phase highlight relationships similarity measure among resources link data main issue feature type base link data solution development design develop experimental prototype test algorithms study thesis result main result obtain first systematic literature view base link data framework execute alyze recommendation algorithms base link data iii dynamic algorithm resource recommendation base knowledge link data relationships comparative study algorithms base link data two implementations propose framework one graph base algorithms machine learn algorithms application framework various scenarios demonstrate feasibility within context real applications conclusions propose framework demonstrate useful develop ing evaluate different configurations algorithms create novel base link data suitable users requirements applications domains contexts layer architecture propose framework also useful towards ',\n",
        " 'advent call big data paradigm motivate flurry research aim enhance machine learn model follow diverse approach context work focus automatic construction feature supervise learn problems differ conventional selection feature new characteristics enhance predictive power infer original dataset particular manuscript propose new iterative feature construction approach base self learn meta heuristic algorithm harmony search solution encode strategy correspondingly cartesian genetic program suit represent combinations feature mean constant length solution vectors propose feature construction algorithm coin adaptive cartesian harmony search achs incorporate modifications allow exploit estimate predictive importance intermediate solutions ultimately attain better convergence rate iterative learn procedure performance propose achs scheme assess compare render state art toy example three practical use case literature excellent performance figure obtain problems shed light widespread applicability propose scheme supervise learn legacy datasets compose already refine characteristics work fund part basque government elkartek program bid project grant ref',\n",
        " 'robot motion plan one central problems robotics receive considerable amount attention roboticists also control artificial intelligence communities despite different type applications physical properties robotic systems many high level task autonomous systems decompose subtasks require point point navigation avoid infeasible regions due obstacles workspace dissertation aim develop new class sample base motion plan algorithms fast efficient asymptotically optimal employ ideas machine learn dynamic program first interpret robot motion plan problem form machine learn problem since underlie search space know priori utilize random geometric graph compute consistent discretizations underlie continuous search space integrate exist algorithms algorithms framework sample base algorithms better exploitation exploration respectively introduce novel sample base algorithm call rrt improve upon well know rrt algorithm leverage value policy iteration methods new information collect propose algorithms yield provable guarantee correctness completeness asymptotic optimality also develop adaptive sample strategy consider exploration classification regression problem use online machine learn algorithms learn relevant region query region contain optimal solution without significant computational overhead extend application sample base algorithms class stochastic optimal control problems problems differential constraints specifically introduce path integral rrt algorithm solve optimal control stochastic systems rrt algorithm use close loop prediction trajectory generation differential systems one key benefit rrt many systems give low level track controller easier handle differential constraints complex steer procedures need unlike exist kinodynamic sample base algorithms implementation result sample base planners route plan full scale autonomous helicopter autonomous aerial cargo utility system program aacus program providework deal aspects support vector machine learn large scale data mine task base decomposition algorithm support vector machine train run serial well share memory parallel mode introduce transformation train data allow usage expensive generalize kernel without additional cost present experiment gaussian kernel usage kernel function possible order speed decomposition algorithm analyze critical problem work set selection large train data set addition analyze influence work set size onto scalability parallel decomposition scheme test conclusions lead several modifications algorithm improvement overall support vector machine learn performance method allow use extensive parameter search methods optimize classification accuracyconditional exponential model one widely use conditional model machine learn field improve iterative scale iis one major algorithms find optimal parameters conditional exponential model paper propose faster iterative algorithm name fis able find optimal parameters faster iis algorithm theoretical analysis show propose algorithm yield tighter bind traditional iis algorithm empirical study text classification three different datasets show new iterative scale algorithm converge substantially faster iis algorithm conjugate gradient algorithm furthermore examine quality optimal parameters find learn algorithm case incomplete train experiment show limit amount computation allow convergence achieve new algorithm fis able obtain lower test errors iis method methodpaper highlight new opportunities design large scale machine learn ing systems consequence blur traditional boundaries allow algorithm designers application level practitioners stay part oblivious detail underlie hardware level implementations hardware software design methodology advocate hinge deploy ment compute intensive machine learn kernels onto compute platforms trade determinism computation improvement speed energy efficiency achieve revisit digital stochastic circuit approximate matrix computations ubiquitous machine learn algorithms theoret ical empirical evaluation undertake assess impact hardware induce computational noise algorithm performance proof concept stochastic hardware simulator employ train deep neural network image recognition problemsdescribe software system enable tight integration vision control modules complex high dof humanoid robots demonstrate icub humanoid robot perform visual object detection reach grasp action key capability system reactive avoidance obstacle object detect video stream carry reach grasp task subsystems architecture independently improve update example show use machine learn techniques improve visual perception collect image robot interaction environment describe task software design constraints lead layer modular system architectureelectrical medical record restrict difficult centralize machine learn model train due privacy regulatory issue one solution train model distribute manner involve many party process however sometimes certain party trustable project aim propose alternative method traditional federate learn central analyzer order conduct train situation without trustable central analyzer propose algorithm call federate machine learn anonymous random hybridization abbreviate fearh use mainly hybridization algorithm degenerate integration connections medical record data model parameters add randomization parameter set share party base experiment new algorithm similar aucroc aucpr result compare machine learn centralize manner original federate machine learncurrently optimization reconfigurable design parameters typically do manually often involve substantial amount effort main focus thesis reduce effort designer focus implementation design correctness leave tool carry optimization address thesis make three main contributions udfirst present initial investigation reconfigurable design optimization machine learn optimizer mlo algorithm algorithm base surrogate model technology particle swarm optimization use surrogate model long hardware generation time mitigate automatic optimization possible first time best knowledge show model predict hardware generation fail well design perform udsecond introduce new algorithm call automatic reconfigurable design efficient global optimization ardego base efficient global optimization ego algorithm compare mlo support parallelism use simpler optimization loop ardego algorithm use multiple optimization compute nod optimization speed greatly improve relative mlo hardware generation time random nature two similar configurations take vastly different amount time generate make parallelization complicate novelty efficient use optimization compute nod achieve extension asynchronous parallel ego algorithm constrain problems udthird show result design synthesis benchmarking reuse design port different platform code revise achieve new auto transfer algorithm methodology make best use available synthesis benchmarking result novel contribution design automation reconfigurable systems open accesconditional exponential model one widely use conditional model machine learn field improve iterative scale iis one major algorithms find optimal parameters conditional exponential model paper propose faster iterative algorithm name fis able find optimal parameters faster iis algorithm theoretical analysis show propose algorithm yield tighter bind traditional iis algorithm empirical study text classification three different datasets show new iterative scale algorithm converge substantially faster iis algorithm conjugate gradient algorithm furthermore examine quality optimal parameters find learn algorithm case incomplete train experiment show limit amount computation allow convergence achieve new algorithm fis able obtain lower test errors iis method methodmachine learn model currently deploy variety real world applications model predictions use make decisions healthcare bank loan numerous critical task deployment artificial intelligence technologies become ubiquitous unsurprising adversaries begin develop methods manipulate machine learn model advantage visual analytics community develop methods open black box machine learn model little work focus help user understand model vulnerabilities context adversarial attack paper present visual analytics framework explain explore model vulnerabilities adversarial attack framework employ multi faceted visualization scheme design support analysis data poison attack perspective model data instance feature local structure demonstrate framework two case study binary classifiers illustrate model vulnerabilities respect vary attack strategiesgeometric perspective nonlinear binary classification algorithms include stateof art versions support vector machine svm radial basis function network rbfn classifiers base idea reconstruct indicator function pro pose instead use reconstruction sign distance function sdf basis binary classification discuss properties sign distance function exploit inclassification algorithms develop simple versions classifiers test several linear nonlinear problems linear test accuracy new algorithm exceed ofstandard svm methods average fewer misclassifications performance new methods also match exceed standard methods several nonlinear problemsincluding classification benchmark diagnostic micro array data set machine learn microarray data introduction binary classification basic problem machine learn applications many field binary classification many potential direct applications also basis many multi category classification methods particular interest applications biology medicine availability micro array proteomic data set contain thousands even tens thousands measurements particularly make important develop good classification algorithms since reliable use data could presumably revolutionize diagnostic medicine several binary classification algorithms develop study intensely past years notable among support vector machine svm methods use radial basis function function kernels svm methods show perform reasonably well classify micro array data demonstrate extraction useful information large data set feasiblepaper propose two new neuro fuzzy scheme one classification one cluster problems classification scheme base simpson fuzzy min max method relax assumptions make enable scheme handle mutually nonexclusive class neuro fuzzy cluster scheme multiresolution algorithm model mechanics human pattern recognition also present data exhaustive comparison techniques neural statistical machine learn traditional approach pattern recognition applications data set use comparisons include machine learn repository university california irvine find propose scheme compare quite well exist techniques addition offer advantage one pass learn online adaptationrecurrent neural network rnns widely use computational neuroscience machine learn ing applications rnn neuron com put output nonlinear function tegrated input importance rnns especially model brain process undis put also widely acknowledge com putations standard rnn model may simplification real neuronal network com pute suggest rnn approach may make neurobiologically plausi ble computationally powerful sion bayesian inference techniques non linear dynamical systems scheme use rnn generative model dynamic input cause environment speech kine matics give generative rnn model rive bayesian update equations decode output critically update define recogniz ing rnn rrnn neurons compute exchange prediction prediction error message rrnn several desirable feature con ventional rnn example fast cod dynamic stimuli robustness ini tial condition noise furthermore imple ments predictive cod scheme dynamic put suggest bayesian inversion recurrent neural network may useful model brain function machine learn tool illustrate use rrnn plication online decode recognition human kinematicspaper present detail evaluation set well know machine learn classifiers front dynamic non deterministic software anomalies system state prediction base monitor system metrics allow software proactive rejuvenation trigger automat ically random forest approach achieve validation errors less comparison well know algorithms avaluation order reduce automatically number monitor parameters need predict software anomalies analyze lasso regularization technique jointly machine learn ing classifiers evaluate prediction accuracy could guarantee within acceptable threshold allow reduce drastically around best case number monitor parameters framework base lasso regularization techniques validate use commerce environment apache tomcat server mysql database servercrucial part safety critical systems development identify system behaviours lead accidents complex systems hard need consider interaction multiple simultaneous failures normal system behaviours multi agent simulation provide way explore behaviour emerge model system normal failure condition however easy generate vast amount data simulations hard comprehend interpret machine learn apply help understand pattern implicit data paper first describe concept hazard analysis perform use multi agent simulation describe machine learn techniques use extract rule simulation output approach illustrate use military system systems case studyrecent advance sensor compute technology possible use real time machine learn techniques monitor state manufacture machine however make accurate predictions raw sensor data still difficult challenge work data process pipeline develop predict condition mill machine tool use raw sensor data acceleration audio time series sensor data aggregate block correspond individual cut operations computer numerical control cnc mill machine block data preprocessed use well know computationally efficient signal process techniques novel kernel function propose approximate covariance preprocessed block time series data several gaussian process regression model train predict tool condition different covariance kernel function model novel covariance function outperform model use common covariance function train model express use predictive model markup language pmml possible demonstrate predictive model component pipeline represent standardize form tool condition model show accurate especially predict condition lightly wear tool',\n",
        " 'large datasets big data become ubiquitous cause potential value derive insights data across wide range business scientific applications increasingly recognize data growth accom panied rapid adoption large elastic multi tenant compute cluster compute cloud lead virtu ous cycle scalability cloud compute make possi ble analyze ever larger datasets proliferation big data lead adoption cloud compute particular machine learn one foundational dis ciplines data analysis summarization inference big data become routine organizations erate large cloud usually base systems hadoop support mapreduce program paradigm widely recognize mapreduce highly scal able suffer critical weakness machine learn ing support iteration consequently one program around limitation lead fragile inef ficient code reliance programmer inher ently flaw multi tenant cloud environment since programmer visibility state system program execute prior work seek address problem either develop spe cialized systems aim stylize applications aug menting mapreduce hoc support save state across iterations drive external loop paper advocate support loop first class construct propose extension mapreduce program paradigm call iterative mapreduce develop optimizer class iterative mapreduce program cover machine learn techniques provide theoretical justifications key optimization step empirically demonstrate system optimize program significant machine learn task competitive state art specialize solutionspresent new frank wolfe type algorithm applicable tominimization problems nonsmooth convex objective provide convergencebounds show scheme yield call coreset result variousmachine learn problems include median balance development sparse pca graph cut ell norm regularize support vector machine svm among others mean algorithm provide approximate solutions tothese problems time complexity bound dependent size ofthe input problem framework motivate grow body work onsublinear algorithms various data analysis problems entirelydeterministic make use smooth proximal operators apart fromthese theoretical result show experimentally algorithm verypractical case also offer significant computational advantage onlarge problem instance provide open source implementation beadapted problems fit overall structurecurrent age fourth industrial revolution industry digital world wealth data internet things iot data cybersecurity data mobile data business data social media data health data etc intelligently analyze data develop correspond smart automate applications knowledge artificial intelligence particularly machine learn key various type machine learn algorithms supervise unsupervised semi supervise reinforcement learn exist area besides deep learn part broader family machine learn methods intelligently analyze data large scale paper present comprehensive view machine learn algorithms apply enhance intelligence capabilities application thus study key contribution explain principles different machine learn techniques applicability various real world application domains cybersecurity systems smart cities healthcare commerce agriculture many also highlight challenge potential research directions base study overall paper aim serve reference point academia industry professionals well decision makers various real world situations application areas particularly technical point viewindustry aim make collaborative robotics accessible effective inside factories human robot interaction enhance mean advance perception systems allow flexible reliable production one contenders challenge intent improve cooperation industry within competition develop novel visual servoing system base machine learn technique automation wind copper wire production electric motor image base visual servoing systems often limit speed image process module run frequency order magnitude lower respect robot control speed article solution problem propose visual servoing function synthesize use gaussian mixture model gmm machine learn system guarantee extremely fast response issue relate data size reduction collection data set need properly train learner discuss performance propose method compare standard visual servoing algorithm use train gmm system develop test path follow application aluminium bar simulate real stator teeth generic electric motor experimental result demonstrate propose method able reproduce visual servoing function minimal error guarantee extremely high work frequencyfast coreset minimum enclose ball kernel algorithm propose first transfer kernel methods center constrain minimum enclose ball problem subsequently train kernel methods use propose meb algorithm primal variables kernel methods recover via kkt condition detail theoretical analysis rigid proof new algorithm give experiment investigate via use several typical classification datasets uci machine learn benchmark datasets moreover performances compare standard support vector machine seriously consider conclude propose algorithm own comparable even superior performances yet rather fast converge speed experiment study paper finally comment exist problems future development directions discuss school hotel tourism management academic research referee referee conference papework deal aspects support vector machine learn large scale data mine task base decomposition algorithm support vector machine train run serial well share memory parallel mode introduce transformation train data allow usage expensive generalize kernel without additional cost present experiment gaussian kernel usage kernel function possible order speed decomposition algorithm analyze critical problem work set selection large train data set addition analyze influence work set size onto scalability parallel decomposition scheme test conclusions lead several modifications algorithm improvement overall support vector machine learn performance method allow use extensive parameter search methods optimize classification accuracybegin machine learn methodology origin artificial intelligence rapidly spread different research communities successful outcomes chapter aim introduce system analysers designers comparatively new statistical supervise machine learn algorithm call support vector machine svm explain two useful areas svm classification regression basic mathematical formulation simple demonstration make easy understand svm prospect challenge future research emerge area also describe future research svm provide improve quality access users therefore develop automate svm system state art technologies paramount importance hence chapter link important step system analysis design perspective evolve research arenasensors existence key component cyber physical systems make susceptible failures due complex environments low quality production age defective sensors either stop communicate convey incorrect information unsteady situations threaten safety economy reliability system objective study construct lightweight machine learn base fault detection diagnostic system within limit energy resources memory computation wireless sensor network wsn paper context aware fault diagnostic cafd scheme propose base ensemble learn algorithm call extra tree evaluate performance propose scheme realistic wsn scenario compose humidity temperature sensor observations replicate extreme low intensity fault six commonly occur type sensor fault consider drift hard bias spike erratic precision degradation stick data loss propose cafd scheme reveal ability accurately detect diagnose low intensity sensor fault timely manner moreover efficiency extra tree algorithm term diagnostic accuracy score roc auc train time demonstrate comparison cut edge machine learn algorithms support vector machine neural networkpaper cluster base machine learn algorithm call cluster algorithm system cas introduce cas algorithm test evaluate performance find fruitful result present heuristics facilitate machine learn author boost research work infobase ministry civil service use analyze cas algorithm cas algorithm compare machine learn algorithms like unimem cobweb classit find strong point propose algorithm combine advantage two different approach machine learn first approach learn examples cas support single multiple inheritance exceptions cas also avoid probability assumptions well understand concept formation second approach learn observation cas apply set operators prove effective conceptual cluster show cas build search cluster hierarchy incorporate characterize objectdissertation present four application drive robotic manipulation task solve use combination feature base machine learn dimensionality reduction optimization techniques first study previously publish image process algorithm whose goal learn classify pixels image consider good bad grasp point exploit ideas behind dimensionality reduction general principal component analysis particular formulate feature selection search space reduction hypotheses provide approach reduce algorithm computation time retain classification accuracy second incorporate image process technique new method compute valid end effector orientations grasp task combination generate unimanual rigid object grasp planner specifically fast accurate three layer hierarchical supervise machine learn framework develop robot kinesthetically teach set valid end effector orientations human loop third solve challenge bimanual regrasping pick place operation require object transfer one manipulator another cast optimization problem objective minimize execution time optimization problem supplement image process unimanual grasp algorithm jointly identify two good grasp point object proper orientations end effector fourth target deformable object solve problem use cooperative manipulators perform towel fold task solve problem new learn algorithm combine imitation reinforcement learn way human demonstrations use reduce search space reinforcement learn algorithm result quick convergence fast learn capabilities collectively task solve dissertation establish application orient feature base machine learn techniques robotics although task different range unimanual bimanual manipulation handle rigid deformable object mathematical frameworks design principles behind implementations similar addition common use feature machine learn dimensionality reduction task commonly design general efficient modular anthropomorphic manipulator end effector sensor independent properties affect choices make algorithms development also allevipaper highlight new opportunities design large scale machinelearning systems consequence blur traditional boundaries haveallowed algorithm designers application level practitioners stay forthe part oblivious detail underlie hardware levelimplementations hardware software design methodology advocate herehinges deployment compute intensive machine learn kernels ontocompute platforms trade determinism computation improvementin speed energy efficiency achieve revisit digitalstochastic circuit approximate matrix computations ubiquitousin machine learn algorithms theoretical empirical evaluation isundertaken assess impact hardware induce computational noise onalgorithm performance proof concept stochastic hardware simulatoris employ train deep neural network image recognition problemspaper propose concept machine learn integrate grid scheme least square support vector machine lssvm call lssvm mix kernel order solve data classification problems purpose lssvm execute feature selections mix kernel applications parameter optimization learn paradigm propose learn paradigm include three step first orthogonal design utilize initialize number input feature candidate parameters store feature randomly select accord first grid acquire first step feature candidate parameters pass lssvm finally artificial bee colony algorithm recently popular heuristic algorithm use optimize parameters lssvm learn illustration evaluation purpose ten remarkable data set university california irvine database use test target experimental result reveal propose lssvm produce classification model easily interpret use small number feature term accuracy hit ratio lssvm significantly outperform methods list paper find imply lssvm promise approach classification explorationemergence competency base train current evaluation scheme surgical skills evolve include newer methods assessment train artificial intelligence machine learn algorithms utilize extensive data set analyze operator performance study aim address question artificial intelligence uncover novel metrics surgical performance support vector machine algorithms train differentiate senior junior participants execute virtual reality hemilaminectomy algorithms achieve good classification performance participants canadian universities divide group accord train level senior junior ask perform virtual reality hemilaminectomy position angle force application simulate burr suction instrument along tissue volumes remove record intervals raw data manipulate create metrics train machine learn algorithms five algorithms include support vector machine train predict whether task perform senior junior participant accuracy algorithm assess leave one cross validation forty one individuals enrol senior junior participants twelve metrics relate safety procedure efficiency motion tool coordination select follow cross validation support vector machine achieve accuracy algorithms achieve accuracy respectively artificial intelligence define novel metrics surgical performance outline train level virtual reality spinal simulation procedure significance result lie potential artificial intelligence complement current educational paradigms better prepare residents surgical proceduresmachine learn one important subfields computer science use solve variety interest artificial intelligence problems different languages framework tool define data need solve machine learn base problems however great number diverse alternatives make difficult intercommunication portability usability definitions design algorithms developer may create paper take first step towards language development environment independent underlie technologies allow developers design solutions solve machine learn base problems simple fast way automatically generate code technologies consider transparent bridge among current technologies rely model drive engineer approach focus creation model definition artifacts underlie technologiesconditional exponential model one widely use conditional model machine learn field improve iterative scale iis one major algorithms find optimal parameters conditional exponential model paper propose faster iterative algorithm name fis able find optimal parameters faster iis algorithm theoretical analysis show propose algorithm yield tighter bind traditional iis algorithm empirical study text classification three different datasets show new iterative scale algorithm converge substantially faster iis algorithm conjugate gradient algorithm furthermore examine quality optimal parameters find learn algorithm case incomplete train experiment show limit amount computation allow convergence achieve new algorithm fis able obtain lower test errors iis method methodthesis consist four chapters chapter focus theoretical result high order laplacian base regularization function estimation study iterate laplacian regularization context supervise learn order achieve nice theoretical properties like thin plate splines good performance complex region like soap film smoother chapter propose innovative static path plan algorithm call within environment full obstacles theoretically show reduce number vertex simulation study approach outperform arm standard heuristic stronger ones true distance heuristics tdh yield faster query time adequate usage memory reasonable preprocessing time chapter propose lpa algorithm extend algorithm context dynamic path plan achieve better performance compare benchmark lifelong plan lpa term robustness worst case computational complexity employ beamlet graphical structure lpa encode information environment hierarchical multiscale fashion therefore produce robust dynamic path plan algorithm chapter focus approach prediction spot electricity spike via combination boost wavelet analysis extensive numerical experiment show approach improve prediction accuracy compare result support vector machine thank fact gradient boost tree method inherit good properties decision tree robustness irrelevant covariates fast computational capability good interpretation phdcommittee chair huo xiaoming committee member deng shi jie committee member shapiro alex committee member tsiotras panagiotras committee member yuan minevolve area cybersecurity present dynamic battlefield cyber criminals security experts intrusions become major concern cyberspace different methods employ tackle threats need ever update traditional methods rudimentary approach manually update blacklist whitelists another method involve manually create rule usually one common methods date lot similar research involve incorporate machine learn artificial intelligence host network base intrusion systems recently originally present problems low accuracy growth area machine learn last decade lead vast improvements machine learn algorithms requirements research apply nearest neighbour fold cross validation random forest machine learn algorithms network base intrusion detection system order improve accuracy intrusion detection system project focus specific feature selection improve increase detection accuracy use fold cross validation algorithm random forest algorithm approximately sample nsl kdd datasetgenetic algorithms random heuristic search rhs algorithms adaptive systems wide range applications search optimisation pattern recognition machine learn well signal process despite widespread use general theory still lack promise approach offer dynamical system model describe stochastic trajectory population dynamics genetic algorithm help underlie deterministic heuristic function fix point however even simple genetic algorithm sga fitnessproportional selection crossover mutation determination population trajectory fix point heuristic function unfeasible practical problem size order simplify mathematical analysis selection introduce paper base strong selection scheme possible derive dynamical system model respective fix point close form addition theoretical analysis experimental result present index term genetic algorithm selection random heuristic search dynamical system modelsupport vector machine popular method machine learn incremental support vector machine algorithm ideal selection face large learn data set paper new incremental support vector machine learn algorithm propose improve efficiency large scale data process model incremental learn algorithm similar standard support vector machine goal concept update incremental learn train procedure include new train data time complexity independent whole train set compare incremental version train speed approach improve change hyperplane reducemachine learn base indoor localization use suffer collection construction maintenance label train databases practical implementation semi supervise learn methods develop efficient indoor localization methods reduce use label train data boost efficiency accuracy indoor localization paper propose new time series semi supervise learn algorithm key aspect develop method distinguish conventional semi supervise algorithms use unlabeled data learn algorithm find spatio temporal relationships unlabeled data pseudolabels generate compensate lack label train data next step another balance optimization learn algorithm learn position model propose method evaluate estimate location smartphone user use receive signal strength indicator rssi measurement experimental result show develop learn algorithm outperform exist semi supervise algorithms accord variation number train data access point also propose method discuss term give better performance analysis impact learn parameters moreover extend localization scheme conjunction particle filter execute include additional information floor planconcept boost emerge field machine learn basicidea boost accuracy weak classify tool combine variousinstances accurate prediction general concept lateradapted field statistical model review article attempt tohighlight evolution boost algorithms machine learn tostatistical model describe adaboost algorithm classification aswell two prominent statistical boost approach gradientboosting likelihood base boost although appraoches typicallytreated separately literature share methodological rootsand follow fundamental concepts compare initial machinelearning algorithms must see black box prediction scheme statistical boost result statistical model offer astraight forward interpretation highlight methodological background andpresent common software implementations work examples andcorresponding code find appendixcore data structure fca formal concept analysis concept lattice widely use field machine learn data mine useful study algorithms build concept lattice practical applications several algorithms build concept lattice develop paper present efficient algorithm name cmcg concept matrix base concepts generation build concept lattice correspond hasse graph base concept matrix novel notion algorithm cmcg find lower neighbor concept use rank attribute concept matrix generate correspond hasse graph validity algorithm prove theory experiment pseudo cod cmcg algorithm give performance cmcg superior one lattice algorithm prove endobjective study apply machine learn algorithms real time personalize wait time prediction emergency departments also aim introduce concept systems think enhance performance prediction model four popular algorithms apply stepwise multiple linear regression artificial neural network iii support vector machine gradient boost machine linear regression model serve baseline model comparison conduct computational experiment base dataset collect emergency department hong kong model diagnostics perform result cross validate four machine learn algorithms use systems knowledge outperform baseline model stepwise multiple linear regression reduce mean square error almost three algorithms similar performances reduce mean square error approximately reductions mean square error due utilization systems knowledge observe multi dimensional stochasticity arise environment impose great challenge wait time prediction introduction concept systems think lead significant enhancements model suggest interdisciplinary efforts could potentially improve prediction performance machine learn algorithms utilization systems knowledge could significantly improve performance wait time prediction wait time prediction less urgent patients challengesequential monte carlo smc method simulation base approach compute posterior distributions smc methods often work well applications consider intractable methods due high dimensionality computationally demand smc implement efficiently fpgas design productivity remain challenge paper introduce design flow generate efficient implementation reconfigurable smc design templating smc structure design flow enable efficient map smc applications multiple fpgas propose design flow consist parametrisable smc computation engine open source software tem plate enable efficient map variety smc design reconfigurable hardware design parameters critical performance solution quality tune use machine learn algorithm base surrogate model experimental result three case study show design performance substantially improve rameter optimisation propose design flow demonstrate capability produce reconfigurable implementations range smc applications significant improvement speed energy efficiency optimise cpu gpu implementations keywords fpga sequential monte carlo machine learn ingpaper review quality sim ilarity measure applications machine learn ing present measure analyze perspective granular compute granular compute allow analyze information differ ent level different approach analysis show measure base two basic aspects universe object granular ity information principle similar problems similar solutions use measure method formulate build relations similar ity relations result use improve machine learn techniqueseffective defect detection still hot issue come software quality assurance static source code analysis play thereby important role since offer possibility automate defect detection early stag development detect defect see classification problem machine learn recently investigate use purpose study present new model automate defect detection mean machine learn ers base static java code feature model comprise extraction necessary feature well application suitable classifiers realize prototype feature extraction study prototype output order identify suitable classifiers finally overall approach evaluate use open source project suitability study evaluation show several classifiers suitable model rotation forest multilayer perceptron jrip classifier make approach effective detect defect accuracy higher although approach comprise prototype show potential become effective alternative nowa days defect detection methodsrecent years grow number searchers investigate performance machine learn ing base traffic classification use statistical properties classification techniques require packet payload inspection techniques assist internet service providers work within legal technical limitations direct payload inspection potential new applications include automate market research automate traffic prioritisation lawful interception many new applications couple flow classifi cation subsequent flow treatment block shape highly desirable develop diffuse extensions exist packet filter provide base traffic classification base statistical properties couple flow classification flow treatment report describe selection exist packet filter extend design overall architecture key components well machine learn techniqueintroductory tutorial paper special session machine learn speak dialogue systems last decade research field speak dialogue sys tems sds experience increase growth yet sign optimization sds consist combin ing speech language process systems automatic speech recognition asr parsers natural language gener ation nlg text speech tts synthesis systems also require development dialogue strategies take least account performances subsystems others nature task form fill tutor robot control database search browse user behaviour cooperativeness expertise due great variability factor reuse previous hand craft design also make difficult reason statistical machine learn ing methods apply automatic sds optimization lead research area last years paper provide short review field recent advanceapproach common machine learn ing literature know relevance feedback learn apply provide method manage agile sensors context machine learn application image retrieval relevance feedback proceed follow user goal image mind retrieve database image learn system system compute image set image display query oftentimes decision image display do use divergence metrics kullback leibler divergence user indicate relevance image goal image system update estimate typically probability mass function database image procedure repeat desire image find method manage agile sensors proceed analogous manner goal system learn number state group move target occupy surveillance region system compute sense action take query base divergence measure call nyi divergence measurement make provide relevance feedback system update probability density number state target procedure repeat time sensor available use show use simulate measurements real record target trajectories method sensor management yield ten fold gain sensor efficiency compare periodic scanchallenge test machine learn applica tions intend learn properties data set correct answer already know absence test oracle one approach test plications use metamorphic test proper tie application exploit define transforma tion function input new output unchanged easily predict base original output output expect defect must exist application seek enumerate classify metamorphic properties machine learn ing algorithms demonstrate apply reveal defect applications interest addition result test present set properties use define metamorphic relationships metamorphic test use general proach test machine learn applicationsweb languages machine learn adaptive communicator heuristics problems program artiftcial intelligence flexible interface mechanism computers user application program thip report consider adaptive communi ator interpose application program set input cutput devices communicator couled devices program transform raw signal low level logical object communicator envision series context analysis systems separate context set operate rule communicate transmit logical object report conclude label direct graph web appropriate data base organization man machine interaction web grammars form pattern replacement rule use manipulate data base pattern replacement rule view heuristics suitable machine learn ing arpa arpenergy prediction machine tool deliver many advantage manufacture enterprise range energy efficient process plan machine tool monitor physics base energy prediction model propose past understand energy usage pattern machine tool however uncertainties machine operate environment make difficult predict energy consumption target machine reliably take advantage opportunity collect extensive contextual energy consumption data discuss data drive approach develop energy prediction model machine tool paper first present methodology efficiently effectively collect process data extract machine tool sensors present data drive model use predict energy consumption machine tool machine generic part specifically use gaussian process regression non parametric machine learn technique develop prediction model energy prediction model generalize multiple process parameters operations finally apply generalize model method assess uncertainty intervals predict energy consume machine part use mori seiki nvd machine tool furthermore model use process plan optimize energy efficiency machine process',\n",
        " 'conditional exponential model one widely use conditional model machine learn field improve iterative scale iis one major algorithms find optimal parameters conditional exponential model paper propose faster iterative algorithm name fis able find optimal parameters faster iis algorithm theoretical analysis show propose algorithm yield tighter bind traditional iis algorithm empirical study text classification three different datasets show new iterative scale algorithm converge substantially faster iis algorithm conjugate gradient algorithm furthermore examine quality optimal parameters find learn algorithm case incomplete train experiment show limit amount computation allow convergence achieve new algorithm fis able obtain lower test errors iis method methodmax product algorithm local message pass scheme attempt tocompute probable assignment map give probability distribution successfully employ method approximate inference forapplications arise cod theory computer vision machine learn however max product algorithm guarantee converge mapassignment guarantee recover map assignment alternative convergent message pass scheme propose overcomethese difficulties work provide systematic study suchmessage pass algorithms extend know result exhibit newsufficient condition convergence local global optima providinga combinatorial characterization optima base graph cover anddescribing new convergent correct message pass algorithm whosederivation unify many know convergent message pass algorithms convergent correct message pass algorithms represent stepforward analysis max product style message pass algorithms theconditions need guarantee convergence global optimum toorestrictive theory practice limitation convergent andcorrect message pass scheme characterize graph cover andillustrated example comment complete rework expansion previous versionone class classification problem often address solve constrain quadratic optimization problem spirit support vector machine paper derive novel one class classification approach investigate original sparsification criterion criterion know coherence criterion base fundamental quantity describe behavior dictionaries sparse approximation problems propose framework allow derive new theoretical result associate coherence criterion one class classification algorithm solve least square optimization problem also provide adaptive update scheme experiment conduct real datasets time series illustrate relevance approach exist methods accuracy computational efficiency index term support vector machine machine learn kernel methods one class classificationunambiguous algorithm add study applicability domain appropriate measure goodness fit robustness represent key characteristics ideally fulfil qsar model consider regulatory purpose paper propose new algorithm rinh base rivality index construction qsar classification model index capable predict activity data set molecules mean measurement rivality nearest neighbor belong different class contribute robust measurement reliability predictions order demonstrate goodness propose algorithm select four independent orthogonally different benchmark data set balance unbalance high low modelable compare result obtain use different machine learn algorithms result validate use data set different balance size corroborate propose algorithm able generate highly accurate classification model contribute valuable measurements reliability predictions applicability domain build modelactive machine learn algorithms use large number unlabeled examples available get label costly require consult human expert many conventional active learn algorithms focus refine decision boundary expense explore new regions current hypothesis misclassifies propose new active learn algorithm balance exploration refine decision boundary dynamically adjust probability explore step experimental result demonstrate improve performance data set require extensive exploration remain competitive data set algorithm also show significant tolerance noisemachine learn invoke imagination many scientific mind due potential solve complex difficult real world problems paper give methods construct machine learn tool use support vector machine svms first give simple example illustrate basic concept demonstrate practical problem practical problem concern electronic monitor fishways automatic count different fish species purpose environmental management australian rivers result illustrate power svm approach sample problem computational attractiveness practical implementations december open accesmachine learn concept incorporate number software devices computer science information industry software devices capable decision make like human brain capability decision make govern artificial intelligence techniques techniques follow many algorithms develop decision make machine learn decision make depend upon profound train contemporary data particular domain data play major important part one element machine learn algorithm main focus paper develop machine learn algorithm help train available medical domain data prepare data model negotiate query achieve analysis different machine learn methodologies like support vector machine svm decision tree recursive partition algorithm model build process new data mine machine learn algorithm propose along performance analysis medical domain dataset analysis indicate data size increase continuous increase algorithm accuracy concurrently time consumption also increase general term classification algorithinternational audiencein many machine learn settings label examples difficult collect unlabeled data abundant also binary classification problems positive examples examples target class available additional data use improve accuracy supervise learn algorithms investigate paper design learn algorithms positive unlabeled data many machine learn data mine algorithms decision tree induction algorithms naive bay algorithms use examples order evaluate statistical query like algorithms kearns design statistical query learn model order describe algorithms design algorithm scheme transform like algorithm algorithm base positive statistical query estimate probabilities set positive instance instance statistical query estimate probabilities instance space prove class learnable statistical query learn model learnable positive statistical query instance statistical query lower bind weight target concept estimate polynomial time design decision tree induction algorithm posc base use positive unlabeled examples give experimental result algorithm case imbalanced class sense one two class say positive class heavily underrepresented compare class remain open problem challenge encounter many real world applicationsdeliver fault free code clear goal devel oper however best method achieve aim still open question despite several approach propose literature exist overall best way one possible solution propose recently combine static source code analysis discipline machine learn ing approach direction define within work implement prototype validate subse quently show possible translation piece source code machine learn algorithm input suitability task fault detection context present work two prototypes veloped show feasibility present idea output generate open source project collect use train rank various machine learn ing classifiers term accuracy false positive false negative rat best among subsequently validate open source project first study least classifiers include multilayerper ceptron ibk adaboost bftree could convince except latter fail completely could validate second study despite prototype show suitability machine learn algorithms static source code analysisterm machine learn refer collection tool use identify pattern data oppose traditional methods pattern identification machine learn tool rely artificial intelligence map patter large amount data self improve new data become available quicker accomplish task review describe various techniques machine learn use past prediction detection management infectious diseases tool bring battle covid addition also discuss applications various stag pandemic advantage disadvantage possible pit falldue latest advancements monitor technologies interest possibility early detection quality issue components grow considerably manufacture industry however implementation techniques limit outside research environment due demand scenarios pose production environments paper propose method assess health machine process machine tool apply range machine learn techniques sensor data aim work provide complete diagnosis condition provide rapid indication machine tool process change beyond acceptable limit make realistic solution production environments prior research author find good visibility simulate failure modes number machine operations machine tool fingerprint routines define sensor suite current research set utilise system streamline test procedure obtain large dataset test techniques upon various supervise unsupervised techniques implement use range feature extract raw sensor signal principal component analysis continuous wavelet transform latter classify use convolutional neural network cnn custom make network pre train network transfer learn detection classification accuracies simulate failure modes across classical cnn techniques test promise approach certain conditionaccord standard procedure build classier use machine learn fully automate process follow preparation train data domain expert contrast interactive machine learn engage users actually generat ing classier oers natural way integrate background knowledge model stage long interactive tool design support cient eective communication paper show appropriate techniques empower users create model compete classiers build state art learn algorithms demonstrate users even users domain experts often construct good classiers without help learn algorithm use simple two dimensional visual interface experiment real data demonstrate surprisingly success hinge domain attribute support good predictions users generate accurate classiers whereas domains many high order attribute interactions favor standard machine learn ing techniques also present articial example domain knowledge allow expert user create much accurate model automatic learn algorithms result indicate system potential produce highly accurate classiers hand domain expert strong interest domain therefore insights partition data moreover small expert dened model oer additional advantage generally intelligible generate automatic techniques',\n",
        " 'paper present integrate hybrid optimization algorithm train radial basis function neural network rbf train neural network still challenge exercise machine learn domain traditional train algorithms general suffer trap local optima lead premature convergence make ineffective apply datasets diverse feature train algorithms base evolutionary computations become popular due robust nature overcome drawbacks traditional algorithms accordingly paper propose hybrid train procedure differential search algorithm functionally integrate particle swarm optimization pso surmount local trap search procedure new population initialization scheme propose use logistic chaotic sequence enhance population diversity aid search capability demonstrate effectiveness propose rbf hybrid train algorithm experimental analysis publicly available benchmark datasets perform subsequently experiment conduct practical application case wind speed prediction expound superiority propose rbf train algorithm term prediction accuracyability witness nonlocal correlations lie core foundational aspects quantum mechanics application process information commonly achieve via violation bell inequalities unfortunately however systematic derivation quickly become unfeasible scenario interest grow complexity cope propose machine learn approach detection quantification nonlocality consist ensemble multilayer perceptrons blend genetic algorithms achieve high performance number relevant bell scenarios show machine learn quantify nonlocality discover new kinds nonlocal correlations inaccessible current methods well also apply framework distinguish classical quantum even postquantum correlations result offer novel method proof principle relevance machine learn understand nonlocality',\n",
        " 'machine learn data mine artificial intelligence big data increasingly apply psychological science although areas research benefit tremendously new set statistical tool often use biological genetic variables hype substantiate traditional areas research argue phenomenon result measurement errors prevent machine learn algorithms accurately model nonlinear relationships indeed exist shortcoming showcased across set simulate examples demonstrate model selection machine learn algorithm regression depend measurement quality regardless sample size conclude set recommendations discussion ways better integrate machine learn statistics traditionally practice psychological sciencealgorithm machine learn algorithm new hybrid intelligent algorithm use solve resources schedule problem new algorithm contain adaptive particle swarm optimization apso algorithm modify genetic algorithm mga machine learn algorithm mga use realize global search apso use get local search choose process depend definite information ant algorithm machine learn principle propose iteration part optimal solution deserve search optimal solution layer simulational result base well know benchmark suit literature show algorithm better optimization performancerobot program demonstration intuitive method program robot programmer show particular task perform use interface device allow measurement record human motion parameters relevant perform demonstrate task paper present analysis learn interaction requirements characteristic rpd system base requirements new system architecture propose support phase interactive program process example task experimental result give keywords program demonstration man machine interface machine learn introduction due cost involve development maintenance robot program automatic program techniques program human demonstration pbd currently attract lot interest moreover learn capabilities become continuously attractivepaper deal concepts expert system data mine belong artificial intelligence field main task expert system ratiocination machine learn algorithm find better optimal solution paper mainly focus diagnose disease effect emu bird mechanism particle swarm optimization pso algorithm artificial bee colony abc algorithm decisive rule database mine could apply expert system thus apply optimization techniques result best global optimize solution keywords optimization global optimal solution machine learn algorithm expert system decisive rulemachine learn branch artificial intelligence increasingly use health research include nurse maternal outcomes research machine learn algorithms complex involve statistics terminology common health research purpose methods paper describe three machine learn algorithms detail provide example use maternal outcomes research three algorithms classification regression tree least absolute shrinkage selection operator random forest may use understand risk group select variables model rank variables contribution outcome respectively machine learn plenty contribute health research also drawbacks discuss well provide example different algorithms function use complete cross sectional study examine association oxytocin total dose exposure primary cesarean section result algorithms compare do find use traditional methodsartificial intelligence broad set sophisticate computer base statistical tool become widely available cardiovascular medicine large data repositories need operational efficiency grow focus precision care set transform artificial intelligence applications range new pathophysiologic discoveries decision support individual patient care optimization system wide logistical process machine learn dominant form artificial intelligence wherein complex statistical algorithms learn deduce pattern datasets supervise machine learn use classify large data train algorithm accurately predict outcome whereas unsupervised machine learn algorithm uncover mathematical relationships within unclassified data artificial multilayered neural network deep learn one successful tool artificial intelligence demonstrate superior efficacy disease phenomapping early warn systems risk prediction automate process interpretation image increase operational efficiency artificial intelligence demonstrate ability learn assimilation large datasets unravel complex relationships discover prior unfound pathophysiological state develop predictive model artificial intelligence need widespread exploration adoption large scale implementation cardiovascular practicemotion sensor model crucial components current algorithms mobile robot localization map model typically provide hand tune human operator often derive intensive careful calibration experiment operator knowledge experience robot operate environment paper demonstrate parameters motion sensor model automatically estimate normal robot operations via machine learn methods thereby eliminate necessity manually tune model laborious calibration process result real world robotic experiment present show effectiveness estimation approachbachelor thesis deal application machine learn algorithms generation check circuit contain detail descriptions individual algorithms machine learn choose achieve purpose except familiarization theoretical properties also part devote specific utilization mention algorithms form classifiers classifiers work different settings influence accuracy learn subsequent classification differences individual classifiers settings illustrate experimental part thesis experiment conduct various circuit include control units robot develop department computer systems faculty information technology brno university technologyinternational audiencein real world robotic applications many factor low level vision motion control parameters high level behaviors determine quality robot performance thus many task robots require fine tune parameters implementation behaviors basic control action well strategic deci sional process recent years machine learn techniques use find optimal parameter set different behaviors however drawback learn techniques time consumption practical applications methods design physical robots must effective small amount data paper present method concurrent learn best strategy optimal parameters extend policy gradient reinforcement learn algorithm result experimental work simulate environment real robot show high convergence ratereal world robotic applications many factor low level vision motion control parameters high level behaviors determine quality robot performance thus many task robots quire fine tune parameters implementation behaviors basic control action well strategic deci sional process recent years machine learn techniques use find optimal parameter set different behaviors however drawback learn techniques time consumption practical applications methods design physical robots must effective small amount data paper present method concurrent learn best strategy optimal parameters extend policy gradient reinforcement learn algorithm result experimental work simulate environment real robot show high convergence rateresearchers discover many successful algorithms methodologies solve problems intersection machine learn education research umbrella category educational data mine enjoy series successes span research process post hoc data analysis generate model use model successful educational interventions however successes arise use pre exist psychological educational construct guess thus use semi supervise fully supervise machine learn algorithms algorithms novel discovery also know unsupervised cluster enjoy significantly fewer successes domain partially education data exhibit unique complex structure thesis mixture algorithm development simulation experimentation real world data design define test novel paradigm cluster education range domains paradigm target cluster revolve around inclusion high level target student learn pre test post test approach differ exist machine learn approach design completely initial concept final execution solve educational research problems take advantage structural complexities problematic algorithms thesis include range data set draw variety research domains include new data experiment psychological sense however thesis include analysis methodology result implications educational research perspective rely entirely education data research problemsnowadays big data environment conventional compute platform base von neumann architecture encounter bottleneck increase requirement computation capability efficiency brain inspire compute neuromorphic compute demonstrate great potential revolutionize technology world consider one promise solutions achieve tremendous compute power efficiency single chip neuromorphic compute systems represent great promise many scientific intelligent applications many design propose realize traditional cmos technology however progress slow recently rebirth neuromorphic compute inspire development novel nanotechnology thesis propose neuromorphic compute systems reram memristor crossbar array include work three major part memristor devices model relate circuit design resistive memory reram technology investigate physical mechanism statistical analysis intrinsic challenge weight sense scheme assign different weight cells different bite line propose area power overhead peripheral circuitry effectively reduce minimize amplitude sneak paths neuromorphic compute system design leverage memristor devices algorithm scale neural network machine learn algorithms base similarity memristive effect biological synaptic behavior first spike neural network snn rate cod model develop algorithm level map hardware design supervise learn addition speed accuracy improvement another neuromorphic system adopt analog input signal different voltage amplitude current sense scheme build moreover use single memristor crossbar neural net work layer explore application specific optimization reliability improvement develop neuromorphic systems thesis impact device failure memristor base neuromorphic compute systems cognitive applications evaluate retrain remapping design algorithm level hardware level develop rescue large accuracy lossquantum compute artificial intelligence combine together may revolutionize future technologies significant school think regard artificial intelligence base generative model propose general quantum algorithm machine learn base quantum generative model prove propose model capable represent probability distributions compare classical generative model exponential speedup learn inference least instance quantum computer efficiently simulate classically result open new direction quantum machine learn offer remarkable example quantum algorithm show exponential improvement classical algorithms important application fieldopus branch bind search algorithm enable efficient systematic search space order search operators apply significant opus achieve maximise effect prune action possible guarantee general case prune shall occur prune possible effect maximise experimental application opus data drive machine learn demonstrate hard search problems possible guarantee solution reasonable time solve real world data within acceptable time frame indeed opus demonstrate enable systematic search extremely large search space less time take common heuristic machine learn search algorithms use systematic search concept learn enable better experimental comparison alternative inductive bias previously possible precise inductive bias describeincreasingly clear machine learn algorithms need integrate iterative scientific discovery loop data query repeatedly mean inductive query computer provide guidance experiment perform chapter summarise several key challenge achieve integration machine learn data mine algorithms methods discovery quantitative structure activity relationships qsars introduce concept robot scientist step discovery process automate discuss representation molecular data knowledge discovery tool analyse discuss adaptation machine learn data mine algorithms guide qsar experimentmodern clinical practice require integration interpretation ever expand volumes clinical data therefore imperative develop efficient ways process understand large amount data neurologists work understand function biological neural network artificial neural network form machine learn algorithm likely increasingly encounter clinical practice use increase clinicians need understand basic principles common type algorithm aim provide coherent introduction jargon heavy subject equip neurologists tool understand critically appraise apply insights burgeon fieldfield data science continue grow anever increase demand tool make machine learn accessible tonon experts paper introduce concept tree base pipelineoptimization automate one tedious part machinelearning pipeline design implement open source tree base pipelineoptimization tool tpot python demonstrate effectiveness aseries simulate real world benchmark data set particular showthat tpot design machine learn pipelines provide significantimprovement basic machine learn analysis require little noinput prior knowledge user also address tendency tpotto design overly complex pipelines integrate pareto optimization whichproduces compact pipelines without sacrifice classification accuracy assuch work represent important step toward fully automate machinelearning pipeline design comment page figure preprint appear gecco edit yet make reviewer commentdata mine machine learn experimental sciences lot insight behaviour algorithms obtain implement study behave run datasets paper present new experimental methodology base concept experiment databases experiment database see special kind inductive database experimental methodology consist fill query database show novel methodology numerous advantage exist one paper present novel interest application inductive databases may significant impact experimental research machine learn data minedata mine machine learn experimental sciences lot insight behaviour algorithms obtain implement study behave run datasets however experiment often extensive systematic ideally would therefore experimental result must interpret caution paper present new experimental methodology base concept experiment databases experiment database see special kind inductive database experimental methodology consist fill query database show novel methodology numerous advantage exist one paper present novel interest application inductive databases may significant impact experimental research machine learn data mine',\n",
        " 'daily task drill laser mark spot weldingapplication cartesian robot request reach hand tip desiredtarget location task become complex handle multiple pointsin shortest travel time space reason study wasconducted primary objective develop computational intelligent systemthat would contribute towards encourage productive quality way materialhandling process objective project design develop andoptimize performance cartesian robotic arm term position andspeed perform spot weld application genetic algorithm beintroduce able look optimum sequence solve path planningvia evolutionary solutions determine best combination paths order tominimize total motion weld time shortest travel distance newalgorithm test implement cartesian robot laser pointer willreplace spot weld torch demonstration purpose project thisproject involve develop machine learn system capable ofperforming independent learn capability give task design anddevelopment project involve two major section first section concernsabout hardware construction wire test second section involvessoftware design control movement robot spot weld thehardware design categorize two aspects electrical design andmechanical design electrical design involve wire control components suchas stepper motor controller input output devices well power supplyand safety devices finally develop algorithm test andimplemented cartesian robot systemrobots employ carry well define task customize static environments often high speed astonish level precision however robots usually depend totally action detail control scheme develop advance line plan phase due recent progress electronics compute power control agent technology computer vision machine learn development autonomous robots capable solve high level deliberate task natural environments approach seriously article provide essential vision learn relate aspects develop autonomous camera equip robot systemsinternet base tele operation systems utilize ubiquitous connectivity low cost bandwidth offer internet send command receive friendly feedback tele operate remote systems many scenarios efficiency task completion tactic coordination human machine multi robots require feasible several hurdle cross include internet type delay uncertainties environment uncertainties object manipulate paper propose general event base adaptive tele operation control mechanism integrate machine learn deal uncertainties mechanism test experiment tele operation soccer robots system power hybrid learn experimental result confirm mechanism present ieee robot automat soc shandong univ chinese univ hong kong chiba inst technol robot soc japan soc instrument control engn japan soc mech engn robiowant think think think think something seymour papert softbot software robot program interact software environment issue command interpret environment feedback softbots much easier build physical robots softbots attractive substrate machine learn agentarchitecture research illustrate point consider rodney unix softbot development university washington rodney use prodigy planner theo frame system represent world model custom build execution sense modules allow rodney interact unix shell real time motivation much recent work machine learn plan influence two theme ffl integrate architectures focus program combine plan learn execution capabilities laird rosenbloom mitchell suttonsmart factory research pace current decade due development many enable technologies tool available developers lead progress cyber physical systems manufacture coin cyber physical production systems ultimate goal domain integrate underlie technologies connect physical plant virtual factory real time improvement product quality process improvements predictive maintenance mass customization well mass production involve technology modules include sensor network machine learn internet things human machine interface augment reality collaborative robotics physical element research micro factory scenario envisage consist high precision micro nano position stage instal tabletop size conventional machine tool collaborative robot handle micro part run machine operations factory devices human worker supervision task due multi faceted technologies involve virtual physical systems simultaneous design strategy follow domains first flexure base micro position axis stage device design instal conventional axis desktop size mill machine secondly work zone consider effective human robot collaboration production area work zone consider social space design safe secure way help integrate devices iot',\n",
        " 'direction walter potter thesis investigate techniques development mobile robot control architec tures work split two main part first part chapters simu lation technique propose potential control architectures could investigate without recourse construct physical robot second part chapter machine learn technique develop could eventually incorporate robot control architecture neither part thesis represent fully complete project rather foundations pursue two separate project could contribute toolbox development intelligent robotsdirection walter potter thesis investigate techniques development mobile robot control architectures work split two main part first part chapters simulation technique propose potential control architectures could investigate without recourse construct physical robot second part chapter machine learn technique develop could eventually incorporate robot control architecture neither part thesis represent fully complete project rather foundations pursue two separate project could contribute toolbox development intelligent robots index word',\n",
        " 'application aco base algorithms data mine grow last years several supervise unsupervised learn algorithms develop use bio inspire approach recent work concern unsupervised learn focus cluster aco base techniques show great potential time new cluster techniques seek continuity data specially focus spectral base approach opposition classical centroid base approach attract increase research interest area still study aco cluster techniques work present hybrid spectral base aco cluster algorithm inspire aco cluster acoc algorithm propose approach combine acoc spectral laplacian generate new search space algorithm order obtain promise solutions new algorithm call sacoc compare well know algorithms mean spectral cluster acoc experiment measure accuracy algorithm synthetic datasets real world datasets extract uci machine learn repositoryproceed international conference ants brussels belgium september final publication available springer via http doi org application aco base algorithms data mine grow last years several supervise unsupervised learn algorithms develop use bio inspire approach recent work concern unsupervised learn focus cluster show great potential aco base techniques work present aco base cluster algorithm inspire aco cluster acoc algorithm propose approach restructure acoc centroid base technique medoid base technique properties search space necessarily know instead rely information distance amongst data new algorithm call macoc compare well know algorithms mean partition around medoids acoc experiment measure accuracy algorithm synthetic datasets real world datasets extract uci machine learn repository work partly support spanish ministryof science education project tin savier airbusdefense space project fuam fuam',\n",
        " 'paper describe methodologies apply result achieve framework esprit basic research action learn project learn one rst project work towards application machine learn techniques elds industrial relevance much complex domains usually treat research particular learn aim ease program robots enhance ability cooperate humans paper give short introduction learn robotics three applications consideration learn afterwards learn methodologies use applications experimental setups result obtain describe general find provide good examples good interface learn performance components crucial success extension quot program demonstration quot paradigm robotics become one key aspects learn',\n",
        " 'currently robot control systems specifically design engineer fine tune particular problems particular robots lead significant waste man hours engineer phd level work implement reimplement adapt controllers similar task different robots result inefficient robotics industry whole thus need automate least semi automate controller reusability arise project investigate hurdle need overcome attain controller universality look possible methods bootstrap controllers different robot sensors actuators case study conduct perform migration controller wheel robot mobile vision system pioneer robot legged robot mobile head mount camera aldebaran nao two robots different modalities make task challenge mean two different robots perform task machine learn methods deploy use artificial neural network ann learn entire sensor decision system robot motor api tree leave sensor feature extraction low level motor control hand engineer method work reasonably well effectively link number controllers design pioneer onto nao sensors actuators preliminary methods provide insight future prospect robots program learn help humansnowadays large amount data automatically generate devices sensors measure instance parameters production process environmental condition transport goods energy consumption smart home traffic volume air pollution water consumption pulse blood pressure individuals collection transmission data enable electronics software sensors network connectivity embed physical object object infrastructure connect object call internet things iot already billion devices connect iot number twice large world population time iot provide data physical environment level detail never know human history understand data create opportunities improve way live learn work entertain instance information obtain data analysis modules embed exist process could help optimization lead sustainable systems save resources sectors manufacture logistics energy utilities public sector healthcare iot inherent distribute nature resource constraints dynamism network participants well amount diverse type data collect challenge even advance automate data analysis methods know today currently strong research focus centralization data cloud process accord paradigm parallel high performance compute however resources devices sensors data generate side might suffice transmit data instance pervasive distribute systems wireless sensors network highly communication constrain stream high throughput applications data mass simply huge send exist communication line like satellite connections hence iot require new generation distribute algorithms resource aware intelligently reduce amount data transmit process throughout analysis chain thesis deal distribute analysis vertically partition sensor measurements communication constraints particularly challenge scenario observations distribute nod feature value learn accurate prediction model may require combination information different nod necessarily lead communication main question design communication efficient algorithms scenario time preserve sufficient accuracy first part ',\n",
        " 'introduce analyze new algorithm linear classification combine rosenblatt perceptron algorithm helmbold warmuth leave one method like vapnik maximal margin classifier algorithm take advantage data linearly separable large margins compare vapnik algorithm however much simpler implement much efficient term computation time also show algorithm efficiently use high dimensional space use kernel function perform experiment use algorithm variants classify image handwritten digits performance algorithm close good performance maximal margin classifiers problem save significantly computation time program effort introduction one influential developments theory machine learn last years vapnik work supp',\n",
        " 'volume include extend revise versions paper present workshop combinations intelligent methods applications cima intend become forum exchange experience ideas among researchers practitioners deal combinations different intelligent methods artificial intelligence aim create integrate hybrid methods benefit components exist present efforts combine soft compute methods fuzzy logic neural network genetic algorithms another stream efforts integrate case base reason machine learn soft compute methods combinations widely explore like neuro symbolic methods neuro fuzzy methods methods combine rule base case base reason cima hold conjunction ieee international conference tool artificial intelligence ictai',\n",
        " 'izbolj evanje obstoje proizvodnje obdelovalnih sistemov zahteva nenehno posodabljanje integracijo najnovej tehnologij proizvodne sisteme proizvodnih spremenljivk edalje tem pove uje mno ica podatkov moramo obdelati velikokrat klasi analiti metode optimizacije odpovedo zaradi tega smo prisiljeni bolje izkoristiti razpolo ljive proizvodne vire zato moramo pose naprednej pristopih evanja problemov evanje zahtevnih problemov edalje pogosteje uporabljajo razli podro umetne inteligence zlasti strojnega enja pregled sedaj opravljenih raziskav pokazal obstoje razviti sistemi precej ozko usmerjeni disertaciji predlagamo popolnoma nov pristop modeliranju cnc obdelav pomo novega gravitacijskega iskalnega algoritma gsa spada med metode skupinske inteligence razviti inteligentni sistem deluje osnovi osnovnih newtonovih fizikalnih zakonov oziroma osnovi interakcij med masnimi telesi prostoru primerjavo potrditev ustreznosti rezultatov doktorske disertacije smo uporabili tudi metodo modeliranja rojem delcev pso primerjava pokazala gsa algoritem primeren modeliranje obdelav odrezovanjem saj odstopanja eksperimentalnih podatkov sprejemljivih mejah dobljeni modeli dobro opisali postopek odrezovanja materiala stru enjem smo uporabili kot postopek odrezovanja posebej velja omeniti gsa algoritem najslab primeru vsaj dvakrat hitrej enakovrednega pso algoritma dobljen model cnc obdelave smo nato uporabili kriterijsko optimiranje obdelovalnih parametrov optimalne hrapavosti obdelane povr ine rezalnih sil asovne obstojnosti orodja vsaka izmed omenjenih odvisnih spremenljivk prispeva optimalnemu delovanju cnc obdelovalnega stroja kar zni uje stro proizvodnje kriterijsko optimiranje smo izvedli pomo nsga algoritma optimiranje smo morali dolo iti tudi omejitve smo dolo ili pomo teoreti nih izra unov jih preverili pomo eksperimentalnih podatkov zaradi obsega dela smo omejili stru enje hkrati delu predstavljene osnove prilagoditev uporabo metod ostalih obdelovalnih strojih saj predlagan pristop ']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hhRbGukWS9t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jVW7VNoa7Vq",
        "outputId": "a264cb5d-1392-43b6-e2f5-0910eb3fb52f"
      },
      "source": [
        "# I had to give the topics of each paper because evaluating abstracts is hard \n",
        "# So this code produces a list with all the clusters, and which can be further used to evaluate the performance of this program\n",
        "\n",
        "import re\n",
        "length2 = len(final_list)\n",
        "topicss = []\n",
        "for i in range(length2):\n",
        "    topicss.append([])\n",
        "for i in range(len(final_list)):\n",
        "    \n",
        "    for j in final_list[i]:\n",
        "        count = 0\n",
        "        for k in range(len(data_list2)):\n",
        "          if j == data_list2[k]:\n",
        "            topicss[i].append(topic_list2[k])\n",
        "          else:\n",
        "            count+=1\n",
        "        \n",
        "       \n",
        "        \n",
        "\n",
        "\n",
        "topicss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Adversarial Scene Editing Automatic Object Removal from Weak Supervision',\n",
              "  'Dialogbased Interactive Image Retrieval',\n",
              "  'Generalizing Graph Matching beyond Quadratic Assignment Model',\n",
              "  'Greedy Hash Towards Fast Optimization for Accurate Hash Coding in CNN',\n",
              "  'HighResolution Image Inpainting using MultiScale Neural Patch Synthesis'],\n",
              " ['A Block Coordinate Ascent Algorithm for MeanVariance Optimization',\n",
              "  'A Deep Bayesian Policy Reuse Approach Against NonStationary Agents',\n",
              "  'A LinearTime Kernel GoodnessofFit Test',\n",
              "  'A Neural Compositional Paradigm for Image Captioning',\n",
              "  'A Reduction of Imitation Learning and Structured Prediction to NoRegret Online Learning',\n",
              "  'A Test of Relative Similarity For Model Selection in Generative Models',\n",
              "  'A Unified Feature Disentangler for MultiDomain Image Translation and Manipulation',\n",
              "  'A2Nets Double Attention Networks',\n",
              "  'Active Learning for NonParametric Regression Using Purely Random Trees',\n",
              "  'Adversarial vulnerability for any classifier',\n",
              "  'An Efficient Pruning Algorithm for Robust Isotonic Regression',\n",
              "  'Are GANs Created Equal A LargeScale Study',\n",
              "  'Are ResNets Provably Better than Linear Predictors',\n",
              "  'AutoEncoding Variational Bayes',\n",
              "  'Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift',\n",
              "  'BatchInstance Normalization for Adaptively StyleInvariant Neural Networks',\n",
              "  'Bayesian Model Selection Approach to Boundary Detection with NonLocal Priors',\n",
              "  'Bayesian Policy Reuse',\n",
              "  'Bayesian Semisupervised Learning with Graph Gaussian Processes',\n",
              "  'Boosted Sparse and LowRank Tensor Regression',\n",
              "  'Boosting Black Box Variational Inference',\n",
              "  'Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search',\n",
              "  'Compound random measures and their use in Bayesian nonparametrics',\n",
              "  'Constructing Deep Neural Networks by Bayesian Network Structure Learning',\n",
              "  'Convolutional LSTM Network A Machine Learning Approach for Precipitation Nowcasting',\n",
              "  'Cooperative Holistic Scene Understanding Unifying 3D Object, Layout, and Camera Pose Estimation',\n",
              "  'Deep Attentive Tracking via Reciprocative Learning',\n",
              "  'Dendritic cortical microcircuits approximate the backpropagation algorithm',\n",
              "  'DifNet Semantic Segmentation by Diffusion Networks',\n",
              "  'Distilling the Knowledge in a Neural Network',\n",
              "  'Distributed Stochastic Optimization via Adaptive SGD',\n",
              "  'DomainInvariant Projection Learning for ZeroShot Recognition',\n",
              "  'Dual Principal Component Pursuit Improved Analysis and Efficient Algorithms',\n",
              "  'DVAE++ Discrete Variational Autoencoders with Overlapping Transformations',\n",
              "  'FrequencyDomain Dynamic Pruning for Convolutional Neural Networks',\n",
              "  'Gaussian Process Conditional Density Estimation',\n",
              "  'Generalized ZeroShot Learning via Synthesized Examples',\n",
              "  'Generalizing Tree Probability Estimation via Bayesian Networks',\n",
              "  'Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization',\n",
              "  'Generative Adversarial Nets',\n",
              "  'Generative Image Inpainting with Contextual Attention',\n",
              "  'GeneticGated Networks for Deep Reinforcement Learning',\n",
              "  'Geometrically Coupled Monte Carlo Sampling',\n",
              "  'Geometry Based Data Generation',\n",
              "  'High Dimensional Linear Regression using Lattice Basis Reduction',\n",
              "  'HitNet Hybrid Ternary Recurrent Neural Network',\n",
              "  'How Does Batch Normalization Help Optimization',\n",
              "  'How to Start Training The Effect of Initialization and Architecture',\n",
              "  'Hybrid Knowledge Routed Modules for Largescale Object Detection',\n",
              "  'Hybrid RetrievalGeneration Reinforced Agent for Medical Image Report Generation',\n",
              "  'Implicit Reparameterization Gradients',\n",
              "  'Improved Training of Wasserstein GANs',\n",
              "  'Informative Features for Model Comparison',\n",
              "  'Intriguing properties of neural networks',\n",
              "  'IntroVAE Introspective Variational Autoencoders for Photographic Image Synthesis',\n",
              "  'Joint Active Feature Acquisition and Classification with VariableSize Set Encoding',\n",
              "  'Kalman Normalization Normalizing Internal Representations Across Network Layers',\n",
              "  'KDGAN Knowledge Distillation with Generative Adversarial Networks',\n",
              "  'Learning Disentangled Joint Continuous and Discrete Representations',\n",
              "  'Learning semantic similarity in a continuous space',\n",
              "  'Learning to Exploit Stability for 3D Scene Parsing',\n",
              "  'Learning to Reconstruct Shapes from Unseen Classes',\n",
              "  'LinkNet Relational Embedding for Scene Graph',\n",
              "  'Lowshot Learning via CovariancePreserving Adversarial Augmentation Networks',\n",
              "  'Matching Networks for One Shot Learning',\n",
              "  'MetaReg Towards Domain Generalization using MetaRegularization',\n",
              "  'Mirrored Langevin Dynamics',\n",
              "  'Mixture Matrix Completion',\n",
              "  'Model Agnostic Supervised Local Explanations',\n",
              "  'Modelling sparsity, heterogeneity, reciprocity and community structure in temporal interaction data',\n",
              "  'Modular Networks Learning to Decompose Neural Computation',\n",
              "  'Multivariate Time Series Imputation with Generative Adversarial Networks',\n",
              "  'Neural Architecture Search with Bayesian Optimisation and Optimal Transport',\n",
              "  'Neural Nearest Neighbors Networks',\n",
              "  'New Insight into Hybrid Stochastic Gradient Descent Beyond WithReplacement Sampling and Convexity',\n",
              "  'Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling',\n",
              "  'NonLocal Recurrent Network for Image Restoration',\n",
              "  'On Learning Markov Chains',\n",
              "  'On the Dimensionality of Word Embedding',\n",
              "  'Optimization for Approximate Submodularity',\n",
              "  'PartiallySupervised Image Captioning',\n",
              "  'Pelee A RealTime Object Detection System on Mobile Devices',\n",
              "  'PointCNN Convolution On XTransformed Points',\n",
              "  'Private Sequential Learning',\n",
              "  'Random Feature Stein Discrepancies',\n",
              "  \"Regularizing by the Variance of the Activations' SampleVariances\",\n",
              "  'Reinforced Continual Learning',\n",
              "  'Removing the Feature Correlation Effect of Multiplicative Noise',\n",
              "  'Rich feature hierarchies for accurate object detection and semantic segmentation',\n",
              "  'Robust PCA via Outlier Pursuit',\n",
              "  'Semantic Scene Completion from a Single Depth Image',\n",
              "  'Supervised autoencoders Improving generalization performance with unsupervised regularizers',\n",
              "  'Symbolic Graph Reasoning Meets Convolutions',\n",
              "  'Synthesized Policies for Transfer and Adaptation across Tasks and Environments',\n",
              "  'TADAM Task dependent adaptive metric for improved fewshot learning',\n",
              "  'The Description Length of Deep Learning models',\n",
              "  'ToddlerInspired Visual Object Learning',\n",
              "  'Toward Multimodal ImagetoImage Translation',\n",
              "  'Trajectory Convolution for Action Recognition',\n",
              "  'Treetotree Neural Networks for Program Translation',\n",
              "  'Unsupervised Learning of Viewinvariant Action Representations',\n",
              "  'Variational Inference A Review for Statisticians',\n",
              "  'Variational Memory EncoderDecoder'],\n",
              " ['Adapted Deep Embeddings A Synthesis of Methods for kShot Inductive Transfer Learning',\n",
              "  'Attention Is All You Need',\n",
              "  'Conditional Adversarial Domain Adaptation',\n",
              "  'Deep Neural Nets with Interpolating Function as Output Activation',\n",
              "  'DVAE Discrete Variational Autoencoders with Relaxed Boltzmann Priors',\n",
              "  'Generative Neural Machine Translation',\n",
              "  'Group Normalization',\n",
              "  'Imagetoimage translation for crossdomain disentanglement',\n",
              "  'IMPALA Scalable Distributed DeepRL with Importance Weighted ActorLearner Architectures',\n",
              "  'SLAYER Spike Layer Error Reassignment in Time',\n",
              "  'Uncertainty Sampling is Preconditioned Stochastic Gradient Descent on ZeroOne Loss',\n",
              "  'Video Prediction via Selective Sampling'],\n",
              " ['QSGD CommunicationEfficient SGD via Gradient Quantization and Encoding',\n",
              "  'A flexible model for training action localization with varying levels of supervision',\n",
              "  'A loss framework for calibrated anomaly detection',\n",
              "  'A simple neural network module for relational reasoning',\n",
              "  'Automatic Program Synthesis of Long Programs with a Learned Garbage Collector',\n",
              "  'Bayesian Pose Graph Optimization via Bingham Distributions and Tempered Geodesic MCMC',\n",
              "  'Block stochastic gradient iteration for convex and nonconvex optimization',\n",
              "  'BottomUp and TopDown Attention for Image Captioning and Visual Question Answering',\n",
              "  'BourGAN Generative Networks with Metric Embeddings',\n",
              "  'Connectionist Temporal Classification with Maximum Entropy Regularization',\n",
              "  'Context Encoders Feature Learning by Inpainting',\n",
              "  'Deep Defense Training DNNs with Improved Adversarial Robustness',\n",
              "  'Deep Functional Dictionaries Learning Consistent Semantic Structures on 3D Models from Functions',\n",
              "  'Deep Residual Learning for Image Recognition',\n",
              "  'Deep Supervised Discrete Hashing',\n",
              "  'Densely Connected Convolutional Networks',\n",
              "  'Discovery of Latent 3D Keypoints via Endtoend Geometric Reasoning',\n",
              "  'Distilled Wasserstein Learning for Word Embedding and Topic Modeling',\n",
              "  'DropMax Adaptive Variational Softmax',\n",
              "  'Efficient Algorithms for Nonconvex Isotonic Regression through Submodular Optimization',\n",
              "  'Efficient Stochastic Gradient Hard Thresholding',\n",
              "  'Embedding Logical Queries on Knowledge Graphs',\n",
              "  'EvolutionGuided Policy Gradient in Reinforcement Learning',\n",
              "  'Explanations based on the Missing Towards Contrastive Explanations with Pertinent Negatives',\n",
              "  'FishNet A Versatile Backbone for Image, Region, and Pixel Level Prediction',\n",
              "  'Foreground Clustering for Joint Segmentation and Localization in Videos and Images',\n",
              "  'FRAGE FrequencyAgnostic Word Representation',\n",
              "  'Graph Neural Networks A Review of Methods and Applications',\n",
              "  'Image Inpainting via Generative Multicolumn Convolutional Neural Networks',\n",
              "  'InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets',\n",
              "  'Interpretable Distribution Features with Maximum Testing Power',\n",
              "  'Isolating Sources of Disentanglement in Variational Autoencoders',\n",
              "  'Learning Deep Embeddings with Histogram Loss',\n",
              "  'Learning longrange spatial dependencies with horizontal gated recurrent units',\n",
              "  'Learning Pipelines with Limited Data and Domain Knowledge A Study in Parsing Physics Problems',\n",
              "  'Learning to Navigate in Cities Without a Map',\n",
              "  'MaximumEntropy Fine Grained Classification',\n",
              "  'MULAN A Blind and OffGrid Method for Multichannel Echo Retrieval',\n",
              "  'MultiClass Learning From Theory to Algorithm',\n",
              "  'MultiTask Learning as MultiObjective Optimization',\n",
              "  'MultiTask Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics',\n",
              "  'Neural Turing Machines',\n",
              "  'NeuralSymbolic VQA Disentangling Reasoning from Vision and Language Understanding',\n",
              "  'On OracleEfficient PAC RL with Rich Observations',\n",
              "  'PAC Reinforcement Learning with Rich Observations',\n",
              "  'PAClearning in the presence of adversaries',\n",
              "  'Parsimonious Quantile Regression of Financial Asset Tail Dynamics via Sequential Learning',\n",
              "  'Posterior Concentration for Sparse Deep Learning',\n",
              "  'Prototypical Networks for Fewshot Learning',\n",
              "  'Quantifying Learning Guarantees for Convex but Inconsistent Surrogates',\n",
              "  'Regularization Learning Networks Deep Learning for Tabular Datasets',\n",
              "  'Rethinking the Inception Architecture for Computer Vision',\n",
              "  'Revisiting MultiTask Learning with ROCK a Deep Residual Auxiliary Block for Visual Detection',\n",
              "  'Simple random search of static linear policies is competitive for reinforcement learning',\n",
              "  'Snap ML A Hierarchical Framework for Machine Learning',\n",
              "  'SplineNets Continuous Neural Decision Graphs',\n",
              "  'Stochastic Nested Variance Reduction for Nonconvex Optimization',\n",
              "  'Structural Causal Bandits Where to Intervene',\n",
              "  'Temporal Regularization for Markov Decision Process',\n",
              "  'TextAdaptive Generative Adversarial Networks Manipulating Images with Natural Language',\n",
              "  'The Pessimistic Limits and Possibilities of Marginbased Losses in Semisupervised Learning',\n",
              "  'Topdown Tree Long ShortTerm Memory Networks',\n",
              "  'Variational Neural Machine Translation',\n",
              "  'VideotoVideo Synthesis',\n",
              "  'Virtual Class Enhanced Discriminative Embedding Learning',\n",
              "  'Visual Object Networks Image Generation with Disentangled 3D Representations',\n",
              "  'Wasserstein AutoEncoders',\n",
              "  'Weight Uncertainty in Neural Networks',\n",
              "  'ZeroShot Transfer with Deictic ObjectOriented Representation in Reinforcement Learning'],\n",
              " ['Quadratic Decomposable Submodular Function Minimization'],\n",
              " ['Nonmonotone Submodular Maximization in Exponentially Fewer Iterations',\n",
              "  'A Practical Algorithm for Distributed Clustering and Outlier Detection',\n",
              "  'Efficient nonmyopic batch active search',\n",
              "  'Fast Similarity Search via Optimal Sparse Lifting',\n",
              "  'GIANT Globally Improved Approximate Newton Method for Distributed Optimization',\n",
              "  'Global Gated Mixture of Secondorder Pooling for Improving Deep Convolutional Neural Networks',\n",
              "  'Hybrid DeterministicStochastic Methods for Data Fitting',\n",
              "  'MetaGradient Reinforcement Learning',\n",
              "  'Multitask Boosting for Survival Analysis with Competing Risks',\n",
              "  'Nondelusional Qlearning and valueiteration',\n",
              "  'Nonparametric learning from Bayesian models with randomized objective functions',\n",
              "  'Outrageously Large Neural Networks The SparselyGated MixtureofExperts Layer',\n",
              "  'Precision and Recall for Time Series',\n",
              "  'Quo Vadis, Action Recognition A New Model and the Kinetics Dataset',\n",
              "  'Relating Leverage Scores and Density using Regularized Christoffel Functions',\n",
              "  'SelfErasing Network for Integral Object Attention',\n",
              "  'Semantically Consistent Regularization for ZeroShot Recognition',\n",
              "  'Stochastic Composite Mirror Descent Optimal Bounds with High Probabilities',\n",
              "  'UncertaintyAware Attention for Reliable Interpretation and Prediction'],\n",
              " ['Attention in Convolutional LSTM for Gesture Recognition',\n",
              "  'Discriminationaware Channel Pruning for Deep Neural Networks',\n",
              "  'Fully Neural Network Based Speech Recognition on Mobile and Embedded Devices',\n",
              "  'Representation Learning for Treatment Effect Estimation from Observational Data'],\n",
              " ['Inhomogeneous Hypergraph Clustering with Applications',\n",
              "  'Revisiting Decomposable Submodular Function Minimization with Incidence Relations',\n",
              "  \"RestKatyusha Exploiting the Solution's Structure via Scheduled Restart Schemes\",\n",
              "  'Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients',\n",
              "  'Accelerated Stochastic Matrix Inversion  General Theory and  Speeding up BFGS Rules for Faster SecondOrder Optimization',\n",
              "  '(Probably) Concave Graph Matching',\n",
              "  'Interactive Structure Learning with Structural QuerybyCommittee',\n",
              "  'Joint Subbands Learning with Clique Structures for Wavelet Domain SuperResolution'],\n",
              " ['Gradient Descent for Spiking Neural Networks',\n",
              "  'An intriguing failing of convolutional neural networks and the CoordConv solution',\n",
              "  'DeepExposure Learning to Expose Photos with Asynchronously Reinforced Adversarial Learning',\n",
              "  'Found Graph Data and Planted Vertex Covers',\n",
              "  'SoftGated WarpingGAN for PoseGuided Person Image Synthesis',\n",
              "  'Solving Large Sequential Games with the Excessive Gap Technique',\n",
              "  'StructureAware Convolutional Neural Networks',\n",
              "  'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks',\n",
              "  'Using FiringRate Dynamics to Train Recurrent Networks of Spiking Model Neurons'],\n",
              " ['SARAH A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient',\n",
              "  'Wasserstein Variational Inference',\n",
              "  'A GameTheoretic Approach to Recommendation Systems with Strategic Content Providers',\n",
              "  \"RestKatyusha Exploiting the Solution's Structure via Scheduled Restart Schemes\",\n",
              "  'Revisiting Decomposable Submodular Function Minimization with Incidence Relations'],\n",
              " ['Least Squares Generative Adversarial Networks',\n",
              "  'HOGWILD A LockFree Approach to Parallelizing Stochastic Gradient Descent',\n",
              "  'Learning Versatile Filters for Efficient Convolutional Neural Networks'],\n",
              " ['An Offpolicy Policy Gradient Theorem Using Emphatic Weightings',\n",
              "  'Online Learning with an Unknown Fairness Metric',\n",
              "  'Visual Memory for Robust Path Following',\n",
              "  'Why so gloomy A Bayesian explanation of human pessimism bias in the multiarmed bandit task'],\n",
              " ['Algorithmic Linearly Constrained Gaussian Processes',\n",
              "  'ShapeNet An InformationRich 3D Model Repository',\n",
              "  'Training DNNs with Hybrid Block Floating Point'],\n",
              " ['Adversarial Examples that Fool both Computer Vision and TimeLimited Humans'],\n",
              " ['(Probably) Concave Graph Matching',\n",
              "  'Mondrian Forests Efficient Online Random Forests'],\n",
              " ['An Overview of Machine Teaching'],\n",
              " [\"Answerer in Questioner's Mind Information Theoretic Approach to GoalOriented Visual Dialog\"],\n",
              " ['Compact Bilinear Pooling'],\n",
              " ['Solving Large Sequential Games with the Excessive Gap Technique',\n",
              "  '3DAware Scene Manipulation via Inverse Graphics',\n",
              "  'Out of the Box Reasoning with Graph Convolution Nets for Factual Visual Question Answering',\n",
              "  'Chain of Reasoning for Visual Question Answering'],\n",
              " ['PacGAN The power of two samples in generative adversarial networks',\n",
              "  'DeepExposure Learning to Expose Photos with Asynchronously Reinforced Adversarial Learning',\n",
              "  'Deep Supervised Hashing with Triplet Labels'],\n",
              " ['Alternating optimization of decision trees, with application to learning sparse oblique trees',\n",
              "  'Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling'],\n",
              " ['Mostly ExplorationFree Algorithms for Contextual Bandits',\n",
              "  'Revisiting Normalized Gradient Descent Fast Evasion of Saddle Points',\n",
              "  'Enhancing the Accuracy and Fairness of Human Decision Making'],\n",
              " ['Fairness Behind a Veil of Ignorance A Welfare Analysis for Automated Decision Making'],\n",
              " ['Generalized ZeroShot Learning with Deep Calibration Network',\n",
              "  'MetaGAN An Adversarial Approach to FewShot Learning'],\n",
              " ['Continuous control with deep reinforcement learning',\n",
              "  'Online Learning with an Unknown Fairness Metric',\n",
              "  'Long shortterm memory and Learningtolearn in networks of spiking neurons'],\n",
              " ['Trust Region Policy Optimization',\n",
              "  'Leveraged volume sampling for linear regression',\n",
              "  'Learning Optimal Reserve Price against Nonmyopic Bidders',\n",
              "  'Sequential Context Encoding for Duplicate Removal'],\n",
              " ['Visual Memory for Robust Path Following',\n",
              "  'SpatioTemporal Low Count Processes with Application to Violent Crime Events',\n",
              "  'Recurrent World Models Facilitate Policy Evolution',\n",
              "  'Variable Importance Using Decision Trees',\n",
              "  'Local Differential Privacy for Evolving Data'],\n",
              " ['MetaAnchor Learning to Detect Objects with Customized Anchors'],\n",
              " ['Out of the Box Reasoning with Graph Convolution Nets for Factual Visual Question Answering'],\n",
              " ['Joint Subbands Learning with Clique Structures for Wavelet Domain SuperResolution',\n",
              "  'Using FiringRate Dynamics to Train Recurrent Networks of Spiking Model Neurons',\n",
              "  'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks',\n",
              "  'SoftGated WarpingGAN for PoseGuided Person Image Synthesis',\n",
              "  'Overlapping Clustering Models, and One (class) SVM to Bind Them All'],\n",
              " ['StructureAware Convolutional Neural Networks',\n",
              "  'Learning Versatile Filters for Efficient Convolutional Neural Networks',\n",
              "  'Progressive Neural Networks',\n",
              "  'SelfSupervised Generation of Spatial Audio for 360 Video'],\n",
              " ['Learning Spatiotemporal Features with 3D Convolutional Networks',\n",
              "  'Sigsoftmax Reanalysis of the Softmax Bottleneck'],\n",
              " ['Understanding the Role of Adaptivity in Machine Teaching The Case of Version Space Learners']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFpOzI5qb7tT",
        "outputId": "2f641b22-c638-4fb9-e98b-dad6b2fd41d5"
      },
      "source": [
        "!pip install yake"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yake\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/c4de4fb40639ec674f944d82e5b0be5a5a9162fc8e83e379ab10b83ee1f9/yake-0.4.8-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from yake) (0.8.9)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from yake) (2.5.1)\n",
            "Collecting segtok\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.7/dist-packages (from yake) (8.0.0)\n",
            "Collecting jellyfish\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/a6/4d039bc827a102f62ce7a7910713e38fdfd7c7a40aa39c72fb14938a1473/jellyfish-0.8.2-cp37-cp37m-manylinux2014_x86_64.whl (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from yake) (1.19.5)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->yake) (4.4.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->yake) (2019.12.20)\n",
            "Building wheels for collected packages: segtok\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=28dc7666d1302593226798930fed0a23750e78d236b681b2febb44e919bccf3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "Successfully built segtok\n",
            "Installing collected packages: segtok, jellyfish, yake\n",
            "Successfully installed jellyfish-0.8.2 segtok-1.5.10 yake-0.4.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6YwALQO6ysz"
      },
      "source": [
        "import yake\n",
        "kw_extractor = yake.KeywordExtractor()\n",
        "language = \"en\"\n",
        "max_ngram_size = 2\n",
        "deduplication_threshold = 0.1\n",
        "numOfKeywords = 20\n",
        "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
        "\n",
        "cluster_append = []\n",
        "length = len(appended_final_list)\n",
        "for i in range(length):\n",
        "  cluster_append.append([])\n",
        "for i in range(len(appended_final_list)):\n",
        "  keywords = custom_kw_extractor.extract_keywords(appended_final_list[i])\n",
        "  for k,w in keywords[:5]:\n",
        "    cluster_append[i].append(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2T1UVvd6yvk",
        "outputId": "9a6b56d6-b390-4581-edec-c3c68c3fd8c5"
      },
      "source": [
        "len(appended_final_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao_WOgau6yyA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUq2v8FO6y1D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdbruGgFHgM"
      },
      "source": [
        "# Finding the most occurred keywords of each cluster\n",
        "\n",
        "\n",
        "# from collections import Counter\n",
        "\n",
        "# cluster_append = []\n",
        "# for i in appended_final_list:\n",
        "  \n",
        "#   split_it = i.split()\n",
        " \n",
        "\n",
        "#   Countervar = Counter(split_it)\n",
        " \n",
        "\n",
        "#   most_occur = Countervar.most_common(4)\n",
        "\n",
        "#   cluster_append.append(most_occur)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJdIqwKwWF9Y"
      },
      "source": [
        "for i in range(len(topicss)):\n",
        "  topicss[i].insert(0,cluster_append[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcDeOOAcUZSB"
      },
      "source": [
        "topicss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yf9dNDgETv1B",
        "outputId": "29982fe7-f1ab-484f-8532-0f0b33ea8896"
      },
      "source": [
        "# Creating the graph of the clusters for USER UNDERSTANDING\n",
        "\n",
        "from graphviz import Digraph\n",
        "\n",
        "g = Digraph('G', filename='process.gv', engine='sfdp')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(final_list)):\n",
        "  \n",
        "\n",
        "  \n",
        "  g.edge('Total Samples =  {amount} \\n Clusters defined'.format(amount = len(data_list2)), 'Cluster '+str(i)+'\\n{len}'.format(len = len(final_list[i]))+'\\n'+str(cluster_append[i]))\n",
        "\n",
        "  g.graph_attr['rankdir'] = 'LR'\n",
        "  g.edge_attr.update(arrowhead='vee', arrowsize='0.5',)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "g.view()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'process.gv.pdf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s14Makxq9l59"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}